{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification 1: KKN, Logit, and LDA (MACS 30100)\n",
    "### by [Richard W. Evans](https://sites.google.com/site/rickecon/), January 2018\n",
    "The code in this Jupyter notebook was written using Python 3.6. It uses data files `?`, and ? image files in the `images` folder in the same directory as this notebook. For the code to run properly, you will either need to have access to the internet or you should have the data file in the same folder as the Jupyter notebook file. Otherwise, you will have to change the respective lines of the code that read in the data to reflect the location of that data. Much of this discussion follows the presentation in chapters 2, 3, and 4 of [JWHT17]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning\n",
    "* Provided with a model.\n",
    "* What's been going on so far\n",
    "\n",
    "Parametric Models\n",
    "* GMM, MLE, and linear regression; all parametric so far.\n",
    "* Nonparametric models are still a finite number of parameters; but it's implicitly define, number of parameters can be flexible\n",
    "\n",
    "Classfiers\n",
    "* We'll go over three learning methods.\n",
    "* Logistic Regression is a parametric model and it's supervised learning; logit regression\n",
    "* KNN is a nonparametric model; also supervised learning\n",
    "* Linear Discriminate Analysis\n",
    "\n",
    "Unsupervised Learning Methods\n",
    "* Data being fed, parameters being chosen, etc. Neural nets, random forests, support vector machines.\n",
    "* Deep learning - neural nets of neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quantitative Versus Qualitative Data\n",
    "The regression models of the [Linear Regression](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/LinRegress/LinRegress.ipynb) notebook have quantitative variables as dependent variables. That is, the $y_i$ variable takes on a continuum of values. We use a different class of models to estimate the relationship of exogenous variables to *qualitative* or *categorical* endogenous or dependent variables.\n",
    "\n",
    "Examples of qualitative or categorical variables include:\n",
    "\n",
    "* Binary variables take on two values ($J=2$), most often 0 or 1. Examples: Male or female, dead or alive, accept or reject.\n",
    "* General categorical variables can take on more than two values ($J\\geq 2$). Examples: red, blue, or green; teenager, young adult, middle aged, senior.\n",
    "\n",
    "Note with general categorical variables that order and numerical distance do not matter. As an example let $FlowerColor_i=\\{red=1, blue=2,green=3\\}$ be a function of $neighborhood_i$, $season_i$, and $income_i$.\n",
    "\n",
    "$$ FlowerColor_i = \\beta_0 + \\beta_1 neighborhood_i + \\beta_2 season_i + \\beta_3 income_i + u_i $$\n",
    "\n",
    "We could mathematically estimate this regression model, but would that make sense? What would be wrong with a regression model?\n",
    "\n",
    "With colors, putting them in an order doesn't make sense. Using numbers means that the order matters, and that the distance between those numbers matters. Which is not the case with flower colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The classification setting\n",
    "Let $y_i$ be a qualitative dependent variable on $N$ observations with $i$ being the index of the observation, and let $x_{p,i}$ be the $i$th observation of the $p$th explanatory variable (independent variable) such that $X_i=\\{x_{1,i}, x_{2,i}, ... x_{P,i}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Classifier\n",
    "In this section, we will look at two models for binary (0 or 1) categorical dependent variables. We describe the first model--the linear probability (LP) model--for purely illustrative purposes. This is because the LP model has some serious shortcomings that make it almost strictly dominated by our second model in this section.\n",
    "\n",
    "The second model--the logistic regression (logit, binary classifier) model--will be the focus of this section. There is another variant of this model, the probit model. But the logistic model is the more flexible, more easily interpretable, and more commonly used of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. The linear probability (LP) model\n",
    "One option in which a regression is barely acceptable for modeling a binary (categorical) dependent variable is the linear probability (LP) model. When the dependent variable has only two categories, it can be modeled as $y_i\\in\\{0,1\\}$ without loss of generality. Let the variable $z_i$ be interpreted as the probability that $y_i=1$ given the data $X_i$ and parameter values $\\theta=\\{\\beta_0,\\beta_1,...\\beta_P\\}$.\n",
    "\n",
    "$$ z_i = Pr(y_i=1|X_i,\\theta) = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} + u_i $$\n",
    "\n",
    "The LP model can be a nice, easy, computationally convenient way to estimate the probability of outcome $y_i=1$. This could also be reinterpreted, without loss of generality, as the probability that $y_i=0$. This is equivalent to a redefinition of which outcome is defined as $y_i=1$.\n",
    "\n",
    "The main drawback of the LP model is that the predicted values of the probability that $y_i=1$ or $Pr(y_i=1|X_i,\\theta)$ can be greater than 1 and can be less than 0. It is for this reason that it is very difficult to publish any research based on an LP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The logistic (logit) regression classifier\n",
    "The logistic regression model is a binary dependent variable classifier that constrains its predicted values to be stricly between 0 and 1. This is done by using a sigmoid function as the model on the right-hand-side of the equation,\n",
    "\n",
    "$$ f(x) = \\frac{g(x)}{1 + g(x)} \\quad\\text{for}\\quad x\\in(-\\infty,\\infty) \\quad\\text{and}\\quad g(x)\\geq 0 \\quad\\forall x $$\n",
    "\n",
    "and has the following general shape.\n",
    "\n",
    "![sigmoid.png](attachment:sigmoid.png)\n",
    "\n",
    "The logistic function is the specific case of the sigmoid function in which $g(X)= e^{X\\beta} = e^{\\beta_0 + \\beta_1 x_{1,i} + ...\\beta_P x_{P,i}}$.\n",
    "\n",
    "$$ Pr(y_i=1|X_i,\\theta) = \\frac{e^{X_i\\beta}}{1 + e^{X_i\\beta}} = \\frac{e^{\\beta_0 + \\beta_1 x_{1,i} + ...\\beta_P x_{P,i}}}{1 + e^{\\beta_0 + \\beta_1 x_{1,i} + ...\\beta_P x_{P,i}}} $$\n",
    "\n",
    "We could estimate the paramters $\\theta=\\{\\beta_0,\\beta_1,...\\beta_P\\}$ by GMM using nonlinear least squares or a more general set of moments to match. But maximum likelihood estimation is the most common method for estimating the parameters $\\theta$ because of its more robust statistical properties. Also, the distributional assumptions are built into the model, so they are not overly strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Nonlinear least squares estimation\n",
    "If we define $z_i = Pr(y_i=1|X_i,\\theta)$, then the error in the logistic regression is the following.\n",
    "\n",
    "$$ \\varepsilon_i = y_i - z_i $$\n",
    "\n",
    "The GMM specification of the nonlinear least squares method of estimating the parameter vector $\\theta$ would then be the following.\n",
    "\n",
    "$$ \\hat{\\theta}_{nlls} = \\theta:\\quad \\min_{\\theta} \\sum_{i=1}^N\\varepsilon_i^2 \\quad = \\quad \\min_{\\theta}\\sum_{i=1}^N\\bigl(y_i - z_i \\bigr)^2 \\quad = \\quad \\min_{\\theta} \\sum_{i=1}^N\\Bigl[y_i - Pr(y_i=1|X_i,\\theta)\\Bigr]^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Maximum likelihood estimation\n",
    "We characterized the general likelihood function for a sample of data as the probability that the given sample $(y_i,X_i)$ came from the assumed distribution given parameter values $Pr(y_i=1|X_i,\\theta)$.\n",
    "\n",
    "$$ \\mathcal{L}(y_i,X_i|\\theta) = \\prod_{i:y_i=1}Pr(y_i=1|X_i,\\theta)\\prod_{i':y_{i'}=0}\\Bigl[1 - Pr(y_i=1|X_i,\\theta)\\Bigr] $$\n",
    "\n",
    "The intuition of this likelihood function is that you want the probability of the observations for which $y_i=1$ to be close to one $Pr(X)$, and you want the probability of the observations for which $y_i=0$ to also be close to one $1 - Pr(X)$.\n",
    "\n",
    "The log-likelihood function, which the MLE problem maximizes is the following.\n",
    "\n",
    "$$\\ln\\bigl[\\mathcal{L}(y_i,X_i|\\theta)\\bigr] = \\sum_{i:y_i=1}\\ln\\bigl[Pr(y_i=1|X_i,\\theta)\\bigr] + \\sum_{i':y_{i'}=0}\\ln\\biggl(\\Bigl[1 - Pr(y_i=1|X_i,\\theta)\\Bigr]\\biggr) $$\n",
    "\n",
    "The MLE problem for estimating $\\theta$ of the logistic regression model is, therefore, the following.\n",
    "\n",
    "$$ \\hat{\\theta}_{mle} = \\theta:\\quad \\max_{\\theta} \\ln\\bigl[\\mathcal{L}(y_i,X_i|\\theta)\\bigr] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Titanic example\n",
    "A good example of logistic regression comes from a number of sources. But I am adapting some code from [http://www.data-mania.com/blog/logistic-regression-example-in-python/](http://www.data-mania.com/blog/logistic-regression-example-in-python/). The research question is to use a famous Titanic passenger dataset to try to identify the characteristics that most predict whether you survived $y_i=1$ or died $y_i=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-30b8fa3bc3c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'whitegrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m url = ('https://raw.githubusercontent.com/BigDataGal/Python-for-Data-Science/' +\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sb' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n",
    "from pylab import rcParams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sb\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "sb.set_style('whitegrid')\n",
    "\n",
    "url = ('https://raw.githubusercontent.com/BigDataGal/Python-for-Data-Science/' +\n",
    "      'master/titanic-train.csv')\n",
    "titanic = pd.read_csv(url)\n",
    "titanic.columns = ['PassengerId','Survived','Pclass','Name','Sex','Age',\n",
    "                   'SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VARIABLE DESCRIPTIONS **\n",
    "\n",
    "Survived - Survival (0 = No; 1 = Yes)\n",
    "Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "Name - Name\n",
    "Sex - Sex\n",
    "Age - Age\n",
    "SibSp - Number of Siblings/Spouses Aboard\n",
    "Parch - Number of Parents/Children Aboard\n",
    "Ticket - Ticket Number\n",
    "Fare - Passenger Fare (British pound)\n",
    "Cabin - Cabin\n",
    "Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "**Checking that your target variable is binary**\n",
    "\n",
    "Since we are building a model to predict survival of passangers from the Titanic, our target is going to be \"Survived\" variable from the titanic dataframe. To make sure that it's a binary variable, let's use Seaborn's countplot() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb.countplot(x='Survived', data=titanic, palette='hls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for missing values**\n",
    "\n",
    "It's easy to check for missing values by calling the `isnull()` method, and the `sum()` method off of that, to return a tally of all the `True` values that are returned by the `isnull()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many observations are there in the DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model selection and missing values**\n",
    "\n",
    "The variable `Cabin` is almost all missing values, so we should not include that in our analysis (although that would be an interesting variable to have). We can also probably exclude `Name`, `Ticket` (ticket number). But we will include all the other variables.\n",
    "\n",
    "* Survived - This variable is obviously relevant.\n",
    "* Pclass - Does a passenger's class on the boat affect their survivability?\n",
    "* Sex - Could a passenger's gender impact their survival rate?\n",
    "* Age - Does a person's age impact their survival rate?\n",
    "* SibSp - Does the number of relatives on the boat (that are siblings or a spouse) affect a person survivability? Probably.\n",
    "* Parch - Does the number of relatives on the boat (that are children or parents) affect a person survivability? Probably.\n",
    "* Fare - Does the fare a person paid effect his survivability? Maybe.\n",
    "* Embarked - Does a person's point of embarkation matter? It depends on how the boat was filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_data = titanic.drop(['PassengerId','Name','Ticket','Cabin'], 1)\n",
    "titanic_data.head()Now we have the dataframe reduced down to only relevant variables, but now we need to deal with the missing values in the age variable.\n",
    "\n",
    "Imputing missing values\n",
    "Let's look at how passenger age is related to their class as a passenger on the boat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the dataframe reduced down to only relevant variables, but now we need to deal with the missing values in the age variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputing missing values**\n",
    "\n",
    "Let's look at how passenger age is related to their class as a passenger on the boat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb.boxplot(x='Pclass', y='Age', data=titanic_data, palette='hls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaking roughly, we could say that the younger a passenger is, the more likely it is for them to be in 3rd class. The older a passenger is, the more likely it is for them to be in 1st class. So there is a loose relationship between these variables. So, let's write a function that approximates a passengers age, based on their class. From the box plot, it looks like the average age of 1st class passengers is about 37, 2nd class passengers is 29, and 3rd class pasengers is 24.\n",
    "\n",
    "So let's write a function that finds each null value in the `Age` variable, and for each null, checks the value of the Pclass and assigns an age value according to the average age of passengers in that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def age_approx(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    \n",
    "    if pd.isnull(Age):\n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "        elif Pclass == 2:\n",
    "            return 29\n",
    "        else:\n",
    "            return 24\n",
    "    else:\n",
    "        return Age\n",
    "\n",
    "titanic_data['Age'] = titanic_data[['Age', 'Pclass']].apply(age_approx, axis=1)\n",
    "titanic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 null values in the `Embarked` variable. We can drop those 2 records without loosing too much important information from our dataset, so we will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_data.dropna(inplace=True)\n",
    "titanic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting categorical variables to a dummy indicators**\n",
    "\n",
    "The next thing we need to do is reformat our variables so that they work with the model. Specifically, we need to reformat the Sex and Embarked variables into numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender = pd.get_dummies(titanic_data['Sex'], drop_first=True)\n",
    "gender.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embark_location = pd.get_dummies(titanic_data['Embarked'], drop_first=True)\n",
    "embark_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_data.drop(['Sex', 'Embarked'], axis=1, inplace=True)\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_dmy = pd.concat([titanic_data, gender, embark_location], axis=1)\n",
    "titanic_dmy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataset with all the variables in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for correlation vs. independence between variables**\n",
    "\n",
    "We can use `seaborn`'s `.heatmap` method to get a color-coded heatmap of the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb.heatmap(titanic_dmy.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is OK for the first row and first column to have correlation (dark red or dark blue). But we want to avoid heavy correlation among the independent variables (regressors). It looks like `Fare` and `Pclass` are highly negatively correlated, so let's just keep `Fare`. Also, the embark locations of `Q` and `S` are highly negatively correlated, but that is to be expected because they are mutually exclusive. So we will keep both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_dmy.drop(['Pclass'], axis=1, inplace=True)\n",
    "titanic_dmy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking that your dataset size is sufficient**\n",
    "\n",
    "We have 6 predictive features that remain. The rule of thumb is 50 records per regressor. So we need to have at least 300 records in this dataset. Let's check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_dmy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to divide up the data into training data $(y_{trn,i}, X_{trn,i})$ and test data $(y_{tst,i}, X_{tst,i})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = titanic_dmy.ix[:,(1,2,3,4,5,6)].values\n",
    "y = titanic_dmy.ix[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3,\n",
    "                                                    random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to let scikit-learn run the maximum likelihood estimation of our multiple logistic regression classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression()\n",
    "LogReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = LogReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix tells us Type I and Type II errors. The results from the confusion matrix are telling us that 136 and 70 are the number of correct predictions. 33 and 28 are the number of incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-nearest Neighbors (KNN) Classifier\n",
    "The $K$-nearest Neighbors (KNN) classifier is a nonparametric model, based on the value of the parameter $K$ that predicts the category of a given observation. Let $J\\geq 2$ be the number of possible categories such that $y_i\\in\\{1,2,...J\\}$. Assume that we also have data on two other variables $X_i=(x_{1,i}, x_{2,i})$ for each observation.\n",
    "\n",
    "The KNN classifier finds the $K$ points in the data that are closest to $X_0$ and calculates the conditional probability for each category $j$ in that set of $K$ points. Define the set of observations $i$ in that set of $K$ points closest to $X_0$ as $\\mathcal{N}_0$.\n",
    "\n",
    "$$ Pr(Y=j|X=X_0) = \\frac{1}{K}\\sum_{i\\in\\mathcal{N}_0}I\\left(y_i=j\\right) $$\n",
    "\n",
    "The KNN classifier applies Bayes rule and classifies the test observation $X_0$ to the class with the largest probability.\n",
    "\n",
    "The Figure 2.14 from [JWHT17] illustrates a KNN classifier with $K=3$ for six blue and six orange observations. The left pannel highlights the $K=3$ nearest observations to the observation marked \"x\". Of those three nearest neighbors, 2/3 are blue and 1/3 are orange. So Bayes rule classifies the observation marked \"x\" as blue.\n",
    "\n",
    "![Fig2_14.png](attachment:Fig2_14.png)\n",
    "\n",
    "The panel on the right in Figure 2.14 shows the decision boundary for each classification. This boundary is the point where the two categories have the same probability Pr(Y=j|X) = 1/2.\n",
    "\n",
    "Figure 2.15 below shows a larger set of simulated binary dependent variable data for KNN classifier $K=10$. Notice that the black line decision boundary is very close to the optimal dashed line Bayes classifier (if you knew the true data generating process). We show this to demonstrate how well and how flexibly a KNN classifier can perform.\n",
    "\n",
    "![Fig2_15.png](attachment:Fig2_15.png)\n",
    "\n",
    "Figure 2.16 below presents two extremes in KNN classifiers. The left panel is the $K=1$ KNN classifier. This KNN simply assigns the value of the closest neighbor to the data. Notice how rough the decision boundaries are. This KNN classifier is probably overfitting the data. That is, it is likely capturing too much noise to be good at predicting test data.\n",
    "\n",
    "![Fig2_16.png](attachment:Fig2_16.png)\n",
    "\n",
    "The right panel of Figure 2.16 shows a KNN classifier with $K=100$. This classifier barely has any contour at all. This classifier is likely underfitting the data or ignoring information. This can be seen by comparing the black KNN decision boundary to the dashed optimal Bayes classifier decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, neighbors, linear_model\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "print(X_digits)\n",
    "y_digits = digits.target\n",
    "print(y_digits)\n",
    "\n",
    "n_samples = len(X_digits)\n",
    "\n",
    "X_train = X_digits[:int(.9 * n_samples)]\n",
    "y_train = y_digits[:int(.9 * n_samples)]\n",
    "X_test = X_digits[int(.9 * n_samples):]\n",
    "y_test = y_digits[int(.9 * n_samples):]\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Discriminant Analysis (LDA)\n",
    "Put linear discriminant analysis (LDA) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References\n",
    "* [JWHT17] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. *An Introduction to Statistical Learning with Applications in R*, Springer Texts in Statistics, Springer, 2017."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
