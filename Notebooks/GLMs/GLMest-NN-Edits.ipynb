{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models (MACS 30100)\n",
    "### by [Benjamin Soltoff](http://www.bensoltoff.com/) and [Richard W. Evans](https://sites.google.com/site/rickecon/), February 2018\n",
    "The code in this Jupyter notebook was written using Python 3.6. It uses data files `?`, and ? image files in the `images` folder in the same directory as this notebook. For the code to run properly, you will either need to have access to the internet or you should have the data file in the same folder as the Jupyter notebook file. Otherwise, you will have to change the respective lines of the code that read in the data to reflect the location of that data. Much of this content was taken from Dr. Benjamin Soltoff's notes [here](http://cfss.uchicago.edu/persp005_glm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The General Specification\n",
    "Generalized linear models (GLMs) are a general formulation of linear models for cases in which the error terms can have a general distribution (not necessarily the Normal distribution) and also cases in which the dependent variable is a categorical variable (non necessarily binary). GLMs are typically estimated using maximum likelihood estimation, though they can also be estimated using generalized method of moments as well as Bayesian estimation.\n",
    "\n",
    "A GLM consists of three components:\n",
    "1. A **random component** specifying the conditional distribution of the response variable, $Y_i$, given the values of the dependent (predictor) variables in the model. Typically these distributions are a member of the [exponential family](https://en.wikipedia.org/wiki/Exponential_family), a set of related probability distributions.\n",
    "\n",
    "2. A **linear predictor** that is a linear function of regressors.The regressors are prespecified functions of the explanatory variables. This is exactly like the form youâ€™ve seen for linear and logistic regression, because in fact linear and logistic regression are types of GLMs.\n",
    "\n",
    "$$ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "3. A **link function** $g(\\cdot)$, which transforms the expectation of the response variable $\\mu_i \\equiv E(Y_i)$ to the linear predictor.\n",
    "\n",
    "$$ g(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "* Because the link function must also be **invertible**, we can also write it as the following.\n",
    "\n",
    "$$ \\mu_i = g^{-1}(\\eta_i) = g^{-1}\\left(\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i}\\right) $$\n",
    "\n",
    "* The inverted link function is also known as the **mean function**. The purpose of the link function is to relate the linear predictor to the mean of the distribution function.\n",
    "\n",
    "The GLM approach allows us to embed a linear predictor, which is fairly easy to interpret, into a wide class of nonlinear models with a wide range of error distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link function = function of the mean of the random component\n",
    "* Says that the mean of this process is some general function (link function) of the linear predictor)\n",
    "* Important thing; that it be invertable; can do transformation to the link function that gives us the linear predictor; inversion of the link function gets us the linear predictor\n",
    "\n",
    "For OLS:\n",
    "* Random component = the error\n",
    "* Linear predictor = constant, other regressor and their coefficients\n",
    "* Link function = just the idea function; g() = y_subi\n",
    "* Estimate these by maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Linear regression as a GLM\n",
    "The linear regression that we studied in the [Linear Regression Notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/LinRegress/LinRegress.ipynb) is the simplest special case of a GLM. However, the GLM linear regression does not necessarily assume that the errors are normally distributed. The errors can come from any distribution.\n",
    "\n",
    "The linear regression model is the following, where we specify a generic PDF for the error terms $u_i$.\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} + u_i \\quad\\text{where}\\quad u_i\\sim i.i.d.(0, \\sigma) $$\n",
    "\n",
    "In every case (not just linear regression), the $\\eta$ function is the linear portion of the linear regression, excluding the error terms.\n",
    "\n",
    "$$ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "In the regression equation, we know that the expected value of $Y_i$ is the linear predictor $\\eta_i$, which is a link function of the mean of the errors.\n",
    "\n",
    "$$ E[Y=y_i|X\\beta] = g(\\mu_i=0) = \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "As we did in Section 6 of the [Maximum Likelihood Estimation notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/MLE/MLest.ipynb), we estimate this GLM version of a linear regression by maximum likelihood. Note that this allows us to have more general assumptions on the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Logistic regression\n",
    "The logistic regression that we covered in the [Classifiers 1 notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/Classfcn1/KKNlogitLDA.ipynb) is also a special case of GLM. To characterize the logistic model as a GLM, we assume that the  probability of binary outcome $y_i=1$ given data $X_i$ and parameter vector $\\beta$ is $\\mu_i$.\n",
    "\n",
    "$$ Pr(y_i=1|X\\beta) = \\mu_i \\quad\\text{and}\\quad Pr(y_i=0|X\\beta) = 1 - \\mu_i $$\n",
    "\n",
    "The linear predictor is the same $\\eta_i$.\n",
    "\n",
    "$$ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "The link function $g(\\mu_i)$ is the logistic function.\n",
    "\n",
    "$$ g(\\mu_i) = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} $$\n",
    "\n",
    "So the probability of $Y=y_i\\in\\{0,1\\}$ is the Bernoulli distribution.\n",
    "\n",
    "$$ Pr(Y=y_i|X\\beta) = \\mu_i^{y_i}(1 - \\mu_i)^{y_i} $$\n",
    "\n",
    "The link function is called the log-odds function.\n",
    "\n",
    "$$ \\ln\\left(\\frac{\\mu_i}{1 - \\mu_i}\\right) = \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "As was shown in the [Classifiers 1 notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/Classfcn1/KKNlogitLDA.ipynb), we estimate the parameters of the logistic regression function by maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression (Logit)\n",
    "* Linear Predictor = constant coefficient and coefficients of regressions\n",
    "* Link Function is the logistic function\n",
    "* Bernoulli distribution = fancy term for a coin flip essentially; fully specified distribution for a binary variable \n",
    "* Random component is fully specified by this \n",
    "* Link Function is the g inverse of \\eta_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Multinomial logistic regression\n",
    "Multinomial logistic regression is a model for estimating or predicting the effects on independent variables (i.e., features, regressors) on a categorical dependent variable with more than two categories: $y_i\\in\\{1,2,... J\\}$. Multinomial logistic regression generalizes the logit model to a **polytomy** (more than 2 outcomes) dependent variable $y_i$.\n",
    "\n",
    "Let the linear predictor of features or independent variables or regressors be similar to the old $\\eta_i$. The difference is that we need a linear predictor for each possible type $j$. This means that we will estimate a different set of coefficients $\\beta_j$ in the linear predictor for each type $j$.\n",
    "\n",
    "$$ \\eta_{j,i} = \\beta_{j,0} + \\beta_{j,1} x_{1,i} + \\beta_{j,2} x_{2,i} + ... \\beta_{j,P} x_{P,i} $$\n",
    "\n",
    "We are interested in estimating or predicting the probability $\\mu_{j,i}$ that the observation is of class $j\\in\\{1,2,...J\\}$. We can model the probability of $y_i=j$ by a similar logistic function.\n",
    "\n",
    "$$ Pr(y_i=j|X\\beta) = \\mu_{j,i} = \\frac{e^{\\eta_{j,i}}}{1 + \\sum_{k=1}^{J-1} e^{\\eta_{k,i}}} = \\frac{e^{\\beta_{j,0} + \\beta_{j,1} x_{1,i} + \\beta_{j,2} x_{2,i} + ... \\beta_{j,P} x_{P,i}}}{1 + \\sum_{k=1}^{J-1} e^{\\beta_{k,0} + \\beta_{k,1} x_{1,i} + \\beta_{k,2} x_{2,i} + ... \\beta_{k,P} x_{P,i}}} \\quad\\text{for}\\quad j=1,2,...J-1 $$\n",
    "\n",
    "Note the $J-1$ limit of the sum in the denominator of the logistic function. What we are doing is estimating a logistic model for each type, where the cumulative sum in the denominator leaves out a baseline category. It would be redundant or multicollinear if we included it because of the restriction that the probabilities must sum to 1. So, without loss of generality, we name the baseline category as the last category $y_i=J$, and its probability is a function of all the other estimated logistic models for $j=1,2,...J-1$.\n",
    "\n",
    "$$ \\mu_{J,i} = 1 - \\sum_{j=1}^{J-1}\\mu_{j,i} \\quad\\text{because}\\quad \\sum_{j=1}^J \\mu_{j,i} = 1 $$\n",
    "\n",
    "Because the numerator in the multinomial logistic function does not include interaction terms between the coefficients of the other multinomial logistic equations, this estimation relies on the assumption of [independence of irrelevant alternatives](https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives). This assumption may be violated in some cases.\n",
    "\n",
    "With some algebraic manipulation, the link function of the multinomial logistic regression is also a log odds function, where the denominator probability is the baseline probability $\\mu_{J,i}$.\n",
    "\n",
    "$$ \\ln\\left(\\frac{\\mu_{j,i}}{\\mu_{J,i}}\\right) \\quad\\text{for}\\quad j=1,2,...J-1 $$\n",
    "\n",
    "We can estimate all the coefficients $\\beta_{j,p}$ of this model by maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of J's' as separate categories, each with their own model.\n",
    "* e.g., J = 1 is for blonde hair, J = 2 is for brown hair, J = 3 is for red hair, J = 4 for black hair, etc.\n",
    "* Can't have a j on the regressors; just i's on the regressors \n",
    "* Exponential forces it to be positive; then dividing it bounds between zero and one\n",
    "\n",
    "Going from logit to multinomial logit:\n",
    "* Denominator has to be different if what we are going to get is a probability of four different types, and those four probabilities have to sum to one\n",
    "* Sum of all of our predicted probabilities needs to be 1; four \"hair color\" types, estimated probs of those four needs to sum to one\n",
    "* So the denominator is different because it needs to be 1 plus all the E to the j i's\n",
    "\n",
    "Random Component:\n",
    "* Analogous to the Bernoulli \n",
    "\n",
    "For each type, estimation:\n",
    "* Three of the four hair colors, ened to estimate a set of beta coefficients and that will give us a mu\n",
    "* For the fourth category, that will be 1 - the other three categories\n",
    "\n",
    "Link Function\n",
    "* Gives us log of the probabilitie of being type J over the probability of being type (whatever baseline category is).\n",
    "* Link function is a log odds ratio\n",
    "\n",
    "Estimated coefficients only tell us about a category vs. the baseline category\n",
    "* So like, brown hair vs. black hair and everything else (the baseline category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Multinomial logistic example using Titanic data\n",
    "Let's go back to the Titanic dataset that we used in the logistic regression section of the [Classifiers notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/Classfcn1/KKNlogitLDA.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noranickels/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from pylab import rcParams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "sb.set_style('whitegrid')\n",
    "\n",
    "url = ('https://raw.githubusercontent.com/BigDataGal/Python-for-Data-Science/' +\n",
    "      'master/titanic-train.csv')\n",
    "titanic = pd.read_csv(url)\n",
    "titanic.columns = ['PassengerId','Survived','Pclass','Name','Sex','Age',\n",
    "                   'SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, instead of modeling the binary variable of survival, lets model the port of embarkation $Embarked_i$ as a function of passenger fare $Fare_i$ and age $Age_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Embarked     Fare   Age\n",
       "0        S   7.2500  22.0\n",
       "1        C  71.2833  38.0\n",
       "2        S   7.9250  26.0\n",
       "3        S  53.1000  35.0\n",
       "4        S   8.0500  35.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = titanic[['Embarked', 'Fare', 'Age']]\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10630cb70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0wAAAKvCAYAAABZOk8vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1062fceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tells us how many categories in embarked and how many obsv in each category\n",
    "sb.countplot(x='Embarked', data=titanic, palette='hls')\n",
    "\n",
    "# we want to predict which embarked category based on fare and age; problem: not many for Q\n",
    "# if we create a predictive model, we'll be best at predicting S; won't have as much power for predicting C and Q\n",
    "# the more data we have, the better we'll be at predicting, typically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the three categories of port of embarkation are S=Southampton, C=Cherbourg, and Q=Queenstown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embarked      2\n",
       "Fare          0\n",
       "Age         177\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 3 columns):\n",
      "Embarked    889 non-null object\n",
      "Fare        891 non-null float64\n",
      "Age         714 non-null float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 21.0+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, let's just drop the observations for whom we don't have data on age or embarkation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Embarked     Fare   Age\n",
       "0        S   7.2500  22.0\n",
       "1        C  71.2833  38.0\n",
       "2        S   7.9250  26.0\n",
       "3        S  53.1000  35.0\n",
       "4        S   8.0500  35.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.dropna(inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set S=Southampton as our reference category because it is big. We would not want to do this if we wanted to estimate how the two regressors affect the probability of embarked=S (Southampton). Let's first separate the data into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noranickels/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# higher percentage of training set, better estimation, but then less data to test on\n",
    "# how big test set and training set should be doesn't have a right answer\n",
    "\n",
    "X = titanic[['Fare', 'Age']]\n",
    "y = titanic[['Embarked']]\n",
    "# This function train_test_split is from sklearn.cross_validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .5,\n",
    "                                                    random_state=25)\n",
    "\n",
    "# solver - only a handful of solvers that will work with multinomial logistic regression\n",
    "MultLogReg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "MultLogReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the three categories using this estimated model on the test set\n",
    "\n",
    "y_pred = MultLogReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7,   0,  52],\n",
       "       [  0,   0,  16],\n",
       "       [ 14,   0, 267]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how good of a model is this?\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "# middle column (Qs)\n",
    "# right column S\n",
    "# diagonal is how many right on the full population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          C       0.33      0.12      0.17        59\n",
      "          Q       0.00      0.00      0.00        16\n",
      "          S       0.80      0.95      0.87       281\n",
      "\n",
      "avg / total       0.68      0.77      0.71       356\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noranickels/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# precision column\n",
    "# 33% accurate on embarked in Cherbourg\n",
    "# 0% accurate in predicting people in Queenstown\n",
    "# Southhampton we can predict 80% of those based on fare and age\n",
    "\n",
    "# How to tune this model up? Resampling and machine learning woot woot\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# what if we added a variable? predicting where people embarked\n",
    "# Pclass will be very highly related to fare\n",
    "# maybe sibs, parent and children?\n",
    "# but adding more regressors is hard when you have more observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Other GLM models in scikit-learn\n",
    "The scikit-learn package has a [number of other](http://scikit-learn.org/stable/modules/linear_model.html) GLM models: OLS, Ridge regression, Lasso, Multi-task Lasso, Elastic Net, Multi-task Elastic Net, Least Angle Regression, LARS Lasso, Orthogonal Matching Pursuit (OMP), Bayesian regression, Logistic regression (including multinomial logistic regression), Stochastic Gradient Descent (SGD), Perceptron, Passive Aggressive Algorithms, Robustness regression, and Polynomial regression."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
