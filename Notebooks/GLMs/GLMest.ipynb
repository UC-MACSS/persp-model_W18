{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models (MACS 30100)\n",
    "### by [Benjamin Soltoff](http://www.bensoltoff.com/) and [Richard W. Evans](https://sites.google.com/site/rickecon/), February 2018\n",
    "The code in this Jupyter notebook was written using Python 3.6. It uses data files `?`, and ? image files in the `images` folder in the same directory as this notebook. For the code to run properly, you will either need to have access to the internet or you should have the data file in the same folder as the Jupyter notebook file. Otherwise, you will have to change the respective lines of the code that read in the data to reflect the location of that data. Much of this content was taken from Dr. Benjamin Soltoff's notes [here](http://cfss.uchicago.edu/persp005_glm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The General Specification\n",
    "Generalized linear models (GLMs) are a general formulation of linear models for cases in which the error terms can have a general distribution (not necessarily the Normal distribution) and also cases in which the dependent variable is a categorical variable (non necessarily binary). GLMs are typically estimated using maximum likelihood estimation, though they can also be estimated using generalized method of moments as well as Bayesian estimation.\n",
    "\n",
    "A GLM consists of three components:\n",
    "1. A **random component** specifying the conditional distribution of the response variable, $Y_i$, given the values of the dependent (predictor) variables in the model. Typically these distributions are a member of the [exponential family](https://en.wikipedia.org/wiki/Exponential_family), a set of related probability distributions.\n",
    "\n",
    "2. A **linear predictor** that is a linear function of regressors.The regressors are prespecified functions of the explanatory variables. This is exactly like the form youâ€™ve seen for linear and logistic regression, because in fact linear and logistic regression are types of GLMs.\n",
    "\n",
    "$$ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "3. A **link function** $g(\\cdot)$, which transforms the expectation of the response variable $\\mu_i \\equiv E(Y_i)$ to the linear predictor.\n",
    "\n",
    "$$ g(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "* Because the link function must also be **invertible**, we can also write it as the following.\n",
    "\n",
    "$$ \\mu_i = g^{-1}(\\eta_i) = g^{-1}\\left(\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i}\\right) $$\n",
    "\n",
    "* The inverted link function is also known as the **mean function**. The purpose of the link function is to relate the linear predictor to the mean of the distribution function.\n",
    "\n",
    "The GLM approach allows us to embed a linear predictor, which is fairly easy to interpret, into a wide class of nonlinear models with a wide range of error distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Linear regression as a GLM\n",
    "The linear regression that we studied in the [Linear Regression Notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/LinRegress/LinRegress.ipynb) is the simplest special case of a GLM. However, the GLM linear regression does not necessarily assume that the errors are normally distributed. The errors can come from any distribution.\n",
    "\n",
    "The linear regression model is the following, where we specify a generic PDF for the error terms $u_i$.\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} + u_i \\quad\\text{where}\\quad u_i\\sim i.i.d.(0, \\sigma) $$\n",
    "\n",
    "In every case (not just linear regression), the $\\eta$ function is the linear portion of the linear regression, excluding the error terms.\n",
    "\n",
    "$$ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "In the regression equation, we know that the expected value of $Y_i$ is the linear predictor $\\eta_i$, which is a link function of the mean of the errors.\n",
    "\n",
    "$$ E[Y=y_i|X\\beta] = g(\\mu_i=0) = \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "As we did in Section 6 of the [Maximum Likelihood Estimation notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/MLE/MLest.ipynb), we estimate this GLM version of a linear regression by maximum likelihood. Note that this allows us to have more general assumptions on the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Logistic regression\n",
    "The logistic regression that we covered in the [Classifiers 1 notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/Classfcn1/KKNlogitLDA.ipynb) is also a special case of GLM. To characterize the logistic model as a GLM, we assume that the  probability of binary outcome $y_i=1$ given data $X_i$ and parameter vector $\\beta$ is $\\mu_i$.\n",
    "\n",
    "$$ Pr(y_i=1|X\\beta) = \\mu_i \\quad\\text{and}\\quad Pr(y_i=0|X\\beta) = 1 - \\mu_i $$\n",
    "\n",
    "The linear predictor is the same $\\eta_i$.\n",
    "\n",
    "$$ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "The link function $g(\\mu_i)$ is the logistic function.\n",
    "\n",
    "$$ g(\\mu_i) = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} $$\n",
    "\n",
    "So the probability of $Y=y_i\\in\\{0,1\\}$ is the Bernoulli distribution.\n",
    "\n",
    "$$ Pr(Y=y_i|X\\beta) = \\mu_i^{y_i}(1 - \\mu_i)^{y_i} $$\n",
    "\n",
    "The link function is called the log-odds function.\n",
    "\n",
    "$$ \\ln\\left(\\frac{\\mu_i}{1 - \\mu_i}\\right) = \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "As was shown in the [Classifiers 1 notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/Classfcn1/KKNlogitLDA.ipynb), we estimate the parameters of the logistic regression function by maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Multinomial logistic regression\n",
    "Multinomial logistic regression is a model for estimating or predicting the effects on independent variables (i.e., features, regressors) on a categorical dependent variable with more than two categories: $y_i\\in\\{1,2,... J\\}$. Multinomial logistic regression generalizes the logit model to a **polytomy** (more than 2 outcomes) dependent variable $y_i$.\n",
    "\n",
    "Let the linear predictor of features or independent variables or regressors be similar to the old $\\eta_i$. The difference is that we need a linear predictor for each possible type $j$. This means that we will estimate a different set of coefficients $\\beta_j$ in the linear predictor for each type $j$.\n",
    "\n",
    "$$ \\eta_{j,i} = \\beta_{j,0} + \\beta_{j,1} x_{1,i} + \\beta_{j,2} x_{2,i} + ... \\beta_{j,P} x_{P,i} $$\n",
    "\n",
    "We are interested in estimating or predicting the probability $\\mu_{j,i}$ that the observation is of class $j\\in\\{1,2,...J\\}$. We can model the probability of $y_i=j$ by a similar logistic function.\n",
    "\n",
    "$$ Pr(y_i=j|X\\beta) = \\mu_{j,i} = \\frac{e^{\\eta_{j,i}}}{1 + \\sum_{k=1}^{J-1} e^{\\eta_{k,i}}} = \\frac{e^{\\beta_{j,0} + \\beta_{j,1} x_{1,i} + \\beta_{j,2} x_{2,i} + ... \\beta_{j,P} x_{P,i}}}{1 + \\sum_{k=1}^{J-1} e^{\\beta_{k,0} + \\beta_{k,1} x_{1,i} + \\beta_{k,2} x_{2,i} + ... \\beta_{k,P} x_{P,i}}} \\quad\\text{for}\\quad j=1,2,...J-1 $$\n",
    "\n",
    "Note the $J-1$ limit of the sum in the denominator of the logistic function. What we are doing is estimating a logistic model for each type, where the cumulative sum in the denominator leaves out a baseline category. It would be redundant or multicollinear if we included it because of the restriction that the probabilities must sum to 1. So, without loss of generality, we name the baseline category as the last category $y_i=J$, and its probability is a function of all the other estimated logistic models for $j=1,2,...J-1$.\n",
    "\n",
    "$$ \\mu_{J,i} = 1 - \\sum_{j=1}^{J-1}\\mu_{j,i} \\quad\\text{because}\\quad \\sum_{j=1}^J \\mu_{j,i} = 1 $$\n",
    "\n",
    "Because the numerator in the multinomial logistic function does not include interaction terms between the coefficients of the other multinomial logistic equations, this estimation relies on the assumption of [independence of irrelevant alternatives](https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives). This assumption may be violated in some cases.\n",
    "\n",
    "With some algebraic manipulation, the link function of the multinomial logistic regression is also a log odds function, where the denominator probability is the baseline probability $\\mu_{J,i}$.\n",
    "\n",
    "$$ \\ln\\left(\\frac{\\mu_{j,i}}{\\mu_{J,i}}\\right) \\quad\\text{for}\\quad j=1,2,...J-1 $$\n",
    "\n",
    "We can estimate all the coefficients $\\beta_{j,p}$ of this model by maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Multinomial logistic example using Titanic data\n",
    "Let's go back to the Titanic dataset that we used in the logistic regression section of the [Classifiers notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/Classfcn1/KKNlogitLDA.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from pylab import rcParams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "sb.set_style('whitegrid')\n",
    "\n",
    "url = ('https://raw.githubusercontent.com/BigDataGal/Python-for-Data-Science/' +\n",
    "      'master/titanic-train.csv')\n",
    "titanic = pd.read_csv(url)\n",
    "titanic.columns = ['PassengerId','Survived','Pclass','Name','Sex','Age',\n",
    "                   'SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, instead of modeling the binary variable of survival, lets model the port of embarkation $Embarked_i$ as a function of passenger fare $Fare_i$ and age $Age_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic[['Embarked', 'Fare', 'Age', 'SibSp', 'Parch']]\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.countplot(x='Embarked', data=titanic, palette='hls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the three categories of port of embarkation are S=Southampton, C=Cherbourg, and Q=Queenstown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, let's just drop the observations for whom we don't have data on age or embarkation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.dropna(inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set S=Southampton as our reference category because it is big. We would not want to do this if we wanted to estimate how the two regressors affect the probability of embarked=S (Southampton). Let's first separate the data into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[['Fare', 'Age', 'SibSp', 'Parch']]\n",
    "y = titanic[['Embarked']]\n",
    "# This function train_test_split is from sklearn.cross_validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .5,\n",
    "                                                    random_state=25)\n",
    "\n",
    "MultLogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                solver='newton-cg')\n",
    "MultLogReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = MultLogReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Other GLM models in scikit-learn\n",
    "The scikit-learn package has a [number of other](http://scikit-learn.org/stable/modules/linear_model.html) GLM models: OLS, Ridge regression, Lasso, Multi-task Lasso, Elastic Net, Multi-task Elastic Net, Least Angle Regression, LARS Lasso, Orthogonal Matching Pursuit (OMP), Bayesian regression, Logistic regression (including multinomial logistic regression), Stochastic Gradient Descent (SGD), Perceptron, Passive Aggressive Algorithms, Robustness regression, and Polynomial regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
