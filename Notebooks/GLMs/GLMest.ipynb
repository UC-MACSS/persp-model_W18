{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models (MACS 30100)\n",
    "### by [Benjamin Soltoff](http://www.bensoltoff.com/) and [Richard W. Evans](https://sites.google.com/site/rickecon/), February 2018\n",
    "The code in this Jupyter notebook was written using Python 3.6. It uses data files `?`, and ? image files in the `images` folder in the same directory as this notebook. For the code to run properly, you will either need to have access to the internet or you should have the data file in the same folder as the Jupyter notebook file. Otherwise, you will have to change the respective lines of the code that read in the data to reflect the location of that data. Much of this content was taken from Dr. Benjamin Soltoff's notes [here](http://cfss.uchicago.edu/persp005_glm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The General Specification\n",
    "Generalized linear models (GLMs) are a general formulation of linear models for cases in which the error terms can have a general distribution (not necessarily the Normal distribution) and also cases in which the dependent variable is a categorical variable (non necessarily binary). GLMs are typically estimated using maximum likelihood estimation, though they can also be estimated using generalized method of moments as well as Bayesian estimation.\n",
    "\n",
    "A GLM consists of three components:\n",
    "1. A **random component** specifying the conditional distribution of the response variable, $Y_i$, given the values of the dependent (predictor) variables in the model. Typically these distributions are a member of the [exponential family](https://en.wikipedia.org/wiki/Exponential_family), a set of related probability distributions.\n",
    "\n",
    "2. A **linear predictor** that is a linear function of regressors.The regressors are prespecified functions of the explanatory variables. This is exactly like the form youâ€™ve seen for linear and logistic regression, because in fact linear and logistic regression are types of GLMs.\n",
    "\n",
    "$$ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "3. A **link function** $g(\\cdot)$, which transforms the expectation of the response variable $\\mu_i \\equiv E(Y_i)$ to the linear predictor.\n",
    "\n",
    "$$ g(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "* Because the link function must also be **invertible**, we can also write it as the following.\n",
    "\n",
    "$$ \\mu_i = g^{-1}(\\eta_i) = g^{-1}\\left(\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i}\\right) $$\n",
    "\n",
    "* The inverted link function is also known as the **mean function**. The purpose of the link function is to relate the linear predictor to the mean of the distribution function.\n",
    "\n",
    "The GLM approach allows us to embed a linear predictor, which is fairly easy to interpret, into a wide class of nonlinear models with a wide range of error distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Linear regression as a GLM\n",
    "The linear regression that we studied in the [Linear Regression Notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/LinRegress/LinRegress.ipynb) is the simplest special case of a GLM. However, the GLM linear regression does not necessarily assume that the errors are normally distributed. The errors can come from any distribution.\n",
    "\n",
    "The linear regression model is the following, where we specify a generic PDF for the error terms $u_i$.\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} + u_i \\quad\\text{where}\\quad u_i\\sim i.i.d.(0, \\sigma) $$\n",
    "\n",
    "In every case (not just linear regression), the $\\eta$ function is the linear portion of the linear regression, excluding the error terms.\n",
    "\n",
    "$$ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "In the regression equation, we know that the expected value of $Y_i$ is the linear predictor $\\eta_i$, which is a link function of the mean of the errors.\n",
    "\n",
    "$$ E[Y=y_i|X\\beta] = g(\\mu_i=0) = \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "As we did in Section 6 of the [Maximum Likelihood Estimation notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/MLE/MLest.ipynb), we estimate this GLM version of a linear regression by maximum likelihood. Note that this allows us to have more general assumptions on the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Logistic regression\n",
    "The logistic regression that we covered in the [Classifiers 1 notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/Classfcn1/KKNlogitLDA.ipynb) is also a special case of GLM. To characterize the logistic model as a GLM, we assume that the  probability of binary outcome $y_i=1$ given data $X_i$ and parameter vector $\\beta$ is $\\mu_i$.\n",
    "\n",
    "$$ Pr(y_i=1|X\\beta) = \\mu_i \\quad\\text{and}\\quad Pr(y_i=0|X\\beta) = 1 - \\mu_i $$\n",
    "\n",
    "The linear predictor is the same $\\eta_i$.\n",
    "\n",
    "$$ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "The link function $g(\\mu_i)$ is the logistic function.\n",
    "\n",
    "$$ g(\\mu_i) = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} $$\n",
    "\n",
    "So the probability of $Y=y_i\\in\\{0,1\\}$ is the Bernoulli distribution.\n",
    "\n",
    "$$ Pr(Y=y_i|X\\beta) = \\mu_i^{y_i}(1 - \\mu_i)^{y_i} $$\n",
    "\n",
    "The link function is called the log-odds function.\n",
    "\n",
    "$$ \\ln\\left(\\frac{\\mu_i}{1 - \\mu_i}\\right) = \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} $$\n",
    "\n",
    "As was shown in the [Classifiers 1 notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/Classfcn1/KKNlogitLDA.ipynb), we estimate the parameters of the logistic regression function by maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Multinomial logistic regression\n",
    "Multinomial logistic regression is a model for estimating or predicting the effects on independent variables (i.e., features, regressors) on a categorical dependent variable with more than two categories: $y_i\\in\\{1,2,... J\\}$. Multinomial logistic regression generalizes the logit model to a **polytomy** (more than 2 outcomes) dependent variable $y_i$.\n",
    "\n",
    "Let the linear predictor of features or independent variables or regressors be similar to the old $\\eta_i$. The difference is that we need a linear predictor for each possible type $j$. This means that we will estimate a different set of coefficients $\\beta_j$ in the linear predictor for each type $j$.\n",
    "\n",
    "$$ \\eta_{j,i} = \\beta_{j,0} + \\beta_{j,1} x_{1,i} + \\beta_{j,2} x_{2,i} + ... \\beta_{j,P} x_{P,i} $$\n",
    "\n",
    "We are interested in estimating or predicting the probability $\\mu_{j,i}$ that the observation is of class $j\\in\\{1,2,...J\\}$. We can model the probability of $y_i=j$ by a similar logistic function.\n",
    "\n",
    "$$ Pr(y_i=j|X\\beta) = \\mu_{j,i} = \\frac{e^{\\eta_{j,i}}}{1 + \\sum_{k=1}^{J-1} e^{\\eta_{k,i}}} = \\frac{e^{\\beta_{j,0} + \\beta_{j,1} x_{1,i} + \\beta_{j,2} x_{2,i} + ... \\beta_{j,P} x_{P,i}}}{1 + \\sum_{k=1}^{J-1} e^{\\beta_{k,0} + \\beta_{k,1} x_{1,i} + \\beta_{k,2} x_{2,i} + ... \\beta_{k,P} x_{P,i}}} \\quad\\text{for}\\quad j=1,2,...J-1 $$\n",
    "\n",
    "Note the $J-1$ limit of the sum in the denominator of the logistic function. What we are doing is estimating a logistic model for each type, where the cumulative sum in the denominator leaves out a baseline category. It would be redundant or multicollinear if we included it because of the restriction that the probabilities must sum to 1. So, without loss of generality, we name the baseline category as the last category $y_i=J$, and its probability is a function of all the other estimated logistic models for $j=1,2,...J-1$.\n",
    "\n",
    "$$ \\mu_{J,i} = 1 - \\sum_{j=1}^{J-1}\\mu_{j,i} \\quad\\text{because}\\quad \\sum_{j=1}^J \\mu_{j,i} = 1 $$\n",
    "\n",
    "Because the numerator in the multinomial logistic function does not include interaction terms between the coefficients of the other multinomial logistic equations, this estimation relies on the assumption of [independence of irrelevant alternatives](https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives). This assumption may be violated in some cases.\n",
    "\n",
    "With some algebraic manipulation, the link function of the multinomial logistic regression is also a log odds function, where the denominator probability is the baseline probability $\\mu_{J,i}$.\n",
    "\n",
    "$$ \\ln\\left(\\frac{\\mu_{j,i}}{\\mu_{J,i}}\\right) \\quad\\text{for}\\quad j=1,2,...J-1 $$\n",
    "\n",
    "We can estimate all the coefficients $\\beta_{j,p}$ of this model by maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Multinomial logistic example using Titanic data\n",
    "Let's go back to the Titanic dataset that we used in the logistic regression section of the [Classifiers notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/Classfcn1/KKNlogitLDA.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurencewarner/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from pylab import rcParams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "sb.set_style('whitegrid')\n",
    "\n",
    "url = ('https://raw.githubusercontent.com/BigDataGal/Python-for-Data-Science/' +\n",
    "      'master/titanic-train.csv')\n",
    "titanic = pd.read_csv(url)\n",
    "titanic.columns = ['PassengerId','Survived','Pclass','Name','Sex','Age',\n",
    "                   'SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Greenberg, Mr. Samuel</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PassengerId    Survived      Pclass                   Name   Sex  \\\n",
       "count    891.000000  891.000000  891.000000                    891   891   \n",
       "unique          NaN         NaN         NaN                    891     2   \n",
       "top             NaN         NaN         NaN  Greenberg, Mr. Samuel  male   \n",
       "freq            NaN         NaN         NaN                      1   577   \n",
       "mean     446.000000    0.383838    2.308642                    NaN   NaN   \n",
       "std      257.353842    0.486592    0.836071                    NaN   NaN   \n",
       "min        1.000000    0.000000    1.000000                    NaN   NaN   \n",
       "25%      223.500000    0.000000    2.000000                    NaN   NaN   \n",
       "50%      446.000000    0.000000    3.000000                    NaN   NaN   \n",
       "75%      668.500000    1.000000    3.000000                    NaN   NaN   \n",
       "max      891.000000    1.000000    3.000000                    NaN   NaN   \n",
       "\n",
       "               Age       SibSp       Parch    Ticket        Fare        Cabin  \\\n",
       "count   714.000000  891.000000  891.000000       891  891.000000          204   \n",
       "unique         NaN         NaN         NaN       681         NaN          147   \n",
       "top            NaN         NaN         NaN  CA. 2343         NaN  C23 C25 C27   \n",
       "freq           NaN         NaN         NaN         7         NaN            4   \n",
       "mean     29.699118    0.523008    0.381594       NaN   32.204208          NaN   \n",
       "std      14.526497    1.102743    0.806057       NaN   49.693429          NaN   \n",
       "min       0.420000    0.000000    0.000000       NaN    0.000000          NaN   \n",
       "25%      20.125000    0.000000    0.000000       NaN    7.910400          NaN   \n",
       "50%      28.000000    0.000000    0.000000       NaN   14.454200          NaN   \n",
       "75%      38.000000    1.000000    0.000000       NaN   31.000000          NaN   \n",
       "max      80.000000    8.000000    6.000000       NaN  512.329200          NaN   \n",
       "\n",
       "       Embarked  \n",
       "count       889  \n",
       "unique        3  \n",
       "top           S  \n",
       "freq        644  \n",
       "mean        NaN  \n",
       "std         NaN  \n",
       "min         NaN  \n",
       "25%         NaN  \n",
       "50%         NaN  \n",
       "75%         NaN  \n",
       "max         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, instead of modeling the binary variable of survival, lets model the port of embarkation $Embarked_i$ as a function of passenger fare $Fare_i$ and age $Age_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Embarked     Fare   Age\n",
       "0        S   7.2500  22.0\n",
       "1        C  71.2833  38.0\n",
       "2        S   7.9250  26.0\n",
       "3        S  53.1000  35.0\n",
       "4        S   8.0500  35.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = titanic[['Embarked', 'Fare', 'Age']]\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb.countplot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11d4018d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAHaCAYAAADGytccAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d413dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(y='Embarked', data=titanic, palette='hls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the three categories of port of embarkation are S=Southampton, C=Cherbourg, and Q=Queenstown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embarked      2\n",
       "Fare          0\n",
       "Age         177\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 3 columns):\n",
      "Embarked    889 non-null object\n",
      "Fare        891 non-null float64\n",
      "Age         714 non-null float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 21.0+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, let's just drop the observations for whom we don't have data on age or embarkation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Embarked     Fare   Age\n",
       "0        S   7.2500  22.0\n",
       "1        C  71.2833  38.0\n",
       "2        S   7.9250  26.0\n",
       "3        S  53.1000  35.0\n",
       "4        S   8.0500  35.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.dropna(inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11d290cf8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAHaCAYAAAA63/zWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGmRJREFUeJzt3X1snXX9//HXaceAtStzzJs0bPtu\nIDpvkGyVadgGeDcTY5BksA4CKChqpDojOIHRAUE2nBQFHAQTNU5AnYAaoyaKwNzAQWrGzZiIN9zo\nQJBBthbY2M75/cGX/r4TGFVPP6eUxyNZsnNdvT59n+XK8ux1zrlaqdVqtQAAMKSaGj0AAMCrgegC\nAChAdAEAFCC6AAAKEF0AAAWMavQAL6e3t7fRIwAADNqMGTNedPuwj67kpYcHABhOdnexyMuLAAAF\niC4AgAJEFwBAAaILAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaILAKAA0QUA\nUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaMaPUBJM07/bqNHYITpXX5Co0cA4BXClS4A\ngAJEFwBAAaILAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaILAKAA0QUAUIDo\nAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaILAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAF\niC4AgAJEFwBAAaILAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaILAKAA0QUA\nUIDoAgAoYNRQLfyRj3wkY8eOTZLst99+mT9/fr785S+nubk5s2bNyqmnnppqtZpzzjkn9957b0aP\nHp3zzz8/kydPHqqRAAAaZkiia9u2bUmSlStXDmw78sgjc+mll2bixIk55ZRTsmHDhvz973/P9u3b\n84Mf/CDr16/PsmXLcvnllw/FSAAADTUk0fWHP/whTz/9dE466aTs2LEjXV1d2b59eyZNmpQkmTVr\nVm699dY89thjmT17dpLk4IMPzt133/2i623cuHEoxoT/mnMTgMEakujaa6+9cvLJJ+foo4/O/fff\nn0984hNpa2sb2N/S0pKHHnoofX19aW1tHdje3NycHTt2ZNSoXceaNm1anSa7vU7rwHPqd24CMBL0\n9va+5L4hia4pU6Zk8uTJqVQqmTJlSsaOHZsnn3xyYH9/f3/a2tryzDPPpL+/f2B7tVp9QXABAIwE\nQ/LpxR/96EdZtmxZkuQf//hHnn766YwZMyYPPvhgarVa1qxZk46OjkyfPj2rV69Okqxfvz4HHnjg\nUIwDANBwQ3JZad68eTnjjDOyYMGCVCqVXHDBBWlqasppp52WnTt3ZtasWXnHO96Rt7/97Vm7dm06\nOztTq9VywQUXDMU4AAANNyTRNXr06Fx00UUv2P7DH/5wl8dNTU0577zzhmIEAIBhxc1RAQAKEF0A\nAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEABogsAoADR\nBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6AAAK\nEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEABogsA\noADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6\nAAAKEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEAB\nogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAoYsuh5//PEcdthh+fOf/5wHHnggCxYs\nyLHHHpslS5akWq0mSS677LLMmzcvnZ2dufPOO4dqFACAhhuS6Hr22WfT3d2dvfbaK0mydOnSLFy4\nMFdffXVqtVpuuOGGbNiwIbfddltWrVqVnp6enHvuuUMxCgDAsDBqKBa98MIL09nZmSuvvDJJsmHD\nhhxyyCFJkjlz5mTt2rWZMmVKZs2alUqlkvb29uzcuTObN2/O+PHjX7Dexo0bh2JM+K85NwEYrLpH\n13XXXZfx48dn9uzZA9FVq9VSqVSSJC0tLdm6dWv6+voybty4geOe3/5i0TVt2rQ6TXd7ndaB59Tv\n3ARgJOjt7X3JfXWPrmuvvTaVSiW33nprNm7cmEWLFmXz5s0D+/v7+9PW1pbW1tb09/fvsn3s2LH1\nHgcAYFio+3u6rrrqqnzve9/LypUrM23atFx44YWZM2dO1q1blyRZvXp1Ojo6Mn369KxZsybVajWb\nNm1KtVp90atcAAAjwZC8p+tfLVq0KGeffXZ6enoyderUzJ07N83Nzeno6Mj8+fNTrVbT3d1dYhQA\ngIao1Gq1WqOH2J3e3t7MmDGjLmvNOP27dVkHnte7/IRGjwDAMLK7bnFzVACAAkQXAEABogsAoADR\nBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6AAAK\nEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEABogsA\noADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6\nAAAKEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEAB\nogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEA\nFCC6AAAKEF0AAAWILgCAAkQXAEABg4quVatW7fL4u9/97m6/fufOnTnjjDPS2dmZ4447Lg8++GAe\neOCBLFiwIMcee2yWLFmSarWaJLnssssyb968dHZ25s477/wPnwYAwPA2anc7f/azn+U3v/lN1q1b\nl9/97ndJnguq++67LyeccMJLHnfjjTcmSb7//e9n3bp1Wbp0aWq1WhYuXJiZM2emu7s7N9xwQ9rb\n23Pbbbdl1apVefjhh9PV1ZVrr722jk8PAGB42G10zZ49O6997Wvz5JNPZv78+UmSpqamTJw4cbeL\nvu9978vhhx+eJNm0aVMmTJiQm266KYccckiSZM6cOVm7dm2mTJmSWbNmpVKppL29PTt37szmzZsz\nfvz4Ojw1AIDhY7fRtc8++2TmzJmZOXNmHn/88Wzbti3Jc1e7XnbhUaOyaNGi/OpXv8oll1ySG2+8\nMZVKJUnS0tKSrVu3pq+vL+PGjRs45vnt/xpdGzdu/LefGJTg3ARgsHYbXc8799xzc/PNN+d1r3td\narVaKpVKvv/977/scRdeeGFOO+20HHPMMQPBliT9/f1pa2tLa2tr+vv7d9k+duzYF6wzbdq0wYw5\nCLfXaR14Tv3OTQBGgt7e3pfcN6jouuOOO/LrX/86TU2D+7Djj3/84/zjH//IJz/5yey9996pVCp5\n29velnXr1mXmzJlZvXp13vWud2XSpElZvnx5Tj755DzyyCOpVqteWgQARqRBRdfkyZOzbdu27L33\n3oNa9AMf+EDOOOOMHHfccdmxY0fOPPPM7L///jn77LPT09OTqVOnZu7cuWlubk5HR0fmz5+farWa\n7u7u/+rJAAAMV5VarVZ7uS/q7OzM/fffn8mTJz930CBfXqyH3t7ezJgxoy5rzTh997e6gH9X7/KX\n/hQvAK8+u+uWQV3puuiii+o6EADAq82gouv6669/wbZTTz217sMAAIxUg4quCRMmJElqtVruueee\ngbvJAwAwOIOKrs7Ozl0ef/zjHx+SYQAARqpBRddf//rXgb8/9thjefjhh4dsIACAkWhQ0fV/b+Ww\n55575otf/OKQDQQAMBINKrpWrlyZJ554Ig899FD2228/NzAFAPg3DeoW87/4xS/S2dmZK664IvPn\nz89PfvKToZ4LAGBEGdSVru985zu57rrr0tLSkr6+vpx44ok58sgjh3o2AIARY1BXuiqVSlpaWpIk\nra2t2XPPPYd0KACAkWZQV7omTZqUZcuWpaOjI729vZk0adJQzwUAMKIM6krXMccck3322Se33HJL\nrrvuuhx33HFDPRcAwIgyqOhatmxZ3v/+96e7uzs/+tGPsmzZsqGeCwBgRBlUdI0aNSoHHHBAkmTi\nxIlpahrUYQAA/K9Bvaervb09PT09Ofjgg3PnnXfmda973VDPBQAwogzqktXSpUszfvz43HzzzRk/\nfnyWLl061HMBAIwog7rSteeee+ajH/3oEI8CADByeXMWAEABogsAoADRBQBQgOgCAChAdAEAFCC6\nAAAKEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEAB\nogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEA\nFCC6AAAKEF0AAAWILgCAAkQXAEABogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQX\nAEABogsAoADRBQBQgOgCAChAdAEAFCC6AAAKEF0AAAWILgCAAkQXAEABogsAoIBR9V7w2WefzZln\nnpm///3v2b59ez796U/ngAMOyJe+9KVUKpW88Y1vzJIlS9LU1JTLLrssN910U0aNGpUzzzwzBx10\nUL3HAQAYFuoeXT/96U8zbty4LF++PE888USOOuqovPnNb87ChQszc+bMdHd354Ybbkh7e3tuu+22\nrFq1Kg8//HC6urpy7bXX1nscAIBhoe7R9cEPfjBz584deNzc3JwNGzbkkEMOSZLMmTMna9euzZQp\nUzJr1qxUKpW0t7dn586d2bx5c8aPH1/vkQAAGq7u0dXS0pIk6evry2c/+9ksXLgwF154YSqVysD+\nrVu3pq+vL+PGjdvluK1bt75odG3cuLHeY0JdODcBGKy6R1eSPPzww/nMZz6TY489Nh/+8IezfPny\ngX39/f1pa2tLa2tr+vv7d9k+duzYF11v2rRpdZrs9jqtA8+p37kJwEjQ29v7kvvq/unFf/7znznp\npJNy+umnZ968eUmSt7zlLVm3bl2SZPXq1eno6Mj06dOzZs2aVKvVbNq0KdVq1UuLAMCIVfcrXVdc\ncUW2bNmSFStWZMWKFUmSs846K+eff356enoyderUzJ07N83Nzeno6Mj8+fNTrVbT3d1d71EAAIaN\nSq1WqzV6iN3p7e3NjBkz6rLWjNO/W5d14Hm9y09o9AgADCO76xY3RwUAKEB0AQAUILoAAAoQXQAA\nBYguAIACRBcAQAGiCwCgANEFAFCA6AIAKEB0AQAUILoAAAoQXQAABYguAIACRBcAQAGiCwCgANEF\nAFCA6AIAKEB0AQAUILoAAAoQXQAABYguAIACRBcAQAGiCwCgANEFAFCA6AIAKEB0AQAUILoAAAoQ\nXQAABYguAIACRBcAQAGiCwCgANEFAFCA6AIAKEB0AQAUILoAAAoQXQAABYguAIACRBcAQAGiCwCg\nANEFAFCA6AIAKEB0AQAUILoAAAoQXQAABYguAIACRBcAQAGiCwCggFGNHgCorwfPe3ujR2CEmdR9\nV6NHgBHBlS4AgAJEFwBAAaILAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaIL\nAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaILAKCAIYuuO+64I8cff3yS5IEH\nHsiCBQty7LHHZsmSJalWq0mSyy67LPPmzUtnZ2fuvPPOoRoFAKDhhiS6vvnNb2bx4sXZtm1bkmTp\n0qVZuHBhrr766tRqtdxwww3ZsGFDbrvttqxatSo9PT0599xzh2IUAIBhYUiia9KkSbn00ksHHm/Y\nsCGHHHJIkmTOnDm55ZZb0tvbm1mzZqVSqaS9vT07d+7M5s2bh2IcAICGGzUUi86dOzd/+9vfBh7X\narVUKpUkSUtLS7Zu3Zq+vr6MGzdu4Gue3z5+/PgXrLdx48ahGBP+a8Px3Gxp9ACMOMPxPIdXoiGJ\nrn/V1PT/L6j19/enra0tra2t6e/v32X72LFjX/T4adOm1WmS2+u0Djynfudm/TzY6AEYcYbjeQ7D\nVW9v70vuK/Lpxbe85S1Zt25dkmT16tXp6OjI9OnTs2bNmlSr1WzatCnVavVFr3IBAIwERa50LVq0\nKGeffXZ6enoyderUzJ07N83Nzeno6Mj8+fNTrVbT3d1dYhQAgIYYsujab7/98sMf/jBJMmXKlHzv\ne997wdd0dXWlq6trqEYAABg23BwVAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBA\nAaILAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaILAKAA0QUAUIDoAgAoQHQB\nABQgugAAChBdAAAFiC4AgAJEFwBAAaILAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJE\nFwBAAaILAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaILAKAA0QUAUIDoAgAo\nQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaMaPQAA/LsOvfTQRo/ACLO2a+2Qfw9XugAAChBdAAAF\niC4AgAJEFwBAAaILAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAFiC4AgAJEFwBAAaILAKAA0QUA\nUIDoAgAoQHQBABQgugAAChBdAAAFjGr0ANVqNeecc07uvffejB49Oueff34mT57c6LEAAOqq4Ve6\nfv3rX2f79u35wQ9+kC984QtZtmxZo0cCAKi7Sq1WqzVygKVLl+aggw7Khz70oSTJ7Nmz89vf/nZg\nf29vb6NGAwD4t82YMeNFtzf85cW+vr60trYOPG5ubs6OHTsyatRzo73U4AAAryQNf3mxtbU1/f39\nA4+r1epAcAEAjBQNj67p06dn9erVSZL169fnwAMPbPBEAAD11/D3dD3/6cU//vGPqdVqueCCC7L/\n/vs3ciQAgLpreHQxfFx55ZW55ZZb0tTUlEqlks9//vN529ve1uixoK7uu+++LF++PE8//XSeeuqp\nHHbYYenq6kqlUmn0aFA3d999d3p6evL000+nVqtl5syZ+cxnPpPRo0c3erRXNdFFkuRPf/pTFi9e\nnGuuuSaVSiUbN27MokWL8tOf/rTRo0HdbNmyJccdd1wuvfTS/M///E927tyZz33uczn00EOzYMGC\nRo8HdfHII4/kYx/7WFasWJEpU6akVqvlG9/4Rh5//PEsWbKk0eO9qokukiSbN2/ORz7ykXR1dWXO\nnDl5/etfn+3bt/upiBHl+uuvz4YNG7J48eKBbf39/dljjz2c64wYV1xxRUaPHp2TTjppYFutVst7\n3/ve/PznP89ee+3VwOle3Rr+RnqGh/Hjx+fyyy/P73//+8yfPz8f/OAHc+ONNzZ6LKirRx99NBMn\nTtxlW0tLi+BiRNm0adMLzvNKpZIJEybksccea9BUJMPgPl0MDw888EBaW1uzdOnSJMldd92VU045\nJTNnzsy4ceMaPB3UR3t7e+65555dtj300EN55JFH8s53vrNBU0F9tbe356GHHtplW7VazaZNm7Lv\nvvs2aCoSV7r4X/fee2/OOeecbNu2LUkyZcqUjB07Ns3NzQ2eDOrniCOOyG9/+9s8+OCDSZJnn302\ny5Ytyx//+McGTwb1c+SRR2bVqlW5//77s2XLlpx00kk566yzcsQRR2TMmDGNHu9VzXu6GHD55Zfn\nF7/4RcaMGZNarZZPfOITed/73tfosaCu7r777nzlK19JrVZLf39/jjjiiJx66qk+vciIcvfdd+fi\niy9Of39/nnnmmUyYMCETJkzIl770Ja9eNJDoAoBXgT/84Q+ZOHFiWlpaGj3Kq5boAgAowHu6AAAK\nEF0AAAWILgCAAkQXAEABogsY1tatW5d3v/vdOf744wf+fPazn33Z46677rp89atf/Y++53ve856B\ne9YN1rZt2/Ke97znP/p+wKuDO9IDw9673vWuXHzxxY0eA+C/IrqAV6Tjjz8+b3rTm3LfffdlzJgx\n6ejoyJo1a7Jly5Z861vfSpKsX78+J554Yvr6+tLV1ZXDDz88v/zlL3PVVVcNrPP1r3899913X776\n1a9mjz32yDHHHDOw75prrsnatWvT09OT9evX5+KLL05zc3MmTpyY8847L9u3b89pp52WLVu2ZNKk\nScX/DYBXFtEFDHu/+93vcvzxxw88Puyww5IkBx10UBYvXpyTTz45e+21V7797W9n0aJFuf3225Mk\ne++9d6688sps3rw5Rx99dObMmZP7778/V155Zfbee+90d3dnzZo1ef3rX59t27Zl1apVSZJLLrkk\nK1euzMaNG/P1r389TU1NOfvss3P11Vdn3333zde+9rVcf/312b59ew488MB8/vOfzx133JF169aV\n/8cBXjFEFzDsvdjLizfffHPe+ta3Jkna2tpywAEHDPz9+fdjzZgxI5VKJfvuu2/Gjh2bJ598Mvvu\nu28WLVqUlpaW/OUvf8nBBx+c5LnfN/p/3XrrrWlubk5zc3Mef/zxPProo1m4cGGS5Jlnnsmhhx6a\nJ554IrNnz06SvOMd78ioUf5LBV6aN9IDI9Zdd92VJHnsscfy1FNPZY899sgll1ySiy++OOeff372\n3HPPPP9LOZqadv3vcMWKFWlra8s111yT17zmNXnDG96QFStWZOXKlfnUpz6VmTNnZurUqVm/fn2S\n5J577smOHTvKPkHgFcWPZcCw968vLybPXW16Oc8880xOOOGEPPXUUznvvPPS2tqa6dOn56ijjsqY\nMWPS1taWRx99NPvtt9+LHr948eIcffTRefe7352zzjorp5xySmq1WlpaWvKVr3wl73znO3PGGWdk\nwYIFmTp1avbYY4+6PF9gZPK7FwEACvDyIgBAAaILAKAA0QUAUIDoAgAoQHQBABQgugAAChBdAAAF\n/D+/l8OJVEIbKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d144dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(data = titanic, x = 'Embarked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb.countplot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set S=Southampton as our reference category because it is big. We would not want to do this if we wanted to estimate how the two regressors affect the probability of embarked=S (Southampton). Let's first separate the data into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurencewarner/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = titanic[['Fare', 'Age']]\n",
    "y = titanic[['Embarked']]\n",
    "# This function train_test_split is from sklearn.cross_validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .5,\n",
    "                                                    random_state=25)\n",
    "\n",
    "MultLogReg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "MultLogReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l= [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S',\n",
       "       'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C',\n",
       "       'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "       'S', 'S', 'S', 'S', 'S'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = MultLogReg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7,   0,  52],\n",
       "       [  0,   0,  16],\n",
       "       [ 14,   0, 267]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          C       0.33      0.12      0.17        59\n",
      "          Q       0.00      0.00      0.00        16\n",
      "          S       0.80      0.95      0.87       281\n",
      "\n",
      "avg / total       0.68      0.77      0.71       356\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurencewarner/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Other GLM models in scikit-learn\n",
    "The scikit-learn package has a [number of other](http://scikit-learn.org/stable/modules/linear_model.html) GLM models: OLS, Ridge regression, Lasso, Multi-task Lasso, Elastic Net, Multi-task Elastic Net, Least Angle Regression, LARS Lasso, Orthogonal Matching Pursuit (OMP), Bayesian regression, Logistic regression (including multinomial logistic regression), Stochastic Gradient Descent (SGD), Perceptron, Passive Aggressive Algorithms, Robustness regression, and Polynomial regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
