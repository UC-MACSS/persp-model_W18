{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (MACS 30100)\n",
    "### by [Richard W. Evans](https://sites.google.com/site/rickecon/), January 2018\n",
    "This Jupyter notebook is an adaptation of the QuantEcon Lecture, \"[Linear Regression in Python](https://lectures.quantecon.org/py/ols.html)\" by [Natasha Watkins](https://github.com/natashawatkins). The code in this Jupyter notebook was written using Python 3.6. It uses data files `maketable1.dta`, `maketable2.dta`, and `maketable4.dta`. For the code to run properly, you will either need to have access to the internet or you should have the data file in the same folder as the Jupyter notebook file. Otherwise, you will have to change the respective lines of the code that read in the data to reflect the location of that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. An example analysis: Acemoglu, et al (2001)\n",
    "The research question of the paper [AJR01] is to determine whether or not differences in institutions can help to explain observed economic outcomes. How do we measure institutional differences and economic outcomes? In this paper:\n",
    "* economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates,\n",
    "* institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the [Political Risk Serivces Group](https://www.prsgroup.com/).\n",
    "\n",
    "These variables and other data used in the paper are available for download on [Daron Acemoglu’s webpage](https://economics.mit.edu/faculty/acemoglu/data/ajr2001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial data description\n",
    "We will use pandas’ `.read_stata()` function to read in data contained in the `.dta` files to dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_stata('https://github.com/QuantEcon/QuantEcon.lectures.code/' +\n",
    "                    'raw/master/ols/maketable1.dta')\n",
    "# The pandas DataFrame.head() method returns the first N rows of a DataFrame\n",
    "# with column headings and index numbers. The default is N=5.\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use a scatterplot to see whether any obvious relationship exists between GDP per capita and the protection against expropriation index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "df1.plot(x='avexpr', y='logpgp95', kind='scatter')\n",
    "plt.xlabel(r'Average Expropriation Protection 1985-95')\n",
    "plt.ylabel(r'Log GDP per capita, PPP, 1995')\n",
    "plt.xlim((3.2, 10.5))\n",
    "plt.ylim((5.9, 10.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows a fairly strong positive relationship between protection against expropriation and log GDP per capita. Specifically, if higher protection against expropriation is a measure of institutional quality, then better institutions appear to be positively correlated with better economic outcomes (higher GDP per capita)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running a linear regression\n",
    "Given the plot above, choosing a linear model to describe this relationship seems like a reasonable assumption.\n",
    "\n",
    "We can write a model as,\n",
    "$$logpgp95_i = \\beta_0 + \\beta_1 avexpr_i + u_i $$\n",
    "where:\n",
    "* $\\beta_0$ is the intercept of the linear trend line on the $y$-axis\n",
    "* $\\beta_1$ is the slope of the linear trend line, representing the marginal effect of protection against risk on log GDP per capita\n",
    "* $u_i$ is a random error term (deviations of observations from the linear trend due to factors not included in the model)\n",
    "\n",
    "Visually, this linear model involves choosing a straight line that best fits the data, as in the following plot (Figure 2 in [AJR01])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dropping NA's is required to use numpy's polyfit\n",
    "df1_subset = df1.dropna(subset=['logpgp95', 'avexpr'])\n",
    "\n",
    "# Use only 'base sample' for plotting purposes (smaller sample)\n",
    "df1_subset = df1_subset[df1_subset['baseco'] == 1]\n",
    "\n",
    "X = df1_subset['avexpr']\n",
    "y = df1_subset['logpgp95']\n",
    "labels = df1_subset['shortnam']\n",
    "\n",
    "# Replace markers with country labels\n",
    "plt.scatter(X, y, marker='')\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (X.iloc[i], y.iloc[i]))\n",
    "\n",
    "# Fit a linear trend line\n",
    "plt.plot(np.unique(X),\n",
    "         np.poly1d(np.polyfit(X, y, 1))(np.unique(X)),\n",
    "         color='black')\n",
    "\n",
    "plt.xlim([3.3,10.5])\n",
    "plt.ylim([4,10.5])\n",
    "plt.xlabel('Average Expropriation Protection 1985-95')\n",
    "plt.ylabel('Log GDP per capita, PPP, 1995')\n",
    "plt.xlim((3.2, 10.5))\n",
    "plt.ylim((5.9, 10.5))\n",
    "plt.title('Figure 2: OLS relationship between expropriation risk and income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common technique to estimate the parameters (ββ‘s) of the linear model is Ordinary Least Squares (OLS). As the name implies, an OLS model is solved by finding the parameters that minimize the sum of squared residuals.\n",
    "$$ \\hat{\\beta}_{OLS} = \\beta : \\quad \\min_{\\beta}\\: u(X|\\beta_0,\\beta_1)^T \\: u(X|\\beta_0,\\beta_1) $$\n",
    "\n",
    "where $\\hat{u}_i$ is the difference between the dependent variable observation $logpgp95_i$ and the predicted value of the dependent variable $\\beta_0 + \\beta_1 avexpr_i$. To estimate the constant term $\\beta_0$, we need to add a column of 1’s to our dataset (consider the equation if $\\beta_0$ was replaced with $\\beta_0 x_i$ where $x_i=1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1['const'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct our model using the `statsmodels` module and the `OLS` method. We will use `pandas` DataFrames with `statsmodels`. However, standard arrays can also be used as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the warning that comes up. Most issue boards suggest that this module is not\n",
    "# really depreciated\n",
    "import statsmodels.api as sm\n",
    "\n",
    "reg1 = sm.OLS(endog=df1['logpgp95'], exog=df1[['const', 'avexpr']], missing='drop')\n",
    "type(reg1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have simply constructed our model. The `statsmodels.regression.linear_model.OLS` is simply an object specifying dependent and independent variables, as well as instructions about what to do with missing data. We need to use the `.fit()` method to obtain OLS parameter estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$. This method calculates the OLS coefficients according to the minimization problem,\n",
    "$$ \\hat{\\beta}_{OLS} = \\beta : \\quad \\min_{\\beta}\\: u(X|\\beta_0,\\beta_1)^T \\: u(X|\\beta_0,\\beta_1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = reg1.fit()\n",
    "type(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the fitted regression model stored in `results` (see [statsmodels.regression.linear_model.RegressionResults](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html)). The `results` from the `reg1.fit()` command is a regression results object with a lot of information, similar to the results object of the `scipy.optimize.minimize()` function we worked with in the MLE and GMM notebooks.\n",
    "\n",
    "To view the OLS regression results, we can call the `.summary()` method.\n",
    "\n",
    "[Note that an observation was mistakenly dropped from the results in the original paper (see the note located in maketable2.do from Acemoglu’s webpage), and thus the coefficients differ slightly.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpreting results and output\n",
    "From our results, we see that:\n",
    "* the intercept $\\hat{\\beta}_0=4.63$ (interpretation?)\n",
    "* the slope $\\hat{\\beta}_1=0.53$ (interpretation?)\n",
    "* the positive $\\hat{\\beta}_1>0$ parameter estimate implies that protection from expropriation has a positive effect on economic outcomes, as we saw in the figure.\n",
    "* How would you quantitatively interpret the $\\hat{\\beta}_1$ coefficient?\n",
    "* What do the standard errors on the coefficients tell you?\n",
    "* The p-value of 0.000 for $\\hat{\\beta}_1$ implies that the effect of institutions on GDP is statistically significant (using $p < 0.05$ as a rejection rule)\n",
    "* The R-squared value of 0.611 indicates that around 61% of variation in log GDP per capita is explained by protection against expropriation\n",
    "\n",
    "Using our parameter estimates, we can now write our estimated relationship as\n",
    "$$ \\hat{logpgp95}_i = 4.63 + 0.53 avexpr_i $$\n",
    "\n",
    "This equation describes the line that best fits our data, as shown in Figure 2. We can use this equation to predict the level of log GDP per capita for a value of the index of expropriation protection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Analysis of variance (ANOVA) output\n",
    "The results `.summary()` method provides a lot of regression output. And the `.RegressionResults` object has much more as evidenced in the help page [statsmodels.regression.linear_model.RegressionResults](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html).\n",
    "\n",
    "* The `Df Residuals: 109` displays the degrees of freedom from the residual variance calculation. This equals the number of observations minus the number of regression coefficients, `N-p=111-2`. This is accessed with `results.df_resid`.\n",
    "* The `Df Model: 1` displays the degrees of freedom from the model variance calculation or from the regressors. This equals the number of regression coefficients minus one, `p-1=2-1`. This is accessed with `results.df_model`.\n",
    "* One can specify robust standard errors in their regression. The robust option is specified in the `.fit()` command. You can specify three different types of robust standard errors using the `.fit(cov_type='HC1')`, `.fit(cov_type='HC2')`, or `.fit(cov_type='HC3')` options.\n",
    "* You can do clustered standard errors if you have groups labeled in a variable called `mygroups` by using the `.fit(cov_type='cluster', cov_kwds={'groups': mygroups})`.\n",
    "* R-squared is a measure of fit of the overall model. It is $R^2=1 - SSR/SST$ where $SST$ is the total variance of the dependent variable (total sum of squares), and $SSR$ is the sum of squared residuals (variance of the residuals). Another expresion is the sum of squared predicted values over the total sum of squares $R^2= SSM/SST$, where $SSM$ is the sum of squared predicted values. This is accessed with `results.rsquared`.\n",
    "* Adjusted R-squared is a measure of fit of the overall model that penalizes extra regressors. A property of the R-squared in the previous bullet is that it always increases as you add more explanatory variables. This is accessed with `results.rsquared_adj`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 F-test and log likelihood\n",
    "* The F-statistic is the statistic from an F-test of the joint hypothesis that all the coefficients are equal to zero. The value of the F-statistic is distributed according to the F-distribution $F(d1,d2)$, where $d1=p-1$ and $d2=N-p$.\n",
    "* The Prob (F-statistic) is the probability that the null hypothesis of all the coefficients being zero is true. In this case, it is really small.\n",
    "* Log-likelihood is the sum of the log pdf values of the errors given their being normally distributed with mean 0 and standard deviation implied by the OLS estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Inference on individual parameters\n",
    "* The estimated coefficients of the linear regression are reported in the `results.params` vector object (pandas Series).\n",
    "* The standard error on each estimated coefficient is reported in the summary results column entitled `std err`. These standard errors are reported in the `results.bse` vector object (pandas Series).\n",
    "* The \"t\" column is the $t$ test statistic. It is the value in the support of the students-T distribution that is equivalent to the estimated coefficient if the null-hypothesis were true that the estimated coefficient were 0.\n",
    "* The reported p-value is the probability of a two-sided t-test that gives the probability that the estimated coefficient is greater than its estimated value if the true value were 0. A more intuitive interpretation is the probability of seeing that estimated value if the null hypothesis were true. We usually reject the null hypothesis if the p-value is lower than 0.05.\n",
    "* The summary results report the 95% two-sided confidence interval for the estimated value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predicted values\n",
    "We can obtain an array of predicted $logpgp95_i$ for every value of $avexpr_i$ in our dataset by calling `.predict()` on our results. Let's first get the predicted value for the average country in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_expr = np.mean(df1['avexpr'])\n",
    "mean_expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```none\n",
    "6.515625\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_logpdp95 = 4.63 + 0.53 * 7.07\n",
    "predicted_logpdp95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easier (and more accurate) way to obtain this result is to use `.predict()` and set $constant=1$ and $avexpr_i=$ `mean_expr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.predict(exog=[1, mean_expr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the predicted values against $avexpr_i$ shows that the predicted values lie along the linear line that we fitted below in Figure 3. The observed values of $logpgp95_i$ are also plotted for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing observations from whole sample\n",
    "\n",
    "df1_plot = df1.dropna(subset=['logpgp95', 'avexpr'])\n",
    "\n",
    "# Plot predicted values. alpha is a blending value between 0 (transparent) and 1 (opaque)\n",
    "plt.scatter(df1_plot['avexpr'], results.predict(), alpha=0.5, label='predicted')\n",
    "\n",
    "# Plot observed values\n",
    "plt.scatter(df1_plot['avexpr'], df1_plot['logpgp95'], alpha=0.5, label='observed')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Figure 3. OLS predicted values')\n",
    "plt.xlabel('Average Expropriation Protection 1985-95')\n",
    "plt.ylabel('Log GDP per capita, PPP, 1995')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extending the linear regression model\n",
    "So far we have only accounted for institutions affecting economic performance. Almost certainly there are numerous other factors affecting GDP that are not included in our model. Leaving out variables that affect $logpgp95_i$ will result in **omitted variable bias**, yielding biased and inconsistent parameter estimates. We can extend our bivariate regression model to a **multivariate regression model** by adding in other factors that may affect $logpgp95_i$.\n",
    "\n",
    "[AJR01] consider other factors such as:\n",
    "* the effect of climate on economic outcomes; latitude is used to proxy this\n",
    "* differences that affect both economic performance and institutions, eg. cultural, historical, etc.; controlled for with the use of continent dummies\n",
    "\n",
    "Let’s estimate some of the extended models considered in the paper (Table 2) using data from `maketable2.dta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_stata('https://github.com/QuantEcon/QuantEcon.lectures.code/' +\n",
    "                    'raw/master/ols/maketable2.dta')\n",
    "\n",
    "# Add constant term to dataset\n",
    "df2['const'] = 1\n",
    "\n",
    "# Create lists of variables to be used in each regression\n",
    "X1 = ['const', 'avexpr']\n",
    "X2 = ['const', 'avexpr', 'lat_abst']\n",
    "X3 = ['const', 'avexpr', 'lat_abst', 'asia', 'africa', 'other']\n",
    "\n",
    "# Estimate an OLS regression for each set of variables\n",
    "reg1 = sm.OLS(df2['logpgp95'], df2[X1], missing='drop').fit()\n",
    "reg2 = sm.OLS(df2['logpgp95'], df2[X2], missing='drop').fit()\n",
    "reg3 = sm.OLS(df2['logpgp95'], df2[X3], missing='drop').fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted our model, we will use the `summary_col` module to display the results in a single table (model numbers correspond to those in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "info_dict={'R-squared' : lambda x: \"{:.2f}\".format(x.rsquared),\n",
    "           'No. observations' : lambda x: \"{0:d}\".format(int(x.nobs))}\n",
    "\n",
    "results_table = summary_col(results=[reg1,reg2,reg3],\n",
    "                            float_format='%0.2f',\n",
    "                            stars = True,\n",
    "                            model_names=['Model 1',\n",
    "                                         'Model 3',\n",
    "                                         'Model 4'],\n",
    "                            info_dict=info_dict,\n",
    "                            regressor_order=['const',\n",
    "                                             'avexpr',\n",
    "                                             'lat_abst',\n",
    "                                             'asia',\n",
    "                                             'africa',\n",
    "                                             'other'])\n",
    "\n",
    "results_table.add_title('Table 2 - OLS Regressions')\n",
    "\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Endogeneity, Ommitted Variable Bias, Instrumental Variables\n",
    "As [AJR01] discuss, the OLS models likely suffer from endogeneity issues, resulting in biased and inconsistent model estimates. Namely, there is likely a two-way relationship between institutions and economic outcomes.\n",
    "* richer countries may be able to afford or prefer better institutions\n",
    "* variables that affect income may also be correlated with institutional differences\n",
    "* the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions\n",
    "\n",
    "To deal with endogeneity, we can use two-stage least squares (2SLS) regression, which is an extension of OLS regression. It is also a form of instrumental variables (IV) regression. This method requires replacing the endogenous variable avexpriavexpri with a variable that is:\n",
    "* correlated with $avexpr_i$\n",
    "* not correlated with the error term (ie. it should not directly affect the dependent variable, otherwise it would be correlated with $u_i$ due to omitted variable bias)\n",
    "\n",
    "The new set of regressors are called instruments, which aim to remove endogeneity in our proxy of institutional differences. The main contribution of [AJR01] is the use of settler mortality rates to instrument for institutional differences. They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.\n",
    "\n",
    "Using a scatterplot (Figure 3 in [AJR01]), we can see protection against expropriation is negatively correlated with settler mortality rates, coinciding with the authors’ hypothesis and satisfying the first condition of a valid instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NA's is required to use numpy's polyfit\n",
    "df1_subset2 = df1.dropna(subset=['logem4', 'avexpr'])\n",
    "\n",
    "X = df1_subset2['logem4']\n",
    "y = df1_subset2['avexpr']\n",
    "labels = df1_subset2['shortnam']\n",
    "\n",
    "# Replace markers with country labels\n",
    "plt.scatter(X, y, marker='')\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (X.iloc[i], y.iloc[i]))\n",
    "\n",
    "# Fit a linear trend line\n",
    "plt.plot(np.unique(X),\n",
    "         np.poly1d(np.polyfit(X, y, 1))(np.unique(X)),\n",
    "         color='black')\n",
    "\n",
    "plt.xlim([1.8,8.4])\n",
    "plt.ylim([3.3,10.4])\n",
    "plt.xlabel('Log of Settler Mortality')\n",
    "plt.ylabel('Average Expropriation Risk 1985-95')\n",
    "plt.title('Figure 3: First-stage relationship between settler ' +\n",
    "          'mortality and expropriation risk')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import and select the data\n",
    "df4 = pd.read_stata('https://github.com/QuantEcon/QuantEcon.lectures.code/raw/master/ols/maketable4.dta')\n",
    "df4 = df4[df4['baseco'] == 1]\n",
    "\n",
    "# Add a constant variable\n",
    "df4['const'] = 1\n",
    "\n",
    "# Fit the first stage regression and print summary\n",
    "results_fs = sm.OLS(df4['avexpr'],\n",
    "                    df4[['const', 'logem4']],\n",
    "                    missing='drop').fit()\n",
    "print(results_fs.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```none\n",
    "                            OLS Regression Results\n",
    "==============================================================================\n",
    "Dep. Variable:                 avexpr   R-squared:                       0.270\n",
    "Model:                            OLS   Adj. R-squared:                  0.258\n",
    "Method:                 Least Squares   F-statistic:                     22.95\n",
    "Date:                Mon, 17 Jul 2017   Prob (F-statistic):           1.08e-05\n",
    "Time:                        18:41:29   Log-Likelihood:                -104.83\n",
    "No. Observations:                  64   AIC:                             213.7\n",
    "Df Residuals:                      62   BIC:                             218.0\n",
    "Df Model:                           1\n",
    "Covariance Type:            nonrobust\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const          9.3414      0.611     15.296      0.000       8.121      10.562\n",
    "logem4        -0.6068      0.127     -4.790      0.000      -0.860      -0.354\n",
    "==============================================================================\n",
    "Omnibus:                        0.035   Durbin-Watson:                   2.003\n",
    "Prob(Omnibus):                  0.983   Jarque-Bera (JB):                0.172\n",
    "Skew:                           0.045   Prob(JB):                        0.918\n",
    "Kurtosis:                       2.763   Cond. No.                         19.4\n",
    "==============================================================================\n",
    "\n",
    "Warnings:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4['predicted_avexpr'] = results_fs.predict()\n",
    "\n",
    "results_ss = sm.OLS(df4['logpgp95'],\n",
    "                    df4[['const', 'predicted_avexpr']]).fit()\n",
    "print(results_ss.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```none\n",
    "                            OLS Regression Results\n",
    "==============================================================================\n",
    "Dep. Variable:               logpgp95   R-squared:                       0.477\n",
    "Model:                            OLS   Adj. R-squared:                  0.469\n",
    "Method:                 Least Squares   F-statistic:                     56.60\n",
    "Date:                Mon, 17 Jul 2017   Prob (F-statistic):           2.66e-10\n",
    "Time:                        18:41:29   Log-Likelihood:                -72.268\n",
    "No. Observations:                  64   AIC:                             148.5\n",
    "Df Residuals:                      62   BIC:                             152.9\n",
    "Df Model:                           1\n",
    "Covariance Type:            nonrobust\n",
    "====================================================================================\n",
    "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------------\n",
    "const                1.9097      0.823      2.320      0.024       0.264       3.555\n",
    "predicted_avexpr     0.9443      0.126      7.523      0.000       0.693       1.195\n",
    "==============================================================================\n",
    "Omnibus:                       10.547   Durbin-Watson:                   2.137\n",
    "Prob(Omnibus):                  0.005   Jarque-Bera (JB):               11.010\n",
    "Skew:                          -0.790   Prob(JB):                      0.00407\n",
    "Kurtosis:                       4.277   Cond. No.                         58.1\n",
    "==============================================================================\n",
    "\n",
    "Warnings:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from linearmodels.iv import IV2SLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iv = IV2SLS(dependent=df4['logpgp95'],\n",
    "            exog=df4['const'],\n",
    "            endog=df4['avexpr'],\n",
    "            instruments=df4['logem4']).fit(cov_type='unadjusted')\n",
    "\n",
    "print(iv.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```none\n",
    "                          IV-2SLS Estimation Summary\n",
    "==============================================================================\n",
    "Dep. Variable:               logpgp95   R-squared:                      0.1870\n",
    "Estimator:                    IV-2SLS   Adj. R-squared:                 0.1739\n",
    "No. Observations:                  64   F-statistic:                    37.568\n",
    "Date:                Mon, Jul 17 2017   P-value (F-stat)                0.0000\n",
    "Time:                        18:41:29   Distribution:                  chi2(1)\n",
    "Cov. Estimator:            unadjusted\n",
    "\n",
    "                             Parameter Estimates\n",
    "==============================================================================\n",
    "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
    "------------------------------------------------------------------------------\n",
    "const          1.9097     1.0106     1.8897     0.0588     -0.0710      3.8903\n",
    "avexpr         0.9443     0.1541     6.1293     0.0000      0.6423      1.2462\n",
    "==============================================================================\n",
    "\n",
    "Endogenous: avexpr\n",
    "Instruments: logem4\n",
    "Unadjusted Covariance (Homoskedastic)\n",
    "Debiased: False\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in data\n",
    "df4 = pd.read_stata('https://github.com/QuantEcon/QuantEcon.lectures.code/raw/master/ols/maketable4.dta')\n",
    "\n",
    "# Add a constant term\n",
    "df4['const'] = 1\n",
    "\n",
    "# Estimate the first stage regression\n",
    "reg1 = sm.OLS(endog=df4['avexpr'],\n",
    "              exog=df4[['const', 'logem4']],\n",
    "              missing='drop').fit()\n",
    "\n",
    "# Retrieve the residuals\n",
    "df4['resid'] = reg1.resid\n",
    "\n",
    "# Estimate the second stage residuals\n",
    "reg2 = sm.OLS(endog=df4['logpgp95'],\n",
    "              exog=df4[['const', 'avexpr', 'resid']],\n",
    "              missing='drop').fit()\n",
    "\n",
    "print(reg2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```none\n",
    "                            OLS Regression Results\n",
    "==============================================================================\n",
    "Dep. Variable:               logpgp95   R-squared:                       0.689\n",
    "Model:                            OLS   Adj. R-squared:                  0.679\n",
    "Method:                 Least Squares   F-statistic:                     74.05\n",
    "Date:                Mon, 17 Jul 2017   Prob (F-statistic):           1.07e-17\n",
    "Time:                        18:41:29   Log-Likelihood:                -62.031\n",
    "No. Observations:                  70   AIC:                             130.1\n",
    "Df Residuals:                      67   BIC:                             136.8\n",
    "Df Model:                           2\n",
    "Covariance Type:            nonrobust\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const          2.4782      0.547      4.530      0.000       1.386       3.570\n",
    "avexpr         0.8564      0.082     10.406      0.000       0.692       1.021\n",
    "resid         -0.4951      0.099     -5.017      0.000      -0.692      -0.298\n",
    "==============================================================================\n",
    "Omnibus:                       17.597   Durbin-Watson:                   2.086\n",
    "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               23.194\n",
    "Skew:                          -1.054   Prob(JB):                     9.19e-06\n",
    "Kurtosis:                       4.873   Cond. No.                         53.8\n",
    "==============================================================================\n",
    "\n",
    "Warnings:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in data\n",
    "df1 = pd.read_stata('https://github.com/QuantEcon/QuantEcon.lectures.code/raw/master/ols/maketable1.dta')\n",
    "df1 = df1.dropna(subset=['logpgp95', 'avexpr'])\n",
    "\n",
    "# Add a constant term\n",
    "df1['const'] = 1\n",
    "\n",
    "# Define the X and y variables\n",
    "y = np.asarray(df1['logpgp95'])\n",
    "X = np.asarray(df1[['const', 'avexpr']])\n",
    "\n",
    "# Compute beta hat\n",
    "beta_hat = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "\n",
    "# Print out the results from the 2 x 1 vector beta_hat\n",
    "print('beta0: ', np.round(beta_hat, 2)[0])\n",
    "print('beta1: ', np.round(beta_hat, 2)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```none\n",
    "beta0:  4.63\n",
    "beta1:  0.53\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Weighted Least Squares (WLS)\n",
    "[TODO: Include this.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. References\n",
    "* [AJR01] Daron Acemoglu, Simon Johnson, and James A Robinson. The colonial origins of comparative development: an empirical investigation.\" *The American Economic Review*, 91(5):1369–1401, 2001."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
