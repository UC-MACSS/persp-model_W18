{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods (MACS 30100)\n",
    "### by [Benjamin Soltoff](http://www.bensoltoff.com/) and [Richard W. Evans](https://sites.google.com/site/rickecon/), February 2018\n",
    "The code in this Jupyter notebook was written using Python 3.6. It uses data files `?`, and ? image files in the `images` folder in the same directory as this notebook. For the code to run properly, you will either need to have access to the internet or you should have the data file in the same folder as the Jupyter notebook file. Otherwise, you will have to change the respective lines of the code that read in the data to reflect the location of that data. Some of this content was taken from Dr. Benjamin Soltoff's notes [here](http://cfss.uchicago.edu/persp006_resampling.html).\n",
    "\n",
    "Resampling methods are a way to test the sensitivity of statistical results to estimation using a different sample. It is often too difficult or too expensive to draw a new sample from the population. Resampling methods take advantage of the training-set test-set paradigm to evaluate the sensitivity of estimates to sample variance. The two main classes of resampling methods are\n",
    "\n",
    "1. Cross validation\n",
    "2. Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training vs. Test Sets\n",
    "* Logistic Regression: training = estimates of coefficients, testing = confusion matrix / percent correct / percent wrong\n",
    "* Linear Regression: testing set = actual data values - predicted value, Mean Squared Errors (MSE) common measure of fit (adds up all of the errors in the set\n",
    "\n",
    "Uncertainties with our Estimates so far:\n",
    "* Imputed data (missing data)\n",
    "* Different training vs. test sets (e.g. 50% train/test)\n",
    "* Could draw different random samples = bootstrap!\n",
    "\n",
    "Bootstrap\n",
    "* 1. Draw S random samples of size N with replacement (so that the same observations might show up in different samples)\n",
    "* 2. Compute MSE for each of the samples that we drew\n",
    "* 3. Estimated test MSE is the average over all those randomly drawn samples; Cross Validation boot (CV)\n",
    "* Same process as logit model; estimate and check accuracy, with accuracy being MSE, but we do that S times for different samples from the data\n",
    "* Ending estimate is the average over all those mean squared errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Validation set approach\n",
    "This is the approach that we have already studied in the [classifiers 1 notebook](https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/Classfcn1/KKNlogitLDA.ipynb).\n",
    "\n",
    "1. Partition the data into a training set and a test set.\n",
    "2. Estimate the model using the training set.\n",
    "3. Evaluate the fit or predictive accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Leave-one-out cross validation\n",
    "Leave-one-out cross validation (LOOCV) is an approach in which the model is assessed using $N$ different training sets and test sets of a specific size. Let the data have $N$ observations. LOOCV is to choose a training set with $N-1$ observations, such that the test set only has one observation $y_i$. Repeat this $N$ with a slightly different training set such that each data point is the test set in exactly one of these sebsets.\n",
    "\n",
    "In this case, the mean squared error MSE has no summation because there is only one observation in the test set.\n",
    "\n",
    "$$ MSE_i = (y_j - \\hat{y}_j)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOCV\n",
    "* N observations\n",
    "* 1. Choose training set (N-1)\n",
    "* 2. Estimate model on the N-1 observations\n",
    "* 3. Calculate MSE on the test set; MSE won't be a summation, will just be MSE = y - y_hat\n",
    "* 4. Repeat that N times\n",
    "* 5. Estimated test MSE and CV\n",
    "\n",
    "Bias\n",
    "* LOOCV test bias\n",
    "\n",
    "Variance\n",
    "* LOOCV high variance\n",
    "\n",
    "Least Squares Regression\n",
    "* About bias, not about variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOOCV estimate for the test MSE is the average of these $N$ test error estimates.\n",
    "\n",
    "$$ CV_{loo} = \\frac{1}{N}\\sum_{i=1}^N MSE_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. k-fold cross validation\n",
    "$k$-fold cross validation is a method in which the dataset is randomly divided into $k$ groups (folds). Define a test set of the model as the $k$th fold. For each test set $k$, the model is estimated on the data from the other $k-1$ folds. Let the number of observations in the $k$th fold be $N_k$, and let $\\mathcal{K}$ be the set of observations in the $k$th fold. The $MSE_k$ of the $k$th fold is:\n",
    "\n",
    "$$ MSE_k = \\frac{1}{N_k}\\sum_{i\\in\\mathcal{K}}(y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Then the $k$-fold estimate for the test MSE is the average of these $k$ test error estimates.\n",
    "\n",
    "$$ CV_{kf} = \\frac{1}{k}\\sum_{j=1}^k MSE_j $$\n",
    "\n",
    "LOOCV is a special case of $k$-fold cross validation in which $k=N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Folds\n",
    "* 1. Choose k subgroups of data\n",
    "* 2. Leave Kth group out as test set\n",
    "* 3. Estimate the model on the remaining K-1 groups data\n",
    "* 4. Repeat K times until you have computed MSE(K) of left out group\n",
    "* 5. Estimated test MSE is CV KFolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Bias versus variance\n",
    "Note that the LOOCV method has low bias (estimated on large number of date) but high variance (errors are based on one draw). In contrast, the $k$-fold method has more bias (estimated with less data) but lower variance. Each test set has more observations.\n",
    "\n",
    "* $k$-fold cross validation can often provide more accurate estimates of the test error rate.\n",
    "* $k$-fold is less computationally intensive\n",
    "* LOOCV has the least bias\n",
    "* LOOCV is the most computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootstrapping\n",
    "This name comes from the expression \"to pull oneself up by ones own bootstraps.\" In a way similar to the cross validation methods of the last section, we can use *the bootstrap* to quantify the undertainty associated with a given estimator, learning model, or method. In the econometrics and statistics literature, this often shows up as \"bootstrapped standard errors\". Bootstrapping is valuable because it is so widely applicable to a range of models.\n",
    "\n",
    "1. Randomly draw $S$ datasets of size $N_S$ with replacement. Define each training set of observations as $\\mathcal{K}_s$ and each corresponding test set as $\\mathcal{-K}_{s}$.\n",
    "2. Calculate the MSE for each test set $\\mathcal{-K}_{s}$\n",
    "\n",
    "The bootstrap estimate for the test MSE is the average MSE from each random test set.\n",
    "\n",
    "$$ CV_{boot} = \\frac{1}{S}\\sum_{s=1}^S MSE_s $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Overall:\n",
    "* Bootstrap will have lower bias and lower variance, combined \n",
    "* LOOCV will have lower bias than K folds and bootstrap, but not lowest variance\n",
    "* As long as N is big enough and takes enough draws, bootstrap can beat out K folds and LOOCV for bias and variance\n",
    "* Cannot get bias to beat LOOCV, but LOOCV will have highest variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
