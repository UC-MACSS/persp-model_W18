{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set #8\n",
    "\n",
    "MACS 30100, Dr. Evans \n",
    "\n",
    "Shuting Chen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural network horse race "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cultivar</th>\n",
       "      <th>alco</th>\n",
       "      <th>malic</th>\n",
       "      <th>ash</th>\n",
       "      <th>alk</th>\n",
       "      <th>magn</th>\n",
       "      <th>tot_phen</th>\n",
       "      <th>flav</th>\n",
       "      <th>nonfl_phen</th>\n",
       "      <th>proanth</th>\n",
       "      <th>color_int</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280rat</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cultivar   alco  malic   ash   alk  magn  tot_phen  flav  nonfl_phen  \\\n",
       "0         1  14.23   1.71  2.43  15.6   127      2.80  3.06        0.28   \n",
       "1         1  13.20   1.78  2.14  11.2   100      2.65  2.76        0.26   \n",
       "2         1  13.16   2.36  2.67  18.6   101      2.80  3.24        0.30   \n",
       "3         1  14.37   1.95  2.50  16.8   113      3.85  3.49        0.24   \n",
       "4         1  13.24   2.59  2.87  21.0   118      2.80  2.69        0.39   \n",
       "\n",
       "   proanth  color_int   hue  OD280rat  proline  \n",
       "0     2.29       5.64  1.04      3.92     1065  \n",
       "1     1.28       4.38  1.05      3.40     1050  \n",
       "2     2.81       5.68  1.03      3.17     1185  \n",
       "3     2.18       7.80  0.86      3.45     1480  \n",
       "4     1.82       4.32  1.04      2.93      735  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "drink = pd.read_csv('strongdrink.txt')\n",
    "drink.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAFtCAYAAAD74zZTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfLOkYYwxxjdLGsYYY3z7/wGlYeMWhvHDuAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1168de940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.lmplot('alco','color_int', data=drink, fit_reg=False, hue = 'cultivar')\n",
    "ax.set(xlabel='Alcohol', ylabel='Color Intensity', title='Scatterplot of alcohol and color intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xvals = drink[['alco', 'malic', 'tot_phen', 'color_int']].values\n",
    "yvals = drink['cultivar'].values\n",
    "k = 4\n",
    "clf_mlog = KFold(n_splits=k, shuffle=True, random_state=22)\n",
    "clf_mlog.get_n_splits(Xvals)\n",
    "MSE = np.zeros(k)\n",
    "C = np.zeros(200)\n",
    "MSE_C = np.zeros(200)\n",
    "\n",
    "for c in range(200):\n",
    "    k_ind = int(0)\n",
    "    for train_index, test_index in clf_mlog.split(Xvals):\n",
    "        X_train, X_test = Xvals[train_index], Xvals[test_index]\n",
    "        y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "        LogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                    solver='newton-cg', C = (c/100 + 0.01))\n",
    "        LogReg.fit(X_train, y_train)\n",
    "        y_pred = LogReg.predict(X_test)\n",
    "        error = y_test != y_pred\n",
    "        MSE[k_ind] = error.mean()\n",
    "        k_ind += 1\n",
    "    C[c] = c/100 + 0.01\n",
    "    MSE_C[c] =  MSE.mean()\n",
    "MSE_multilog = pd.DataFrame({'C':C, 'MSE':MSE_C})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C       MSE\n",
       "43  0.44  0.068182\n",
       "44  0.45  0.068182\n",
       "42  0.43  0.068182\n",
       "41  0.42  0.068182\n",
       "40  0.41  0.068182\n",
       "39  0.40  0.068182\n",
       "27  0.28  0.068182\n",
       "28  0.29  0.068182\n",
       "29  0.30  0.068182\n",
       "30  0.31  0.068182\n",
       "31  0.32  0.068182\n",
       "32  0.33  0.068182\n",
       "33  0.34  0.068182\n",
       "34  0.35  0.068182\n",
       "35  0.36  0.068182\n",
       "36  0.37  0.068182\n",
       "37  0.38  0.068182\n",
       "46  0.47  0.068182\n",
       "45  0.46  0.068182\n",
       "38  0.39  0.068182\n",
       "47  0.48  0.068182\n",
       "56  0.57  0.073864\n",
       "54  0.55  0.073864\n",
       "26  0.27  0.073864\n",
       "25  0.26  0.073864"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_multilog.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results shown above, we can conclude that the multinomial logistic model has the smallest MSE of 0.068182 if we set the value of C between 0.28 and 0.44 with 'I2' penalty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_rf = pd.DataFrame({\"n_estimator\" : np.zeros(5),\n",
    "                         \"min_sample_leaf\" : np.zeros(5),\n",
    "                         \"max_depth\" : np.zeros(5),\n",
    "                         \"MSE\" : np.zeros(5)})\n",
    "MSE_df = MSE_rf[:0]\n",
    "\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        for tree in range(5):\n",
    "            rf = RandomForestClassifier(n_estimators = (tree * 50 + 50),\n",
    "                                        min_samples_leaf = (i * 5 + 5),\n",
    "                                        max_depth = (j + 1), bootstrap=True, \n",
    "                                        oob_score=True, random_state=22)\n",
    "            rf.fit(Xvals, yvals)\n",
    "            MSE_rf[\"n_estimator\"][tree] = tree * 50 + 50\n",
    "            MSE_rf[\"max_depth\"][tree] = j + 1\n",
    "            MSE_rf[\"min_sample_leaf\"][tree] = i * 5 + 5\n",
    "            MSE_rf[\"MSE\"][tree] = 1 - rf.oob_score_\n",
    "        MSE_df= pd.concat([MSE_df, MSE_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_sample_leaf</th>\n",
       "      <th>n_estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE  max_depth  min_sample_leaf  n_estimator\n",
       "96   0.068182        5.0             10.0        100.0\n",
       "126  0.068182       11.0             10.0        100.0\n",
       "116  0.068182        9.0             10.0        100.0\n",
       "131  0.068182       12.0             10.0        100.0\n",
       "111  0.068182        8.0             10.0        100.0\n",
       "106  0.068182        7.0             10.0        100.0\n",
       "121  0.068182       10.0             10.0        100.0\n",
       "101  0.068182        6.0             10.0        100.0\n",
       "136  0.068182       13.0             10.0        100.0\n",
       "86   0.068182        3.0             10.0        100.0\n",
       "88   0.068182        3.0             10.0        200.0\n",
       "91   0.068182        4.0             10.0        100.0\n",
       "146  0.068182       15.0             10.0        100.0\n",
       "141  0.068182       14.0             10.0        100.0\n",
       "138  0.073864       13.0             10.0        200.0\n",
       "142  0.073864       14.0             10.0        150.0\n",
       "133  0.073864       12.0             10.0        200.0\n",
       "132  0.073864       12.0             10.0        150.0\n",
       "143  0.073864       14.0             10.0        200.0\n",
       "128  0.073864       11.0             10.0        200.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_df.index = range(len(MSE_df))\n",
    "MSE_df.sort_values(['MSE']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results shown above, the random forest model has the smallest MSE of 0.068182 when max_depth = 5, min_sample_leaf = 10 and n_estimator = 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "clf_svm = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_svm.get_n_splits(Xvals)\n",
    "MSE = np.zeros(k)\n",
    "MSE_C = pd.DataFrame({\"C\" : np.zeros(80),\n",
    "                      \"G\" : np.zeros(80),\n",
    "                      \"MSE\" : np.zeros(80)})\n",
    "MSE_SVM = MSE_C[:0]\n",
    "\n",
    "for g in range(80):\n",
    "    for c in range(80):\n",
    "        k_ind = int(0)\n",
    "        for train_index, test_index in clf_svm.split(Xvals):\n",
    "            X_train, X_test = Xvals[train_index], Xvals[test_index]\n",
    "            y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "            svc = svm.SVC(kernel='rbf', gamma = (g/20 + 0.05),\n",
    "                          C=c/20 + 0.05)\n",
    "            svc.fit(X_train, y_train)\n",
    "            y_pred = svc.predict(X_test)\n",
    "            error = y_test != y_pred\n",
    "            MSE[k_ind] = error.mean()\n",
    "            k_ind += 1\n",
    "        MSE_C['C'][c] = c/20 + 0.05\n",
    "        MSE_C['G'][c] = g/20 + 0.05\n",
    "        MSE_C['MSE'][c] =  MSE.mean()\n",
    "    MSE_SVM = pd.concat([MSE_SVM, MSE_C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>G</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>3.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>3.30</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2.95</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>3.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>3.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2.90</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         C     G       MSE\n",
       "2659  1.00  1.70  0.045455\n",
       "2578  0.95  1.65  0.045455\n",
       "2658  0.95  1.70  0.045455\n",
       "224   3.25  0.15  0.051136\n",
       "225   3.30  0.15  0.051136\n",
       "226   3.35  0.15  0.051136\n",
       "227   3.40  0.15  0.051136\n",
       "228   3.45  0.15  0.051136\n",
       "229   3.50  0.15  0.051136\n",
       "218   2.95  0.15  0.051136\n",
       "230   3.55  0.15  0.051136\n",
       "231   3.60  0.15  0.051136\n",
       "232   3.65  0.15  0.051136\n",
       "233   3.70  0.15  0.051136\n",
       "234   3.75  0.15  0.051136\n",
       "235   3.80  0.15  0.051136\n",
       "223   3.20  0.15  0.051136\n",
       "222   3.15  0.15  0.051136\n",
       "221   3.10  0.15  0.051136\n",
       "220   3.05  0.15  0.051136\n",
       "219   3.00  0.15  0.051136\n",
       "217   2.90  0.15  0.051136\n",
       "216   2.85  0.15  0.051136\n",
       "215   2.80  0.15  0.051136\n",
       "214   2.75  0.15  0.051136"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_SVM.index = range(len(MSE_SVM))\n",
    "MSE_SVM.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results shown above, the SVM model has the smallest MSE of 0.045455 when cost = 1, gamma = 1.7 (or cost = 0.95, gamma = 1.65, or cost = 0.95, gamma = 1.70)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "clf_mlp = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_mlp.get_n_splits(Xvals)\n",
    "MSE = np.zeros(k)\n",
    "activ = np.array(['identity', 'logistic', 'tanh', 'relu'])\n",
    "MSE_al = pd.DataFrame({'activation' : np.zeros(20),\n",
    "                       'hidden layer' : np.zeros(20),\n",
    "                       'alpha' : np.zeros(20),\n",
    "                       'MSE':np.zeros(20)})\n",
    "MSE_mlp = MSE_al[:0]\n",
    "\n",
    "for ac in range(4):\n",
    "    for h in range(8):\n",
    "        for al in range(20):\n",
    "            k_ind = int(0)\n",
    "            for train_index, test_index in clf_mlp.split(Xvals):\n",
    "                X_train, X_test = Xvals[train_index], Xvals[test_index]\n",
    "                y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "                mlp = MLPClassifier(activation=activ[ac], solver='lbfgs',\n",
    "                                    alpha=(al/20 + 0.05), \n",
    "                                    hidden_layer_sizes = ((50 * (h + 1)),))\n",
    "                mlp.fit(X_train, y_train)\n",
    "                y_pred = mlp.predict(X_test)\n",
    "                error = y_test != y_pred\n",
    "                MSE[k_ind] = error.mean()\n",
    "                k_ind += 1\n",
    "            MSE_al['activation'][al] = activ[ac]\n",
    "            MSE_al['hidden layer'][al] = 50 * (h + 1)\n",
    "            MSE_al['alpha'][al] = al/20 + 0.05\n",
    "            MSE_al['MSE'][al] =  MSE.mean()\n",
    "        MSE_mlp = pd.concat([MSE_mlp, MSE_al])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>hidden layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.20</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.85</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.50</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.95</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.65</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.65</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>1.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.50</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.55</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.85</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.40</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.80</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>1.00</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.55</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE activation  alpha  hidden layer\n",
       "543  0.039773       relu   0.20         200.0\n",
       "564  0.039773       relu   0.25         250.0\n",
       "561  0.039773       relu   0.10         250.0\n",
       "596  0.039773       relu   0.85         300.0\n",
       "609  0.039773       relu   0.50         350.0\n",
       "551  0.039773       relu   0.60         200.0\n",
       "506  0.039773       relu   0.35         100.0\n",
       "508  0.045455       relu   0.45         100.0\n",
       "578  0.045455       relu   0.95         250.0\n",
       "612  0.045455       relu   0.65         350.0\n",
       "512  0.045455       relu   0.65         100.0\n",
       "513  0.045455       relu   0.70         100.0\n",
       "519  0.045455       relu   1.00         100.0\n",
       "529  0.045455       relu   0.50         150.0\n",
       "531  0.045455       relu   0.60         150.0\n",
       "510  0.045455       relu   0.55         100.0\n",
       "536  0.045455       relu   0.85         150.0\n",
       "547  0.045455       relu   0.40         200.0\n",
       "555  0.045455       relu   0.80         200.0\n",
       "559  0.045455       relu   1.00         200.0\n",
       "593  0.045455       relu   0.70         300.0\n",
       "562  0.045455       relu   0.15         250.0\n",
       "570  0.045455       relu   0.55         250.0\n",
       "588  0.045455       relu   0.45         300.0\n",
       "571  0.045455       relu   0.60         250.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_mlp.index = range(len(MSE_mlp))\n",
    "MSE_mlp.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baaed on the results shown above, the MLP model has the lowest MSE of 0.039773 when activation = relu, alpha = 0.20 and hidden layer = 200. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logit</th>\n",
       "      <th>MLP</th>\n",
       "      <th>RF</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Logit       MLP        RF       SVM\n",
       "0  0.068182  0.039773  0.068182  0.045455\n",
       "1  0.068182  0.039773  0.068182  0.045455\n",
       "2  0.068182  0.039773  0.068182  0.045455\n",
       "3  0.068182  0.039773  0.068182  0.051136\n",
       "4  0.068182  0.039773  0.068182  0.051136"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = np.array(MSE_multilog.sort_values(['MSE']).head(5)['MSE'])\n",
    "RF = np.array(MSE_df.sort_values(['MSE']).head(5)['MSE'])\n",
    "SVM = np.array(MSE_SVM.sort_values(['MSE']).head(5)['MSE'])\n",
    "mlp = np.array(MSE_mlp.sort_values(['MSE']).head(5)['MSE'])\n",
    "\n",
    "\n",
    "Comp = pd.DataFrame({'Logit':logit, \n",
    "                     'RF':RF,\n",
    "                     'SVM':SVM,\n",
    "                     'MLP':mlp})\n",
    "Comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these four different models by their first five smallest MSE values, we can conclude that MLP neural network model is the best predictor of cultivar since it has the smallest MSE values among these models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
