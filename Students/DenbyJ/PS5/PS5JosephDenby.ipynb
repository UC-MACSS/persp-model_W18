{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 5\n",
    "\n",
    "### Joseph Denby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as lin\n",
    "import scipy.stats as sts\n",
    "import scipy.integrate as intgr\n",
    "import scipy.optimize as opt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import seaborn as sb\n",
    "cmap1 = matplotlib.cm.get_cmap('summer')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks = pd.read_csv('../../../ProblemSets/PS5/data/strongdrink.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drinks['const'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cultivar</th>\n",
       "      <th>alco</th>\n",
       "      <th>malic</th>\n",
       "      <th>ash</th>\n",
       "      <th>alk</th>\n",
       "      <th>magn</th>\n",
       "      <th>tot_phen</th>\n",
       "      <th>flav</th>\n",
       "      <th>nonfl_phen</th>\n",
       "      <th>proanth</th>\n",
       "      <th>color_int</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280rat</th>\n",
       "      <th>proline</th>\n",
       "      <th>const</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cultivar   alco  malic   ash   alk  magn  tot_phen  flav  nonfl_phen  \\\n",
       "0         1  14.23   1.71  2.43  15.6   127      2.80  3.06        0.28   \n",
       "1         1  13.20   1.78  2.14  11.2   100      2.65  2.76        0.26   \n",
       "2         1  13.16   2.36  2.67  18.6   101      2.80  3.24        0.30   \n",
       "3         1  14.37   1.95  2.50  16.8   113      3.85  3.49        0.24   \n",
       "4         1  13.24   2.59  2.87  21.0   118      2.80  2.69        0.39   \n",
       "\n",
       "   proanth  color_int   hue  OD280rat  proline  const  \n",
       "0     2.29       5.64  1.04      3.92     1065      1  \n",
       "1     1.28       4.38  1.05      3.40     1050      1  \n",
       "2     2.81       5.68  1.03      3.17     1185      1  \n",
       "3     2.18       7.80  0.86      3.45     1480      1  \n",
       "4     1.82       4.32  1.04      2.93      735      1  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a). Use a multinomial logistic regression model of the following form with the following linear predictor $\\eta_j$ for $j$ = 1, 2 (the baseline class is $j$ = 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Pr(cultivar_i = j|X\\beta_j) = \\frac{e^{\\eta_j}}{1+\\sum^{J-1}_{j=1}e^{\\eta_j}}$ for $j = 1,2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta_j = \\beta_{j,0} + \\beta_{j,1} alco_i + \\beta_{j,2} malic_i + \\beta_{j,3} tot\\_phen_i + \\beta_{j,4} color\\_int_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    71\n",
       "1    59\n",
       "3    46\n",
       "Name: cultivar, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drinks.cultivar.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1123866a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKlx/wsTQ/Qxp0F+GAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f847f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(x='cultivar', data=drinks, palette='hls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = drinks[['const', 'alco', 'malic', 'tot_phen', 'color_int']]\n",
    "y = drinks[['cultivar']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size = 0.25,\n",
    "       random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephdenby/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultLogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                solver='newton-cg')\n",
    "MultLogReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = MultLogReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the classification report, we can observe the precision values for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.87      1.00      0.93        13\n",
      "          2       1.00      0.90      0.95        21\n",
      "          3       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.96      0.95      0.96        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the error rate ($\\epsilon_j$) to be $1 - precision_j$:\n",
    "\n",
    "$\\epsilon_1 = 1 - .87 = .13$   \n",
    "$\\epsilon_2 = \\epsilon_3 = 1 - 1 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $j = 1$, $\\vec{\\beta}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.45998034e-06,  1.70038994e+00, -2.65604001e-01,  1.22389318e+00,\n",
       "        2.27585993e-02])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultLogReg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $j = 2$, $\\vec{\\beta}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.76923783e-05, -1.46805313e+00, -3.33053748e-01,  6.64013944e-01,\n",
       "       -9.22712974e-01])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultLogReg.coef_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification report, this model performs with perfect precision when predicting members of categories $j = 2$ and $j=3$. According to the countplot above, these categories have the most ($n=71$) and fewest ($n = 46$) observations respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b). Perform a leave-one-out cross validation (LOOCV) with the model from part (a). Report your error rates (1 - precision) for each category. How do your error rates compare to those from part (a)? Report your LOOCV estimate for the test MSE as the average MSE, where $y_i$ is the left out observation from each test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$CV_{loo} = \\frac{1}{N} \\sum^{N}_{i=1}MSE_i = \\frac{1}{N} \\sum^{N}_{i=1}[1-I(y_i = \\hat{y}_i)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xvals = X.values\n",
    "yvals = y.values\n",
    "N_loo = Xvals.shape[0]\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(Xvals)\n",
    "MSE_vec = np.zeros(N_loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephdenby/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for test set [0]  is [0.]\n",
      "MSE for test set [1]  is [0.]\n",
      "MSE for test set [2]  is [0.]\n",
      "MSE for test set [3]  is [0.]\n",
      "MSE for test set [4]  is [0.]\n",
      "MSE for test set [5]  is [0.]\n",
      "MSE for test set [6]  is [0.]\n",
      "MSE for test set [7]  is [0.]\n",
      "MSE for test set [8]  is [0.]\n",
      "MSE for test set [9]  is [0.]\n",
      "MSE for test set [10]  is [0.]\n",
      "MSE for test set [11]  is [0.]\n",
      "MSE for test set [12]  is [0.]\n",
      "MSE for test set [13]  is [0.]\n",
      "MSE for test set [14]  is [0.]\n",
      "MSE for test set [15]  is [0.]\n",
      "MSE for test set [16]  is [0.]\n",
      "MSE for test set [17]  is [0.]\n",
      "MSE for test set [18]  is [0.]\n",
      "MSE for test set [19]  is [0.]\n",
      "MSE for test set [20]  is [0.]\n",
      "MSE for test set [21]  is [0.]\n",
      "MSE for test set [22]  is [1.]\n",
      "MSE for test set [23]  is [1.]\n",
      "MSE for test set [24]  is [1.]\n",
      "MSE for test set [25]  is [1.]\n",
      "MSE for test set [26]  is [0.]\n",
      "MSE for test set [27]  is [1.]\n",
      "MSE for test set [28]  is [0.]\n",
      "MSE for test set [29]  is [0.]\n",
      "MSE for test set [30]  is [0.]\n",
      "MSE for test set [31]  is [0.]\n",
      "MSE for test set [32]  is [1.]\n",
      "MSE for test set [33]  is [0.]\n",
      "MSE for test set [34]  is [1.]\n",
      "MSE for test set [35]  is [0.]\n",
      "MSE for test set [36]  is [0.]\n",
      "MSE for test set [37]  is [1.]\n",
      "MSE for test set [38]  is [1.]\n",
      "MSE for test set [39]  is [0.]\n",
      "MSE for test set [40]  is [0.]\n",
      "MSE for test set [41]  is [1.]\n",
      "MSE for test set [42]  is [0.]\n",
      "MSE for test set [43]  is [0.]\n",
      "MSE for test set [44]  is [0.]\n",
      "MSE for test set [45]  is [0.]\n",
      "MSE for test set [46]  is [0.]\n",
      "MSE for test set [47]  is [0.]\n",
      "MSE for test set [48]  is [0.]\n",
      "MSE for test set [49]  is [0.]\n",
      "MSE for test set [50]  is [0.]\n",
      "MSE for test set [51]  is [0.]\n",
      "MSE for test set [52]  is [0.]\n",
      "MSE for test set [53]  is [0.]\n",
      "MSE for test set [54]  is [0.]\n",
      "MSE for test set [55]  is [0.]\n",
      "MSE for test set [56]  is [0.]\n",
      "MSE for test set [57]  is [0.]\n",
      "MSE for test set [58]  is [0.]\n",
      "MSE for test set [59]  is [0.]\n",
      "MSE for test set [60]  is [0.]\n",
      "MSE for test set [61]  is [1.]\n",
      "MSE for test set [62]  is [0.]\n",
      "MSE for test set [63]  is [1.]\n",
      "MSE for test set [64]  is [0.]\n",
      "MSE for test set [65]  is [1.]\n",
      "MSE for test set [66]  is [1.]\n",
      "MSE for test set [67]  is [1.]\n",
      "MSE for test set [68]  is [0.]\n",
      "MSE for test set [69]  is [0.]\n",
      "MSE for test set [70]  is [0.]\n",
      "MSE for test set [71]  is [0.]\n",
      "MSE for test set [72]  is [0.]\n",
      "MSE for test set [73]  is [0.]\n",
      "MSE for test set [74]  is [0.]\n",
      "MSE for test set [75]  is [0.]\n",
      "MSE for test set [76]  is [0.]\n",
      "MSE for test set [77]  is [0.]\n",
      "MSE for test set [78]  is [0.]\n",
      "MSE for test set [79]  is [0.]\n",
      "MSE for test set [80]  is [0.]\n",
      "MSE for test set [81]  is [0.]\n",
      "MSE for test set [82]  is [0.]\n",
      "MSE for test set [83]  is [1.]\n",
      "MSE for test set [84]  is [0.]\n",
      "MSE for test set [85]  is [0.]\n",
      "MSE for test set [86]  is [0.]\n",
      "MSE for test set [87]  is [0.]\n",
      "MSE for test set [88]  is [0.]\n",
      "MSE for test set [89]  is [0.]\n",
      "MSE for test set [90]  is [0.]\n",
      "MSE for test set [91]  is [0.]\n",
      "MSE for test set [92]  is [0.]\n",
      "MSE for test set [93]  is [0.]\n",
      "MSE for test set [94]  is [0.]\n",
      "MSE for test set [95]  is [0.]\n",
      "MSE for test set [96]  is [0.]\n",
      "MSE for test set [97]  is [0.]\n",
      "MSE for test set [98]  is [1.]\n",
      "MSE for test set [99]  is [0.]\n",
      "MSE for test set [100]  is [0.]\n",
      "MSE for test set [101]  is [0.]\n",
      "MSE for test set [102]  is [0.]\n",
      "MSE for test set [103]  is [0.]\n",
      "MSE for test set [104]  is [0.]\n",
      "MSE for test set [105]  is [0.]\n",
      "MSE for test set [106]  is [0.]\n",
      "MSE for test set [107]  is [0.]\n",
      "MSE for test set [108]  is [0.]\n",
      "MSE for test set [109]  is [0.]\n",
      "MSE for test set [110]  is [0.]\n",
      "MSE for test set [111]  is [0.]\n",
      "MSE for test set [112]  is [0.]\n",
      "MSE for test set [113]  is [0.]\n",
      "MSE for test set [114]  is [0.]\n",
      "MSE for test set [115]  is [0.]\n",
      "MSE for test set [116]  is [0.]\n",
      "MSE for test set [117]  is [0.]\n",
      "MSE for test set [118]  is [0.]\n",
      "MSE for test set [119]  is [0.]\n",
      "MSE for test set [120]  is [0.]\n",
      "MSE for test set [121]  is [1.]\n",
      "MSE for test set [122]  is [0.]\n",
      "MSE for test set [123]  is [0.]\n",
      "MSE for test set [124]  is [0.]\n",
      "MSE for test set [125]  is [0.]\n",
      "MSE for test set [126]  is [0.]\n",
      "MSE for test set [127]  is [0.]\n",
      "MSE for test set [128]  is [0.]\n",
      "MSE for test set [129]  is [0.]\n",
      "MSE for test set [130]  is [1.]\n",
      "MSE for test set [131]  is [0.]\n",
      "MSE for test set [132]  is [0.]\n",
      "MSE for test set [133]  is [0.]\n",
      "MSE for test set [134]  is [4.]\n",
      "MSE for test set [135]  is [0.]\n",
      "MSE for test set [136]  is [0.]\n",
      "MSE for test set [137]  is [0.]\n",
      "MSE for test set [138]  is [4.]\n",
      "MSE for test set [139]  is [0.]\n",
      "MSE for test set [140]  is [0.]\n",
      "MSE for test set [141]  is [0.]\n",
      "MSE for test set [142]  is [0.]\n",
      "MSE for test set [143]  is [0.]\n",
      "MSE for test set [144]  is [0.]\n",
      "MSE for test set [145]  is [0.]\n",
      "MSE for test set [146]  is [0.]\n",
      "MSE for test set [147]  is [0.]\n",
      "MSE for test set [148]  is [0.]\n",
      "MSE for test set [149]  is [0.]\n",
      "MSE for test set [150]  is [0.]\n",
      "MSE for test set [151]  is [0.]\n",
      "MSE for test set [152]  is [0.]\n",
      "MSE for test set [153]  is [0.]\n",
      "MSE for test set [154]  is [0.]\n",
      "MSE for test set [155]  is [0.]\n",
      "MSE for test set [156]  is [0.]\n",
      "MSE for test set [157]  is [0.]\n",
      "MSE for test set [158]  is [0.]\n",
      "MSE for test set [159]  is [0.]\n",
      "MSE for test set [160]  is [0.]\n",
      "MSE for test set [161]  is [0.]\n",
      "MSE for test set [162]  is [0.]\n",
      "MSE for test set [163]  is [0.]\n",
      "MSE for test set [164]  is [0.]\n",
      "MSE for test set [165]  is [0.]\n",
      "MSE for test set [166]  is [0.]\n",
      "MSE for test set [167]  is [0.]\n",
      "MSE for test set [168]  is [0.]\n",
      "MSE for test set [169]  is [0.]\n",
      "MSE for test set [170]  is [0.]\n",
      "MSE for test set [171]  is [0.]\n",
      "MSE for test set [172]  is [0.]\n",
      "MSE for test set [173]  is [0.]\n",
      "MSE for test set [174]  is [0.]\n",
      "MSE for test set [175]  is [0.]\n",
      "test estimate MSE loocv= 0.1534090909090909 , test estimate MSE standard err= 0.5159829242321624\n"
     ]
    }
   ],
   "source": [
    "ytests = np.zeros(N_loo)\n",
    "ypreds = np.zeros(N_loo)\n",
    "for train_index, test_index in loo.split(Xvals):\n",
    "    X_train, X_test = Xvals[train_index], Xvals[test_index]\n",
    "    y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "    ytests[test_index] = y_test\n",
    "    MultLogReg = LogisticRegression(multi_class='multinomial',solver='newton-cg')\n",
    "    MultLogReg.fit(X_train, y_train)\n",
    "    y_pred = LogReg.predict(X_test)\n",
    "    ypreds[test_index] = y_pred\n",
    "    MSE_vec[test_index] = (y_test - y_pred) ** 2\n",
    "    print('MSE for test set', test_index, ' is', MSE_vec[test_index])\n",
    "\n",
    "MSE_loo = MSE_vec.mean()\n",
    "MSE_loo_std = MSE_vec.std()\n",
    "print('test estimate MSE loocv=', MSE_loo,\n",
    "      ', test estimate MSE standard err=', MSE_loo_std)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.84      0.83      0.84        59\n",
      "        2.0       0.85      0.89      0.87        71\n",
      "        3.0       0.98      0.93      0.96        46\n",
      "\n",
      "avg / total       0.88      0.88      0.88       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytests, ypreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! LOOCV leads to worse performance overall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon_1 = 1-.84 = .16$   \n",
    "$\\epsilon_2 = 1-.85 = .15$   \n",
    "$\\epsilon_3 = 1-.98 = .2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c). Perform a $k$-fold cross validation in which the data are divided into $k$ = 4 groups. Use the following code. Report your error rates (1 - precision) for each category. How do your error rates compare to those from parts (a) and (b)? Report your $k$-fold estimate for the test MSE as the average MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k = 4\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=10)\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k index= 0\n",
      "MSE for test set 0  is 1.012396694214876\n",
      "k index= 1\n",
      "MSE for test set 1  is 1.3233471074380165\n",
      "k index= 2\n",
      "MSE for test set 2  is 1.1818181818181819\n",
      "k index= 3\n",
      "MSE for test set 3  is 1.0537190082644627\n",
      "test estimate MSE k-fold= 1.1428202479338843 test estimate MSE standard err= 0.12151143991924328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephdenby/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "MSE_vec_kf = np.zeros(k)\n",
    "ytests = np.array([])\n",
    "ypreds = np.array([])\n",
    "k_ind = int(0)\n",
    "for train_index, test_index in kf.split(Xvals):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print('k index=', k_ind)\n",
    "    X_train, X_test = Xvals[train_index], Xvals[test_index]\n",
    "    y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "    ytests = np.append(ytests, y_test)\n",
    "    LogReg = LogisticRegression()\n",
    "    LogReg.fit(X_train, y_train)\n",
    "    y_pred = LogReg.predict(X_test)\n",
    "    ypreds = np.append(ypreds, y_pred)\n",
    "    MSE_vec_kf[k_ind] = ((y_test - y_pred) ** 2).mean()\n",
    "    print('MSE for test set', k_ind, ' is', MSE_vec_kf[k_ind])\n",
    "    k_ind += 1\n",
    "\n",
    "MSE_kf = MSE_vec_kf.mean()\n",
    "MSE_kf_std = MSE_vec_kf.std()\n",
    "print('test estimate MSE k-fold=', MSE_kf,\n",
    "      'test estimate MSE standard err=', MSE_kf_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.81      0.75      0.78        59\n",
      "        2.0       0.81      0.87      0.84        71\n",
      "        3.0       0.96      0.93      0.95        46\n",
      "\n",
      "avg / total       0.85      0.85      0.85       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytests, ypreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is worse still!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon_1 = 1-.81 = .19$   \n",
    "$\\epsilon_2 = 1-.81 = .19$   \n",
    "$\\epsilon_3 = 1-.96 = .04$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
