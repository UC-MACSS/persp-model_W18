{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS8\n",
    "\n",
    "MACS 30100 Dr. Evens\n",
    "\n",
    "Jie Heng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cultivar</th>\n",
       "      <th>alco</th>\n",
       "      <th>malic</th>\n",
       "      <th>ash</th>\n",
       "      <th>alk</th>\n",
       "      <th>magn</th>\n",
       "      <th>tot_phen</th>\n",
       "      <th>flav</th>\n",
       "      <th>nonfl_phen</th>\n",
       "      <th>proanth</th>\n",
       "      <th>color_int</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280rat</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cultivar   alco  malic   ash   alk  magn  tot_phen  flav  nonfl_phen  \\\n",
       "0         1  14.23   1.71  2.43  15.6   127      2.80  3.06        0.28   \n",
       "1         1  13.20   1.78  2.14  11.2   100      2.65  2.76        0.26   \n",
       "2         1  13.16   2.36  2.67  18.6   101      2.80  3.24        0.30   \n",
       "3         1  14.37   1.95  2.50  16.8   113      3.85  3.49        0.24   \n",
       "4         1  13.24   2.59  2.87  21.0   118      2.80  2.69        0.39   \n",
       "\n",
       "   proanth  color_int   hue  OD280rat  proline  \n",
       "0     2.29       5.64  1.04      3.92     1065  \n",
       "1     1.28       4.38  1.05      3.40     1050  \n",
       "2     2.81       5.68  1.03      3.17     1185  \n",
       "3     2.18       7.80  0.86      3.45     1480  \n",
       "4     1.82       4.32  1.04      2.93      735  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strongdrink = pd.read_csv('strongdrink.txt')\n",
    "strongdrink.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Create a scatterplot of the data where the x-variable is alcohol (alco) and the y-variable is color intensity (color int). Make the dot of each of the three possible cultivar types a diﬀerent color. Make sure your plot has a legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23fbc346c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cultivars = strongdrink.groupby('cultivar')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for kind, cultivar in cultivars:\n",
    "    ax.plot(cultivar['alco'], cultivar['color_int'], marker = 'o',linestyle='', ms=4, label=kind)\n",
    "ax.legend()\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Color Intensity')\n",
    "plt.title('Plot1: Alcohol and Color Intensity', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)  Use sklearn.linear model.LogisticRegression to ﬁt a multinomial logistic model of cultivar on features alcohol (alco), malic acid (malic), total phenols (tot phen), and color intensity (color int) with the following linear predictor. Use k-fold cross-validation to estimate the MSE of the multinomial logit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C       MSE\n",
       "43  0.44  0.068182\n",
       "44  0.45  0.068182\n",
       "42  0.43  0.068182\n",
       "41  0.42  0.068182\n",
       "40  0.41  0.068182\n",
       "39  0.40  0.068182\n",
       "27  0.28  0.068182\n",
       "28  0.29  0.068182\n",
       "29  0.30  0.068182\n",
       "30  0.31  0.068182\n",
       "31  0.32  0.068182\n",
       "32  0.33  0.068182\n",
       "33  0.34  0.068182\n",
       "34  0.35  0.068182\n",
       "35  0.36  0.068182\n",
       "36  0.37  0.068182\n",
       "37  0.38  0.068182\n",
       "46  0.47  0.068182\n",
       "45  0.46  0.068182\n",
       "38  0.39  0.068182\n",
       "47  0.48  0.068182\n",
       "56  0.57  0.073864\n",
       "54  0.55  0.073864\n",
       "26  0.27  0.073864\n",
       "25  0.26  0.073864"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = strongdrink[['alco', 'malic', 'tot_phen', 'color_int']].values\n",
    "y = strongdrink['cultivar'].values\n",
    "k = 4\n",
    "clf_mlog = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_mlog.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "C = np.zeros(200)\n",
    "MSE_C = np.zeros(200)\n",
    "\n",
    "for c in range(200):\n",
    "    k_ind = int(0)\n",
    "    for train_index, test_index in clf_mlog.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        LogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                    solver='newton-cg', C = (c/100 + 0.01))\n",
    "        LogReg.fit(X_train, y_train)\n",
    "        y_pred = LogReg.predict(X_test)\n",
    "        error = y_test != y_pred\n",
    "        MSE[k_ind] = error.mean()\n",
    "        k_ind += 1\n",
    "    C[c] = c/100 + 0.01\n",
    "    MSE_C[c] =  MSE.mean()\n",
    "MSE_multilog = pd.DataFrame({'C':C, 'MSE':MSE_C})\n",
    "MSE_multilog.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table, we can see that C takes value between 0,28 and 0.44 (penalty is 'l2') and when C = 0.44, penalty is 'l2', we get the lowest MSE: 0.068182."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Use sklearn.ensemble.RandomForestClassifier to ﬁt a random forest model of cultivar on the same four features used in part (b). set bootstrap=True, set oob score=True, and set random state=22. Use OOB cross-validation to generate the MSE of your random forest classiﬁer. Play with the values of the tuning parameters n estimators, max depth, and min samples leaf to try and ﬁnd the lowest possible MSE from the OOB cross validation. Report your minimized overall MSE along with the tuning parameter values you used for n estimators, max depth, and min samples leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_rf = pd.DataFrame({\"n_estimator\" : np.zeros(5),\n",
    "                         \"min_sample\" : np.zeros(5),\n",
    "                         \"max_depth\" : np.zeros(5),\n",
    "                         \"MSE\" : np.zeros(5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_df = MSE_rf[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        for tree in range(5):\n",
    "            rf = RandomForestClassifier(n_estimators = (tree * 50 + 50),\n",
    "                                        min_samples_leaf = (i * 5 + 5),\n",
    "                                        max_depth = (j + 1), bootstrap=True, \n",
    "                                        oob_score=True, random_state=22)\n",
    "            rf.fit(X, y)\n",
    "            MSE_rf[\"n_estimator\"][tree] = tree * 50 + 50\n",
    "            MSE_rf[\"max_depth\"][tree] = j + 1\n",
    "            MSE_rf[\"min_sample\"][tree] = i * 5 + 5\n",
    "            MSE_rf[\"MSE\"][tree] = 1 - rf.oob_score_\n",
    "        MSE_df= pd.concat([MSE_df, MSE_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_sample</th>\n",
       "      <th>n_estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE  max_depth  min_sample  n_estimator\n",
       "96   0.068182        5.0        10.0        100.0\n",
       "126  0.068182       11.0        10.0        100.0\n",
       "116  0.068182        9.0        10.0        100.0\n",
       "131  0.068182       12.0        10.0        100.0\n",
       "111  0.068182        8.0        10.0        100.0\n",
       "106  0.068182        7.0        10.0        100.0\n",
       "121  0.068182       10.0        10.0        100.0\n",
       "101  0.068182        6.0        10.0        100.0\n",
       "136  0.068182       13.0        10.0        100.0\n",
       "86   0.068182        3.0        10.0        100.0\n",
       "88   0.068182        3.0        10.0        200.0\n",
       "91   0.068182        4.0        10.0        100.0\n",
       "146  0.068182       15.0        10.0        100.0\n",
       "141  0.068182       14.0        10.0        100.0\n",
       "138  0.073864       13.0        10.0        200.0\n",
       "142  0.073864       14.0        10.0        150.0\n",
       "133  0.073864       12.0        10.0        200.0\n",
       "132  0.073864       12.0        10.0        150.0\n",
       "143  0.073864       14.0        10.0        200.0\n",
       "128  0.073864       11.0        10.0        200.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_df.index = range(len(MSE_df))\n",
    "MSE_df.sort_values(['MSE']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table, we find that when max_depth is 5, min_sample_leaf is 10 and n_estimator is 100, we get get the lowest MSE：0.068182 from the random forest model ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Use sklearn.svm.SVC to ﬁt a support vector machines model of cultivar with a Gaussian radial basis function kernel kernel=’rbf’ on the four features used in parts (b) and (c). Fit the model using k-fold cross validation with k = 4 folds exactly as in part (b). Play with the penalty parameter C and the coeﬃcient on the radial basis function gamma to try and ﬁnd the lowest possible MSE from the k-fold cross validation. Report your minimized overall MSE along with the tuning parameter values you used for C and gamma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>G</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>3.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>3.30</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2.95</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>3.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>3.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2.90</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         C     G       MSE\n",
       "2659  1.00  1.70  0.045455\n",
       "2578  0.95  1.65  0.045455\n",
       "2658  0.95  1.70  0.045455\n",
       "224   3.25  0.15  0.051136\n",
       "225   3.30  0.15  0.051136\n",
       "226   3.35  0.15  0.051136\n",
       "227   3.40  0.15  0.051136\n",
       "228   3.45  0.15  0.051136\n",
       "229   3.50  0.15  0.051136\n",
       "218   2.95  0.15  0.051136\n",
       "230   3.55  0.15  0.051136\n",
       "231   3.60  0.15  0.051136\n",
       "232   3.65  0.15  0.051136\n",
       "233   3.70  0.15  0.051136\n",
       "234   3.75  0.15  0.051136\n",
       "235   3.80  0.15  0.051136\n",
       "223   3.20  0.15  0.051136\n",
       "222   3.15  0.15  0.051136\n",
       "221   3.10  0.15  0.051136\n",
       "220   3.05  0.15  0.051136\n",
       "219   3.00  0.15  0.051136\n",
       "217   2.90  0.15  0.051136\n",
       "216   2.85  0.15  0.051136\n",
       "215   2.80  0.15  0.051136\n",
       "214   2.75  0.15  0.051136"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 4\n",
    "clf_svm = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_svm.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "MSE_C = pd.DataFrame({\"C\" : np.zeros(80),\n",
    "                      \"G\" : np.zeros(80),\n",
    "                      \"MSE\" : np.zeros(80)})\n",
    "MSE_SVM = MSE_C[:0]\n",
    "\n",
    "for g in range(80):\n",
    "    for c in range(80):\n",
    "        k_ind = int(0)\n",
    "        for train_index, test_index in clf_svm.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            svc = svm.SVC(kernel='rbf', gamma = (g/20 + 0.05),\n",
    "                          C=c/20 + 0.05)\n",
    "            svc.fit(X_train, y_train)\n",
    "            y_pred = svc.predict(X_test)\n",
    "            error = y_test != y_pred\n",
    "            MSE[k_ind] = error.mean()\n",
    "            k_ind += 1\n",
    "        MSE_C['C'][c] = c/20 + 0.05\n",
    "        MSE_C['G'][c] = g/20 + 0.05\n",
    "        MSE_C['MSE'][c] =  MSE.mean()\n",
    "    MSE_SVM = pd.concat([MSE_SVM, MSE_C])\n",
    "    \n",
    "MSE_SVM.index = range(len(MSE_SVM))\n",
    "MSE_SVM.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cost is 1, gamma is 1.7, we get the lowest MSE: 0.045455 from the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)  Use sklearn.neural network.MLPClassifier to ﬁt a single hidden layer neural network model of cultivar. Fit the model using k-fold cross validation with k = 4 folds exactly as in parts (b) and (d). Play with the tuning parameters of the hidden layer sizes hidden layer sizes, activation function activation, and the regularization penalty alpha to try and ﬁnd the lowest possible MSE from the k-fold cross validation. Report your minimized overall MSE along with the tuning parameter values you used for hidden layer sizes, activation, and alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "clf_mlp = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_mlp.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "activ = np.array(['identity', 'logistic', 'tanh', 'relu'])\n",
    "MSE_al = pd.DataFrame({'activation' : np.zeros(20),\n",
    "                       'hidden layer' : np.zeros(20),\n",
    "                       'alpha' : np.zeros(20),\n",
    "                       'MSE':np.zeros(20)})\n",
    "MSE_mlp = MSE_al[:0]\n",
    "\n",
    "for ac in range(4):\n",
    "    for h in range(8):\n",
    "        for al in range(20):\n",
    "            k_ind = int(0)\n",
    "            for train_index, test_index in clf_mlp.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                mlp = MLPClassifier(activation=activ[ac], solver='lbfgs',\n",
    "                                    alpha=(al/20 + 0.05), \n",
    "                                    hidden_layer_sizes = ((50 * (h + 1)),))\n",
    "                mlp.fit(X_train, y_train)\n",
    "                y_pred = mlp.predict(X_test)\n",
    "                error = y_test != y_pred\n",
    "                MSE[k_ind] = error.mean()\n",
    "                k_ind += 1\n",
    "            MSE_al['activation'][al] = activ[ac]\n",
    "            MSE_al['hidden layer'][al] = 50 * (h + 1)\n",
    "            MSE_al['alpha'][al] = al/20 + 0.05\n",
    "            MSE_al['MSE'][al] =  MSE.mean()\n",
    "        MSE_mlp = pd.concat([MSE_mlp, MSE_al])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>hidden layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.85</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.20</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.20</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.65</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.30</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.85</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.65</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.40</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.65</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.55</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.85</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.55</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE activation  alpha  hidden layer\n",
       "496  0.039773       relu   0.85          50.0\n",
       "542  0.039773       relu   0.15         200.0\n",
       "551  0.039773       relu   0.60         200.0\n",
       "482  0.039773       relu   0.15          50.0\n",
       "520  0.039773       relu   0.05         150.0\n",
       "553  0.039773       relu   0.70         200.0\n",
       "563  0.039773       relu   0.20         250.0\n",
       "623  0.039773       relu   0.20         400.0\n",
       "628  0.039773       relu   0.45         400.0\n",
       "604  0.045455       relu   0.25         350.0\n",
       "513  0.045455       relu   0.70         100.0\n",
       "552  0.045455       relu   0.65         200.0\n",
       "605  0.045455       relu   0.30         350.0\n",
       "536  0.045455       relu   0.85         150.0\n",
       "613  0.045455       relu   0.70         350.0\n",
       "612  0.045455       relu   0.65         350.0\n",
       "547  0.045455       relu   0.40         200.0\n",
       "584  0.045455       relu   0.25         300.0\n",
       "512  0.045455       relu   0.65         100.0\n",
       "590  0.045455       relu   0.55         300.0\n",
       "573  0.045455       relu   0.70         250.0\n",
       "636  0.045455       relu   0.85         400.0\n",
       "560  0.045455       relu   0.05         250.0\n",
       "570  0.045455       relu   0.55         250.0\n",
       "561  0.045455       relu   0.10         250.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_mlp.index = range(len(MSE_mlp))\n",
    "MSE_mlp.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When activation is relu, alpha is 0.85, hidden layer is 50, we get the lowest MSE: 0.039773\t from the MLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Which of the above three models do you think is the best predictor of cultivar? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logit</th>\n",
       "      <th>MLP</th>\n",
       "      <th>RF</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Logit       MLP        RF       SVM\n",
       "0  0.068182  0.039773  0.068182  0.045455\n",
       "1  0.068182  0.039773  0.068182  0.045455\n",
       "2  0.068182  0.039773  0.068182  0.045455\n",
       "3  0.068182  0.039773  0.068182  0.051136\n",
       "4  0.068182  0.039773  0.068182  0.051136"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_compare = np.array(MSE_multilog.sort_values(['MSE']).head(5)['MSE'])\n",
    "RF_compare = np.array(MSE_df.sort_values(['MSE']).head(5)['MSE'])\n",
    "SVM_compare = np.array(MSE_SVM.sort_values(['MSE']).head(5)['MSE'])\n",
    "mlp_compare = np.array(MSE_mlp.sort_values(['MSE']).head(5)['MSE'])\n",
    "\n",
    "\n",
    "Compare = pd.DataFrame({'Logit':logit_compare, \n",
    "                    'RF':RF_compare,\n",
    "                    'SVM':SVM_compare,\n",
    "                    'MLP':mlp_compare})\n",
    "Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, we find that the MLP model has the lowest MSE values. So, the best predictor of cultivar is MLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
