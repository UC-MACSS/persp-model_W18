{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Li Ruixue\n",
    "## 1. Multinomial logistic regression and cross validation\n",
    "### (a) Use a multinomial logistic regression model of the following form with the following linear predictor \u0011j for j = 1; 2 (the baseline class is j = 3).\n",
    " \n",
    "### Report your two sets of estimated coeffcients for j = 1 and j = 2. Report your error rates (1 - precision) on the test set using the code below. Which category of cultivar is the model best at predicting? Is the most accurately predicted category the one with the most observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "import statsmodels.api as sm\n",
    "from scipy.interpolate import LSQUnivariateSpline\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv('data/strongdrink.txt')\n",
    "wine = wine[['cultivar', 'alco', 'malic', 'tot_phen', 'color_int']]\n",
    "wine.head()\n",
    "len(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cultivar     0\n",
       "alco         0\n",
       "malic        0\n",
       "tot_phen     0\n",
       "color_int    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    71\n",
       "1    59\n",
       "3    46\n",
       "Name: cultivar, dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.cultivar.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liruixue/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For j = 1\n",
      "cultivar = -24.01 + 1.7 alco + -0.27 malic + 1.22 tot_phen + 0.02 color_int\n",
      "\n",
      "For j = 2\n",
      "cultivar = 22.8 + -1.47 alco + -0.33 malic + 0.66 tot_phen + -0.92 color_int\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.87      1.00      0.93        13\n",
      "          2       1.00      0.90      0.95        21\n",
      "          3       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.96      0.95      0.96        44\n",
      "\n",
      "Validation set MSE =  0.045454545454545456\n"
     ]
    }
   ],
   "source": [
    "X = wine[['alco', 'malic', 'tot_phen', 'color_int']]\n",
    "y = wine[['cultivar']]\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size = 0.25, random_state=20)\n",
    "MultLogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                solver='newton-cg')\n",
    "MultLogReg.fit(X_train, y_train)\n",
    "y_pred = MultLogReg.predict(X_test)\n",
    "for i in range(0,2):\n",
    "    print('For j = ' + str(i+1) )\n",
    "    print('cultivar = ' + str(round(MultLogReg.intercept_[i],2))+' + '+\n",
    "         str(round(MultLogReg.coef_[i][0], 2))+' alco + '+\n",
    "         str(round(MultLogReg.coef_[i][1], 2))+' malic + '+\n",
    "         str(round(MultLogReg.coef_[i][2], 2))+' tot_phen + '+\n",
    "         str(round(MultLogReg.coef_[i][3], 2))+' color_int')\n",
    "    print()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_test = np.array([y_test['cultivar']])\n",
    "print('Validation set MSE = ', (y_test != y_pred).astype(int).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rates are:  \n",
    "j = 1: 1 - 0.87 = 0.13  \n",
    "j = 2: 1 - 1 = 0  \n",
    "j = 3: 1 - 1 = 0  \n",
    "The category with the most observations is 2, which is perfectly predicted in this dataset, but so is category 3, which has the least observations. Meanwhile, category 1 which has the median number of observations was not predicted as well, so it's hard to say based on this test that the most accurately predicted category is the one with the most observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Perform a leave-one-out cross validation (LOOCV) with the model from part (a). Report your error rates (1 - precision) for each category? How do your error rates compare to those from part (a)? Report your LOOCV estimate for the test MSE as the average MSE, where yi is the left out observation from each test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xvars = X.values\n",
    "yvals = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liruixue/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "N_loo = Xvars.shape[0]\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(Xvars)\n",
    "MSE_vec = np.zeros(N_loo)\n",
    "y_pred_vec = np.zeros(N_loo)\n",
    "y_test_vec = np.zeros(N_loo)\n",
    "\n",
    "for train_index, test_index in loo.split(Xvars):\n",
    "    X_train, X_test = Xvars[train_index], Xvars[test_index]\n",
    "    y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "    LogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                solver='newton-cg')\n",
    "    LogReg.fit(X_train, y_train)\n",
    "    y_pred = LogReg.predict(X_test)\n",
    "    MSE_vec[test_index] = (y_test != y_pred).astype(int)\n",
    "    y_pred_vec[test_index] = y_pred\n",
    "    y_test_vec[test_index] = y_test\n",
    "\n",
    "print(classification_report(y_test_vec, y_pred_vec))\n",
    "MSE_loo = MSE_vec.mean()\n",
    "print('test estimate MSE loocv=', MSE_loo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rates are:    \n",
    "j = 1: 1 - 0.90 = 0.10    \n",
    "j = 2: 1 - 0.91 = 0.09  \n",
    "j = 3: 1 - 0.96 = 0.04  \n",
    "The average error is bigger than part (a), so is the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Perform a k-fold cross validation in which the data are divided into k = 4 groups. Use the following code. Report your error rates (1 - precision) for each category. How do your error rates compare to those from parts (a) and (b)? Report your k-fold estimate for the test MSE as the average MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "kf = KFold(n_splits=k, random_state=10, shuffle=True)\n",
    "kf.get_n_splits(Xvars)\n",
    "\n",
    "MSE_vec_kf = np.zeros(k)\n",
    "y_pred_vec = np.array([])\n",
    "y_test_vec = np.array([])\n",
    "\n",
    "k_ind = int(0)\n",
    "for train_index, test_index in kf.split(Xvars):\n",
    "    print('\\n\\n')\n",
    "    print('Group ', k_ind)\n",
    "    X_train, X_test = Xvars[train_index], Xvars[test_index]\n",
    "    y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "    LogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                solver='newton-cg')\n",
    "    LogReg.fit(X_train, y_train)\n",
    "    y_pred = LogReg.predict(X_test)\n",
    "    y_pred_vec = np.append(y_pred_vec, y_pred)\n",
    "    y_test_vec = np.append(y_test_vec, y_test)\n",
    "    y_test = np.reshape(y_test, (44,1))\n",
    "    y_pred = np.reshape(y_pred, (44,1))\n",
    "    MSE_vec_kf[k_ind] = ((y_test != y_pred).astype(int)).mean()\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('MSE for test set', k_ind, ' is', MSE_vec_kf[k_ind])\n",
    "    k_ind += 1\n",
    "\n",
    "MSE_kf = MSE_vec_kf.mean()\n",
    "MSE_kf_std = MSE_vec_kf.std()\n",
    "print('\\n\\n')\n",
    "print(classification_report(y_test_vec, y_pred_vec))\n",
    "print('test estimate MSE k-fold=', MSE_kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rates are:    \n",
    "j = 1: 1 - 0.87 = 0.13    \n",
    "j = 2: 1 - 0.91 = 0.09  \n",
    "j = 3: 1 - 0.96 = 0.04\n",
    "It's similar to Leave One Out, and bigger than multinomial logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splines and interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Create a scatterplot of the data with age on the x-axis and Coolness Index on the y-axis. Label your axes, and give the plot a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool = pd.read_csv('data/CoolIndex.txt', names = ['age', 'coolness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.scatter(cool.age, cool.coolness)\n",
    "plt.xlabel('age', fontsize = 20)\n",
    "plt.ylabel('coolness index', fontsize = 20)\n",
    "plt.title('Coolness vs Age', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Use ordinary least squares (OLS) regression to \f",
    "t a stepwise function to these data. Use 5 bins [11; 22), [22; 40), [40; 59), [59; 77), [77; 95]. Remem- ber that your dummy variables must be integer type (0, 1), not boolean type (True, False). Plot this step function on top of the scatterplot of the data from part (a). Label your axes, include a legend, and give the plot a title. Report your estimated step function values for each bin [\f",
    "1; \f",
    "2; \f",
    "3; \f",
    "4; \f",
    "5]. What is the predicted coolness of a 73-year old from the stepwise function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bin1 = ((age >= 11) & (age < 22)).astype(int)\n",
    "age_bin2 = ((age >= 22) & (age < 40)).astype(int)\n",
    "age_bin3 = ((age >= 40) & (age < 59)).astype(int)\n",
    "age_bin4 = ((age >= 59) & (age < 77)).astype(int)\n",
    "age_bin5 = ((age >= 77) & (age <= 95)).astype(int)\n",
    "X_step = pd.DataFrame(dict(age_bin1=age_bin1, age_bin2=age_bin2,\n",
    "                           age_bin3=age_bin3, age_bin4=age_bin4,\n",
    "                           age_bin5=age_bin5))\n",
    "X_step.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = sm.OLS(endog=cool['coolness'], exog=X_step, missing='drop')\n",
    "reg_results = reg.fit()\n",
    "y_pred = reg_results.predict(X_step)\n",
    "print(reg_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_step = np.array([11, 22, 40, 59, 77, 95])\n",
    "cool_step = np.append(reg_results.params[0], reg_results.params)\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.step(age_step, cool_step, color = 'r', label = 'stepwise')\n",
    "plt.scatter(cool.age, cool.coolness, label = 'scatter')\n",
    "plt.xlabel('age', fontsize = 20)\n",
    "plt.ylabel('coolness index', fontsize = 20)\n",
    "plt.title('Coolness vs Age', fontsize = 25)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reg_results.params)):\n",
    "    print('b' + str(i) + ' is ' + '{:.2f}'.format(reg_results.params[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_73 = reg_results.predict([0,0,0,1,0])\n",
    "print('Predicted coolness of a 73-year old is ' + '{:.2f}'.format(y_73[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Fit a linear spline (continuous) to the data over the 5 age bins from part (b). Use the scipy.interpolate.LQUnivariateSpline function with k = 1 (linear) and the knots equal to t =[22,40,59,77]. Plot your continuous linear spline against a scatterplot of the data from part (a) and the estimated step function from part (b). Label your axes, include a legend, and give the plot a title. What is the predicted coolness of a 73-year old from the linear spline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [22, 40, 59, 77]\n",
    "spl_linear = LSQUnivariateSpline(cool.age.values, cool.coolness.values, t, k=1)\n",
    "y_pred_linear = spl_linear(cool.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.step(age_step, cool_step, color = 'r', label = 'stepwise')\n",
    "plt.scatter(cool.age, cool.coolness, label = 'scatter')\n",
    "plt.plot(cool.age, y_pred_linear, color = 'y', label = 'linear spline')\n",
    "plt.xlabel('age', fontsize = 20)\n",
    "plt.ylabel('coolness index', fontsize = 20)\n",
    "plt.title('Coolness vs Age', fontsize = 25)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_73 = spl_linear(73)\n",
    "print('Predicted coolness of a 73-year old is ' + '{:.2f}'.format(y_73))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Fit a cubic spline (continuous) to the data over the 5 age bins from part (b). Use the scipy.interpolate.LQUnivariateSpline function with k = 3 (cubic) and the knots equal to t =[22,40,59,77]. Plot your continuous cubic spline against a scatterplot of the data from part (a) and the es- timated step function from part (b), and the linear spline from part (c). Label your axes, include a legend, and give the plot a title. What is the predicted coolness of a 73-year old from the cubic spline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_cubic = LSQUnivariateSpline(cool.age.values, cool.coolness.values, t, k=3)\n",
    "y_pred_cubic = spl_cubic(cool.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.step(age_step, cool_step, color = 'r', label = 'stepwise')\n",
    "plt.scatter(cool.age, cool.coolness, label = 'scatter')\n",
    "plt.plot(cool.age, y_pred_linear, color = 'y', label = 'linear spline')\n",
    "plt.plot(cool.age, y_pred_cubic, color = 'b', label = 'cubic spline')\n",
    "plt.xlabel('age', fontsize = 20)\n",
    "plt.ylabel('coolness index', fontsize = 20)\n",
    "plt.title('Coolness vs Age', fontsize = 25)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_73 = spl_cubic(73)\n",
    "print('Predicted coolness of a 73-year old is ' + '{:.2f}'.format(y_73))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
