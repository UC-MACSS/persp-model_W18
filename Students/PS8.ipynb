{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS8 \n",
    "## Yangyang Dai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangyangdai/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drink = pd.read_csv('strongdrink.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cultivar</th>\n",
       "      <th>alco</th>\n",
       "      <th>malic</th>\n",
       "      <th>ash</th>\n",
       "      <th>alk</th>\n",
       "      <th>magn</th>\n",
       "      <th>tot_phen</th>\n",
       "      <th>flav</th>\n",
       "      <th>nonfl_phen</th>\n",
       "      <th>proanth</th>\n",
       "      <th>color_int</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280rat</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cultivar   alco  malic   ash   alk  magn  tot_phen  flav  nonfl_phen  \\\n",
       "0         1  14.23   1.71  2.43  15.6   127      2.80  3.06        0.28   \n",
       "1         1  13.20   1.78  2.14  11.2   100      2.65  2.76        0.26   \n",
       "2         1  13.16   2.36  2.67  18.6   101      2.80  3.24        0.30   \n",
       "3         1  14.37   1.95  2.50  16.8   113      3.85  3.49        0.24   \n",
       "4         1  13.24   2.59  2.87  21.0   118      2.80  2.69        0.39   \n",
       "\n",
       "   proanth  color_int   hue  OD280rat  proline  \n",
       "0     2.29       5.64  1.04      3.92     1065  \n",
       "1     1.28       4.38  1.05      3.40     1050  \n",
       "2     2.81       5.68  1.03      3.17     1185  \n",
       "3     2.18       7.80  0.86      3.45     1480  \n",
       "4     1.82       4.32  1.04      2.93      735  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drink.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a  : Plot a scatterplot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAGDCAYAAAA2xlnwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nySi4i4iIlMz/B0xwySYlcOOIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e1c080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = drink.groupby('cultivar')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "for name, group in groups:\n",
    "    ax.plot(group['alco'], group['color_int'], marker='o', linestyle='', label=name)\n",
    "ax.legend()\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Color Intensity')\n",
    "plt.title('Scatterplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b: Uses sklearn to fit multinomial logit. Use k-fold cross validation to estimate MSE. Play with param values of penalty and C to get lowest possible k-fold MSE. Report the MSE and param results from this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = drink[['alco', 'malic', 'tot_phen', 'color_int']].values\n",
    "y = drink['cultivar'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "kf = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "C = np.zeros(200)\n",
    "MSE_C = np.zeros(200)\n",
    "\n",
    "for c in range(200):\n",
    "    k_ind = int(0)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        LogReg = LogisticRegression(multi_class='multinomial', fit_intercept = True,\n",
    "                                    solver='newton-cg', C = (c/100 + 0.01))\n",
    "        LogReg.fit(X_train, y_train)\n",
    "        y_pred = LogReg.predict(X_test)\n",
    "        error = y_test != y_pred\n",
    "        MSE[k_ind] = error.mean()\n",
    "        k_ind += 1\n",
    "    C[c] = c/100 + 0.01\n",
    "    MSE_C[c] =  MSE.mean()\n",
    "MSE_logit = pd.DataFrame({'C':C, 'MSE':MSE_C})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C       MSE\n",
       "43  0.44  0.068182\n",
       "44  0.45  0.068182\n",
       "42  0.43  0.068182\n",
       "41  0.42  0.068182\n",
       "40  0.41  0.068182\n",
       "39  0.40  0.068182\n",
       "27  0.28  0.068182\n",
       "28  0.29  0.068182\n",
       "29  0.30  0.068182\n",
       "30  0.31  0.068182\n",
       "31  0.32  0.068182\n",
       "32  0.33  0.068182\n",
       "33  0.34  0.068182\n",
       "34  0.35  0.068182\n",
       "35  0.36  0.068182\n",
       "36  0.37  0.068182\n",
       "37  0.38  0.068182\n",
       "46  0.47  0.068182\n",
       "45  0.46  0.068182\n",
       "38  0.39  0.068182\n",
       "47  0.48  0.068182\n",
       "56  0.57  0.073864\n",
       "54  0.55  0.073864\n",
       "26  0.27  0.073864\n",
       "25  0.26  0.073864\n",
       "48  0.49  0.073864\n",
       "52  0.53  0.073864\n",
       "51  0.52  0.073864\n",
       "50  0.51  0.073864\n",
       "49  0.50  0.073864"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = MSE_logit.sort_values(['MSE'])\n",
    "df_new.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that when C = 0.44, penalty is 'l2', lowest MSE = 0.068182."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c: Use sklearn to fit the model in part b using a Random Forest classifier with bootstrap = True, oob_score=True, and random_state=22. Use OOB cross-validation to get MSE. Play with param values of n_estimators, max_depth, min_sample_leaf. Report the MSE and param values from this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "MSE_loop = pd.DataFrame({\"n_estimator\" : np.zeros(5),\n",
    "                         \"min_sample_leaf\" : np.zeros(5),\n",
    "                         \"max_depth\" : np.zeros(5),\n",
    "                         \"MSE\" : np.zeros(5)})\n",
    "MSE_RF = MSE_loop[:0]\n",
    "\n",
    "for ml in range(15):\n",
    "    for md in range(15):\n",
    "        for tree in range(5):\n",
    "            RF = RandomForestClassifier(n_estimators = (tree * 50 + 50),\n",
    "                                        min_samples_leaf = (ml * 5 + 5),\n",
    "                                        max_depth = (md + 1), bootstrap=True, \n",
    "                                        oob_score=True, random_state=22)\n",
    "            RF.fit(X, y)\n",
    "            MSE_loop[\"n_estimator\"][tree] = tree * 50 + 50\n",
    "            MSE_loop[\"max_depth\"][tree] = md + 1\n",
    "            MSE_loop[\"min_sample_leaf\"][tree] = ml * 5 + 5\n",
    "            MSE_loop[\"MSE\"][tree] = 1 - RF.oob_score_\n",
    "        MSE_RF = pd.concat([MSE_RF, MSE_loop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_sample_leaf</th>\n",
       "      <th>n_estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE  max_depth  min_sample_leaf  n_estimator\n",
       "96   0.068182        5.0             10.0        100.0\n",
       "126  0.068182       11.0             10.0        100.0\n",
       "116  0.068182        9.0             10.0        100.0\n",
       "131  0.068182       12.0             10.0        100.0\n",
       "111  0.068182        8.0             10.0        100.0\n",
       "106  0.068182        7.0             10.0        100.0\n",
       "121  0.068182       10.0             10.0        100.0\n",
       "101  0.068182        6.0             10.0        100.0\n",
       "136  0.068182       13.0             10.0        100.0\n",
       "86   0.068182        3.0             10.0        100.0\n",
       "88   0.068182        3.0             10.0        200.0\n",
       "91   0.068182        4.0             10.0        100.0\n",
       "146  0.068182       15.0             10.0        100.0\n",
       "141  0.068182       14.0             10.0        100.0\n",
       "138  0.073864       13.0             10.0        200.0\n",
       "142  0.073864       14.0             10.0        150.0\n",
       "133  0.073864       12.0             10.0        200.0\n",
       "132  0.073864       12.0             10.0        150.0\n",
       "143  0.073864       14.0             10.0        200.0\n",
       "128  0.073864       11.0             10.0        200.0\n",
       "127  0.073864       11.0             10.0        150.0\n",
       "123  0.073864       10.0             10.0        200.0\n",
       "117  0.073864        9.0             10.0        150.0\n",
       "118  0.073864        9.0             10.0        200.0\n",
       "147  0.073864       15.0             10.0        150.0\n",
       "113  0.073864        8.0             10.0        200.0\n",
       "112  0.073864        8.0             10.0        150.0\n",
       "108  0.073864        7.0             10.0        200.0\n",
       "107  0.073864        7.0             10.0        150.0\n",
       "103  0.073864        6.0             10.0        200.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_RF.index = range(len(MSE_RF))\n",
    "MSE_RF.sort_values(['MSE']).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When max_depth = 5, min_sample_leaf = 10 and n_estimator = 100, the random forest model get the lowest MSE of 0.068182."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1d: Use sklearn to fit the model in part b using a SVM. Do k-folds with k=4 as in part b. Play with param values of C and gamma. Report the MSE and param values from this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "kf = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "MSE_C = pd.DataFrame({\"Cost\" : np.zeros(80),\n",
    "                      \"Gamma\" : np.zeros(80),\n",
    "                      \"MSE\" : np.zeros(80)})\n",
    "MSE_SVM = MSE_C[:0]\n",
    "\n",
    "for g in range(80):\n",
    "    for c in range(80):\n",
    "        k_ind = int(0)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            svc = svm.SVC(kernel='rbf', gamma = (g/20 + 0.05),\n",
    "                          C=c/20 + 0.05)\n",
    "            svc.fit(X_train, y_train)\n",
    "            y_pred = svc.predict(X_test)\n",
    "            error = y_test != y_pred\n",
    "            MSE[k_ind] = error.mean()\n",
    "            k_ind += 1\n",
    "        MSE_C['Cost'][c] = c/20 + 0.05\n",
    "        MSE_C['Gamma'][c] = g/20 + 0.05\n",
    "        MSE_C['MSE'][c] =  MSE.mean()\n",
    "    MSE_SVM = pd.concat([MSE_SVM, MSE_C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cost</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>3.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>3.30</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2.95</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>3.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>3.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2.90</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cost  Gamma       MSE\n",
       "2659  1.00   1.70  0.045455\n",
       "2578  0.95   1.65  0.045455\n",
       "2658  0.95   1.70  0.045455\n",
       "224   3.25   0.15  0.051136\n",
       "225   3.30   0.15  0.051136\n",
       "226   3.35   0.15  0.051136\n",
       "227   3.40   0.15  0.051136\n",
       "228   3.45   0.15  0.051136\n",
       "229   3.50   0.15  0.051136\n",
       "218   2.95   0.15  0.051136\n",
       "230   3.55   0.15  0.051136\n",
       "231   3.60   0.15  0.051136\n",
       "232   3.65   0.15  0.051136\n",
       "233   3.70   0.15  0.051136\n",
       "234   3.75   0.15  0.051136\n",
       "235   3.80   0.15  0.051136\n",
       "223   3.20   0.15  0.051136\n",
       "222   3.15   0.15  0.051136\n",
       "221   3.10   0.15  0.051136\n",
       "220   3.05   0.15  0.051136\n",
       "219   3.00   0.15  0.051136\n",
       "217   2.90   0.15  0.051136\n",
       "216   2.85   0.15  0.051136\n",
       "215   2.80   0.15  0.051136\n",
       "214   2.75   0.15  0.051136\n",
       "213   2.70   0.15  0.051136\n",
       "212   2.65   0.15  0.051136\n",
       "211   2.60   0.15  0.051136\n",
       "210   2.55   0.15  0.051136\n",
       "209   2.50   0.15  0.051136"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_SVM.index = range(len(MSE_SVM))\n",
    "MSE_SVM.sort_values(['MSE']).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that when cost = 1, gamma = 1.7, the SVM model will get the lowest MSE of 0.045455.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1e: Use sklearn to fit the model to MLPC. Repeat k-folds with k=4. Play with params hidden_layer_sizes, activation, and alpha. Report the MSE and param values from this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangyangdai/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yangyangdai/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yangyangdai/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yangyangdai/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "kf = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "activ = np.array(['identity', 'logistic', 'tanh', 'relu'])\n",
    "MSE_al = pd.DataFrame({\"activation\" : np.zeros(20),\n",
    "                       \"hidden layer\" : np.zeros(20),\n",
    "                       \"alpha\" : np.zeros(20),\n",
    "                       \"MSE\":np.zeros(20)})\n",
    "MSE_mlp = MSE_al[:0]\n",
    "\n",
    "for ac in range(4):\n",
    "    for h in range(8):\n",
    "        for al in range(20):\n",
    "            k_ind = int(0)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                mlp = MLPClassifier(activation=activ[ac], solver='lbfgs',\n",
    "                                    alpha=(al/20 + 0.05), random_state=25,\n",
    "                                    hidden_layer_sizes = ((50 * (h + 1)),))\n",
    "                mlp.fit(X_train, y_train)\n",
    "                y_pred = mlp.predict(X_test)\n",
    "                error = y_test != y_pred\n",
    "                MSE[k_ind] = error.mean()\n",
    "                k_ind += 1\n",
    "            MSE_al['activation'][al] = activ[ac]\n",
    "            MSE_al['hidden layer'][al] = 50 * (h + 1)\n",
    "            MSE_al['alpha'][al] = al/20 + 0.05\n",
    "            MSE_al['MSE'][al] =  MSE.mean()\n",
    "        MSE_mlp = pd.concat([MSE_mlp, MSE_al])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>hidden layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.65</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.20</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.40</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.50</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.85</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.50</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.90</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.30</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.50</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.95</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>1.00</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.80</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE activation  alpha  hidden layer\n",
       "601  0.039773       relu   0.10         350.0\n",
       "532  0.039773       relu   0.65         150.0\n",
       "603  0.039773       relu   0.20         350.0\n",
       "553  0.039773       relu   0.70         200.0\n",
       "527  0.039773       relu   0.40         150.0\n",
       "584  0.039773       relu   0.25         300.0\n",
       "609  0.039773       relu   0.50         350.0\n",
       "613  0.039773       relu   0.70         350.0\n",
       "562  0.039773       relu   0.15         250.0\n",
       "616  0.045455       relu   0.85         350.0\n",
       "506  0.045455       relu   0.35         100.0\n",
       "593  0.045455       relu   0.70         300.0\n",
       "509  0.045455       relu   0.50         100.0\n",
       "551  0.045455       relu   0.60         200.0\n",
       "520  0.045455       relu   0.05         150.0\n",
       "548  0.045455       relu   0.45         200.0\n",
       "617  0.045455       relu   0.90         350.0\n",
       "600  0.045455       relu   0.05         350.0\n",
       "525  0.045455       relu   0.30         150.0\n",
       "526  0.045455       relu   0.35         150.0\n",
       "528  0.045455       relu   0.45         150.0\n",
       "531  0.045455       relu   0.60         150.0\n",
       "549  0.045455       relu   0.50         200.0\n",
       "502  0.045455       relu   0.15         100.0\n",
       "618  0.045455       relu   0.95         350.0\n",
       "588  0.045455       relu   0.45         300.0\n",
       "400  0.045455       tanh   0.05         250.0\n",
       "564  0.045455       relu   0.25         250.0\n",
       "499  0.045455       relu   1.00          50.0\n",
       "635  0.045455       relu   0.80         400.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_mlp.index = range(len(MSE_mlp))\n",
    "MSE_mlp.sort_values(['MSE']).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, when activation = 'relu', alpha = 0.65 and hidden_layer_sizes = 150, the MLP model has the lowest MSE of 0.034091."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1f: Which of the above four models is the best predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logit</th>\n",
       "      <th>MLP</th>\n",
       "      <th>RF</th>\n",
       "      <th>SVM</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Logit       MLP        RF       SVM rank\n",
       "0  0.068182  0.039773  0.068182  0.045455    1\n",
       "1  0.068182  0.039773  0.068182  0.045455    2\n",
       "2  0.068182  0.039773  0.068182  0.045455    3\n",
       "3  0.068182  0.039773  0.068182  0.051136    4\n",
       "4  0.068182  0.039773  0.068182  0.051136    5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_logit = np.array(MSE_logit.sort_values(['MSE']).head(5)['MSE'])\n",
    "top_RF = np.array(MSE_RF.sort_values(['MSE']).head(5)['MSE'])\n",
    "top_SVM = np.array(MSE_SVM.sort_values(['MSE']).head(5)['MSE'])\n",
    "top_mlp = np.array(MSE_mlp.sort_values(['MSE']).head(5)['MSE'])\n",
    "rank = ['1', '2', '3', '4', '5']\n",
    "\n",
    "comparison = pd.DataFrame({'Logit':top_logit, \n",
    "                    'RF':top_RF,\n",
    "                    'SVM':top_SVM,\n",
    "                    'MLP':top_mlp,\n",
    "                    'rank':rank})\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is MLP (activation = 'relu', alpha = 0.65 and hidden_layer_sizes = 150) with lowest MSE (0.034091)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
