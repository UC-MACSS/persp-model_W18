{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 8 (Match 12, 2018)\n",
    "## MACS 30100\n",
    "## Dr. Rick Evans\n",
    "## Kevin Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinsun/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cultivar</th>\n",
       "      <th>alco</th>\n",
       "      <th>malic</th>\n",
       "      <th>ash</th>\n",
       "      <th>alk</th>\n",
       "      <th>magn</th>\n",
       "      <th>tot_phen</th>\n",
       "      <th>flav</th>\n",
       "      <th>nonfl_phen</th>\n",
       "      <th>proanth</th>\n",
       "      <th>color_int</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280rat</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cultivar   alco  malic   ash   alk  magn  tot_phen  flav  nonfl_phen  \\\n",
       "0         1  14.23   1.71  2.43  15.6   127      2.80  3.06        0.28   \n",
       "1         1  13.20   1.78  2.14  11.2   100      2.65  2.76        0.26   \n",
       "2         1  13.16   2.36  2.67  18.6   101      2.80  3.24        0.30   \n",
       "3         1  14.37   1.95  2.50  16.8   113      3.85  3.49        0.24   \n",
       "4         1  13.24   2.59  2.87  21.0   118      2.80  2.69        0.39   \n",
       "\n",
       "   proanth  color_int   hue  OD280rat  proline  \n",
       "0     2.29       5.64  1.04      3.92     1065  \n",
       "1     1.28       4.38  1.05      3.40     1050  \n",
       "2     2.81       5.68  1.03      3.17     1185  \n",
       "3     2.18       7.80  0.86      3.45     1480  \n",
       "4     1.82       4.32  1.04      2.93      735  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset \n",
    "strongdrink = pd.read_csv('data/strongdrink.txt')\n",
    "strongdrink.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAHzCAYAAAB7ZIpqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n14dBkeEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0d26a198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create scatterplot of the data\n",
    "cultivars = strongdrink.groupby('cultivar')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for kind, cultivar in cultivars:\n",
    "    ax.plot(cultivar['alco'], cultivar['color_int'], marker = 'o',linestyle='', ms=4, label=kind)\n",
    "ax.legend()\n",
    "plt.xlabel('Alcohol', fontsize = 12)\n",
    "plt.ylabel('Color Intensity', fontsize = 12)\n",
    "plt.title('Scatterplot of Alcohol and Color Intensity',fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define endogenous and exogenous vars\n",
    "X = strongdrink[['alco', 'malic', 'tot_phen', 'color_int']].values\n",
    "y = strongdrink['cultivar'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "clf_mlog = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_mlog.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "C = np.zeros(200)\n",
    "MSE_C = np.zeros(200)\n",
    "\n",
    "for c in range(200):\n",
    "    k_ind = int(0)\n",
    "    for train_index, test_index in clf_mlog.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        LogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                    solver='newton-cg', C = (c/100 + 0.01))\n",
    "        LogReg.fit(X_train, y_train)\n",
    "        y_pred = LogReg.predict(X_test)\n",
    "        error = y_test != y_pred\n",
    "        MSE[k_ind] = error.mean()\n",
    "        k_ind += 1\n",
    "    C[c] = c/100 + 0.01\n",
    "    MSE_C[c] =  MSE.mean()\n",
    "MSE_multilog = pd.DataFrame({'C':C, 'MSE':MSE_C})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C       MSE\n",
       "43  0.44  0.068182\n",
       "44  0.45  0.068182\n",
       "42  0.43  0.068182\n",
       "41  0.42  0.068182\n",
       "40  0.41  0.068182\n",
       "39  0.40  0.068182\n",
       "27  0.28  0.068182\n",
       "28  0.29  0.068182\n",
       "29  0.30  0.068182\n",
       "30  0.31  0.068182\n",
       "31  0.32  0.068182\n",
       "32  0.33  0.068182\n",
       "33  0.34  0.068182\n",
       "34  0.35  0.068182\n",
       "35  0.36  0.068182\n",
       "36  0.37  0.068182\n",
       "37  0.38  0.068182\n",
       "46  0.47  0.068182\n",
       "45  0.46  0.068182\n",
       "38  0.39  0.068182"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_multilog.sort_values(['MSE']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C takes value between 0,28 and 0.44 w/ penalty of 'l2'. In these instances, the multinomial logistic model has the lowest MSE at 0.068182."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_rf = pd.DataFrame({\"n_estimator\" : np.zeros(5),\n",
    "                         \"min_sample\" : np.zeros(5),\n",
    "                         \"max_depth\" : np.zeros(5),\n",
    "                         \"MSE\" : np.zeros(5)})\n",
    "MSE_df = MSE_rf[:0]\n",
    "\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        for tree in range(5):\n",
    "            rf = RandomForestClassifier(n_estimators = (tree * 50 + 50),\n",
    "                                        min_samples_leaf = (i * 5 + 5),\n",
    "                                        max_depth = (j + 1), bootstrap=True, \n",
    "                                        oob_score=True, random_state=22)\n",
    "            rf.fit(X, y)\n",
    "            MSE_rf[\"n_estimator\"][tree] = tree * 50 + 50\n",
    "            MSE_rf[\"max_depth\"][tree] = j + 1\n",
    "            MSE_rf[\"min_sample\"][tree] = i * 5 + 5\n",
    "            MSE_rf[\"MSE\"][tree] = 1 - rf.oob_score_\n",
    "        MSE_df= pd.concat([MSE_df, MSE_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_sample</th>\n",
       "      <th>n_estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE  max_depth  min_sample  n_estimator\n",
       "96   0.068182        5.0        10.0        100.0\n",
       "126  0.068182       11.0        10.0        100.0\n",
       "116  0.068182        9.0        10.0        100.0\n",
       "131  0.068182       12.0        10.0        100.0\n",
       "111  0.068182        8.0        10.0        100.0\n",
       "106  0.068182        7.0        10.0        100.0\n",
       "121  0.068182       10.0        10.0        100.0\n",
       "101  0.068182        6.0        10.0        100.0\n",
       "136  0.068182       13.0        10.0        100.0\n",
       "86   0.068182        3.0        10.0        100.0\n",
       "88   0.068182        3.0        10.0        200.0\n",
       "91   0.068182        4.0        10.0        100.0\n",
       "146  0.068182       15.0        10.0        100.0\n",
       "141  0.068182       14.0        10.0        100.0\n",
       "138  0.073864       13.0        10.0        200.0\n",
       "142  0.073864       14.0        10.0        150.0\n",
       "133  0.073864       12.0        10.0        200.0\n",
       "132  0.073864       12.0        10.0        150.0\n",
       "143  0.073864       14.0        10.0        200.0\n",
       "128  0.073864       11.0        10.0        200.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_df.index = range(len(MSE_df))\n",
    "MSE_df.sort_values(['MSE']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a max_depth of 5, min_sample of 10, and n_estimator of 100,  the random forest model has a MSE of 0.068182. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "clf_svm = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_svm.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "MSE_C = pd.DataFrame({\"C\" : np.zeros(80),\n",
    "                      \"G\" : np.zeros(80),\n",
    "                      \"MSE\" : np.zeros(80)})\n",
    "MSE_SVM = MSE_C[:0]\n",
    "\n",
    "for g in range(80):\n",
    "    for c in range(80):\n",
    "        k_ind = int(0)\n",
    "        for train_index, test_index in clf_svm.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            svc = svm.SVC(kernel='rbf', gamma = (g/20 + 0.05),\n",
    "                          C=c/20 + 0.05)\n",
    "            svc.fit(X_train, y_train)\n",
    "            y_pred = svc.predict(X_test)\n",
    "            error = y_test != y_pred\n",
    "            MSE[k_ind] = error.mean()\n",
    "            k_ind += 1\n",
    "        MSE_C['C'][c] = c/20 + 0.05\n",
    "        MSE_C['G'][c] = g/20 + 0.05\n",
    "        MSE_C['MSE'][c] =  MSE.mean()\n",
    "    MSE_SVM = pd.concat([MSE_SVM, MSE_C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>G</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>3.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>3.30</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2.95</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>3.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>3.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2.90</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         C     G       MSE\n",
       "2659  1.00  1.70  0.045455\n",
       "2578  0.95  1.65  0.045455\n",
       "2658  0.95  1.70  0.045455\n",
       "224   3.25  0.15  0.051136\n",
       "225   3.30  0.15  0.051136\n",
       "226   3.35  0.15  0.051136\n",
       "227   3.40  0.15  0.051136\n",
       "228   3.45  0.15  0.051136\n",
       "229   3.50  0.15  0.051136\n",
       "218   2.95  0.15  0.051136\n",
       "230   3.55  0.15  0.051136\n",
       "231   3.60  0.15  0.051136\n",
       "232   3.65  0.15  0.051136\n",
       "233   3.70  0.15  0.051136\n",
       "234   3.75  0.15  0.051136\n",
       "235   3.80  0.15  0.051136\n",
       "223   3.20  0.15  0.051136\n",
       "222   3.15  0.15  0.051136\n",
       "221   3.10  0.15  0.051136\n",
       "220   3.05  0.15  0.051136\n",
       "219   3.00  0.15  0.051136\n",
       "217   2.90  0.15  0.051136\n",
       "216   2.85  0.15  0.051136\n",
       "215   2.80  0.15  0.051136\n",
       "214   2.75  0.15  0.051136"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_SVM.index = range(len(MSE_SVM))\n",
    "MSE_SVM.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our C = 1, gamma = 1.7, the SVM model produces the lowest MSE at 0.045455. We can also note that when C = 0.95, gamma = 1.65 or 1.70, the the SVM model generates a similarily low MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "clf_mlp = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_mlp.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "activ = np.array(['identity', 'logistic', 'tanh', 'relu'])\n",
    "MSE_al = pd.DataFrame({'activation' : np.zeros(20),\n",
    "                       'hidden layer' : np.zeros(20),\n",
    "                       'alpha' : np.zeros(20),\n",
    "                       'MSE':np.zeros(20)})\n",
    "MSE_mlp = MSE_al[:0]\n",
    "\n",
    "for ac in range(4):\n",
    "    for h in range(8):\n",
    "        for al in range(20):\n",
    "            k_ind = int(0)\n",
    "            for train_index, test_index in clf_mlp.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                mlp = MLPClassifier(activation=activ[ac], solver='lbfgs',\n",
    "                                    alpha=(al/20 + 0.05), \n",
    "                                    hidden_layer_sizes = ((50 * (h + 1)),))\n",
    "                mlp.fit(X_train, y_train)\n",
    "                y_pred = mlp.predict(X_test)\n",
    "                error = y_test != y_pred\n",
    "                MSE[k_ind] = error.mean()\n",
    "                k_ind += 1\n",
    "            MSE_al['activation'][al] = activ[ac]\n",
    "            MSE_al['hidden layer'][al] = 50 * (h + 1)\n",
    "            MSE_al['alpha'][al] = al/20 + 0.05\n",
    "            MSE_al['MSE'][al] =  MSE.mean()\n",
    "        MSE_mlp = pd.concat([MSE_mlp, MSE_al])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>hidden layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.80</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.50</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.50</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.40</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.50</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.75</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.80</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.20</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.95</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.40</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.75</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.95</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.20</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.50</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.55</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.90</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.80</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.85</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE activation  alpha  hidden layer\n",
       "584  0.039773       relu   0.25         300.0\n",
       "615  0.039773       relu   0.80         350.0\n",
       "529  0.039773       relu   0.50         150.0\n",
       "624  0.039773       relu   0.25         400.0\n",
       "626  0.039773       relu   0.35         400.0\n",
       "569  0.039773       relu   0.50         250.0\n",
       "507  0.039773       relu   0.40         100.0\n",
       "591  0.039773       relu   0.60         300.0\n",
       "582  0.039773       relu   0.15         300.0\n",
       "509  0.045455       relu   0.50         100.0\n",
       "586  0.045455       relu   0.35         300.0\n",
       "614  0.045455       relu   0.75         350.0\n",
       "515  0.045455       relu   0.80         100.0\n",
       "583  0.045455       relu   0.20         300.0\n",
       "518  0.045455       relu   0.95         100.0\n",
       "526  0.045455       relu   0.35         150.0\n",
       "587  0.045455       relu   0.40         300.0\n",
       "534  0.045455       relu   0.75         150.0\n",
       "538  0.045455       relu   0.95         150.0\n",
       "603  0.045455       relu   0.20         350.0\n",
       "549  0.045455       relu   0.50         200.0\n",
       "590  0.045455       relu   0.55         300.0\n",
       "617  0.045455       relu   0.90         350.0\n",
       "555  0.045455       relu   0.80         200.0\n",
       "556  0.045455       relu   0.85         200.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_mlp.index = range(len(MSE_mlp))\n",
    "MSE_mlp.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that when activation = relu, alpha = 0.2 and hidden layer = 250, the MLP model can get the lowest MSE, which is 0.039773."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logit</th>\n",
       "      <th>MLP</th>\n",
       "      <th>RF</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Logit       MLP        RF       SVM\n",
       "0  0.068182  0.039773  0.068182  0.045455\n",
       "1  0.068182  0.039773  0.068182  0.045455\n",
       "2  0.068182  0.039773  0.068182  0.045455\n",
       "3  0.068182  0.039773  0.068182  0.051136\n",
       "4  0.068182  0.039773  0.068182  0.051136"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = np.array(MSE_multilog.sort_values(['MSE']).head(5)['MSE'])\n",
    "RF = np.array(MSE_df.sort_values(['MSE']).head(5)['MSE'])\n",
    "SVM = np.array(MSE_SVM.sort_values(['MSE']).head(5)['MSE'])\n",
    "mlp = np.array(MSE_mlp.sort_values(['MSE']).head(5)['MSE'])\n",
    "\n",
    "\n",
    "Com = pd.DataFrame({'Logit':logit, \n",
    "                    'RF':RF,\n",
    "                    'SVM':SVM,\n",
    "                    'MLP':mlp})\n",
    "Com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP neural network model has the lowest MSE values compared with other three models, which means the MLP neural network model is the best predictor of cultivar among all of these four models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
