{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set #7\n",
    "### by Alexander Tyan, March 5, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1). Classi\f",
    "er horse race (10 points). For this problem, you will use the 397 observations from the Auto.csv dataset.1 This dataset includes 397 observations on miles per gallon (mpg), number of cylinders (cylinders), engine displacement (displacement), horsepower (horsepower), vehicle weight (weight), acceleration (acceleration), vehicle year (year), vehicle origin (origin), and vehicle name (name). We will study the factors that make miles per gallon high or low. Create a binary variable mpg high that equals 1 if mpg high >= median(mpg high) and equals either 0 if mpg high< median(mpg high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 392 entries, 0 to 396\n",
      "Data columns (total 11 columns):\n",
      "mpg             392 non-null float64\n",
      "cylinders       392 non-null int64\n",
      "displacement    392 non-null float64\n",
      "horsepower      392 non-null float64\n",
      "weight          392 non-null int64\n",
      "acceleration    392 non-null float64\n",
      "year            392 non-null int64\n",
      "origin          392 non-null int64\n",
      "name            392 non-null object\n",
      "mpg_high        392 non-null int64\n",
      "constant        392 non-null int64\n",
      "dtypes: float64(4), int64(6), object(1)\n",
      "memory usage: 36.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Auto.csv\", na_values=[\"?\"])\n",
    "mpg_median = df[\"mpg\"].median()\n",
    "df[\"mpg_high\"] = (df[\"mpg\"] >= mpg_median).astype(int)\n",
    "df[\"constant\"] = 1\n",
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a). Use sklearn.linear model.LogisticRegression to \f",
    "t a logistic model of mpg high on features number of cylinders (cyl), engine displacement (dspl), horsepower (hpwr), vehicle weight (wgt), acceleration (accl), vehicle year (yr), vehicle origin (orgn). Make sure to include a constant term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_values = df[['constant', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin']].values\n",
    "X_values_no_const = df[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin']].values\n",
    "y_values = df['mpg_high'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "kf_log = KFold(n_splits=k, shuffle=True, random_state=15)\n",
    "kf_log.get_n_splits(X_values)\n",
    "\n",
    "MSE_vec_kf_log = np.zeros(k)\n",
    "error_vec_cat_0 = np.zeros(k)\n",
    "error_vec_cat_1 = np.zeros(k)\n",
    "\n",
    "k_ind = int(0)\n",
    "for train_index, test_index in kf_log.split(X_values):\n",
    "    X_train, X_test = X_values[train_index], X_values[test_index]\n",
    "    y_train, y_test = y_values[train_index], y_values[test_index]\n",
    "    LogReg = linear_model.LogisticRegression()\n",
    "    LogReg.fit(X_train, y_train)\n",
    "    y_pred = LogReg.predict(X_test)\n",
    "    MSE_vec_kf_log[k_ind] = ((y_test - y_pred) ** 2).mean()\n",
    "    success_stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "    cat_0_precision = success_stats[0][0]\n",
    "    cat_0_error_rate = 1 - cat_0_precision\n",
    "    cat_1_precision = success_stats[0][1]\n",
    "    cat_1_error_rate = 1 - cat_1_precision\n",
    "    error_vec_cat_0[k_ind] = cat_0_error_rate\n",
    "    error_vec_cat_1[k_ind] = cat_1_error_rate\n",
    "    k_ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the MSE of the model as the average MSE across the k = 4 test sets, and report the error rates for each category of mpg high as the average error rate for that category across the k = 4 test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE (Logistic Regression) = 0.107142857143\n",
      "Mean SE of MSE (Logistic Regression) = 0.015306122449\n",
      "Mean Error Rate for Category 0 (Logistic Regression) = 0.0947067070586\n",
      "Mean SE of Error for Category 0 (Logistic Regression) = 0.0440594622751\n",
      "Mean Error Rate for Category 1 (Logistic Regression) = 0.118226649127\n",
      "Mean SE of Error for Category 1 (Logistic Regression) = 0.0236516096988\n"
     ]
    }
   ],
   "source": [
    "MSE_kf = MSE_vec_kf_log.mean()\n",
    "MSE_kf_std = MSE_vec_kf_log.std()\n",
    "error_cat_0 = error_vec_cat_0.mean()\n",
    "error_cat_0_std = error_vec_cat_0.std()\n",
    "error_cat_1 = error_vec_cat_1.mean()\n",
    "error_cat_1_std = error_vec_cat_1.std()\n",
    "print('Mean MSE (Logistic Regression) =', MSE_kf)\n",
    "print('Mean SE of MSE (Logistic Regression) =', MSE_kf_std)\n",
    "print('Mean Error Rate for Category 0 (Logistic Regression) =', error_cat_0)\n",
    "print('Mean SE of Error for Category 0 (Logistic Regression) =', error_cat_0_std)\n",
    "print('Mean Error Rate for Category 1 (Logistic Regression) =', error_cat_1)\n",
    "print('Mean SE of Error for Category 1 (Logistic Regression) =', error_cat_1_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b). Use sklearn.ensemble.RandomForestClassifier to \f",
    "t a random forest model of mpg high on max features=2 out of the seven possible features used in part (a). Set n estimators=20, set bootstrap=True, set oob score=True, and set random state=25.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=2, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
       "            oob_score=True, random_state=25, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest_model = RandomForestClassifier(n_estimators=20, max_features=2, bootstrap=True,\n",
    "                                           oob_score=True, random_state=25)\n",
    "rand_forest_model.fit(X_values_no_const, y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the MSE of the random forest model as the MSE from the .oob prediction object, and report the error rates for each category of mpg high from the .oob prediction object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Random Forest Classifier) = 0.0663265306122\n",
      "Error Rate for Category 0 (Random Forest Classifier) = 0.0618556701031\n",
      "Error Rate for Category 1 (Random Forest Classifier) = 0.0707070707071\n"
     ]
    }
   ],
   "source": [
    "y_pred_forest = rand_forest_model.oob_decision_function_[:,1] - rand_forest_model.oob_decision_function_[:,0]\n",
    "y_pred_forest[y_pred_forest > 0] = np.int(1)\n",
    "y_pred_forest[y_pred_forest <= 0] = np.int(0)\n",
    "\n",
    "MSE_forest = mean_squared_error(y_values, y_pred_forest)\n",
    "success_stats = precision_recall_fscore_support(y_values, y_pred_forest)\n",
    "cat_0_precision = success_stats[0][0]\n",
    "cat_0_error_rate = 1 - cat_0_precision\n",
    "cat_1_precision = success_stats[0][1]\n",
    "cat_1_error_rate = 1 - cat_1_precision\n",
    "\n",
    "\n",
    "print('MSE (Random Forest Classifier) =', MSE_forest)\n",
    "print('Error Rate for Category 0 (Random Forest Classifier) =', cat_0_error_rate)\n",
    "print('Error Rate for Category 1 (Random Forest Classifier) =', cat_1_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c). Use sklearn.svm.SVC to \f",
    "t a support vector machines model of mpg high with a Gaussian radial basis function kernel kernel='rbf' on the seven features used in part (a). Set the penalty parameter to C=1 and set gamma=0.2. Fit the model using k-fold cross validation with k = 4 folds exactly as in part (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexander/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "kf_svc = KFold(n_splits=k, shuffle=True, random_state=15)\n",
    "kf_svc.get_n_splits(X_values_no_const)\n",
    "\n",
    "MSE_vec_kf_svc = np.zeros(k)\n",
    "error_vec_cat_0 = np.zeros(k)\n",
    "error_vec_cat_1 = np.zeros(k)\n",
    "\n",
    "k_ind = int(0)\n",
    "\n",
    "for train_index, test_index in kf_svc.split(X_values_no_const):\n",
    "    X_train, X_test = X_values_no_const[train_index], X_values_no_const[test_index]\n",
    "    y_train, y_test = y_values[train_index], y_values[test_index]\n",
    "    \n",
    "    svc_rbf = svm.SVC(kernel='rbf', gamma=0.2, C=1)\n",
    "    svc_rbf.fit(X_train, y_train)\n",
    "    y_pred_rbf = svc_rbf.predict(X_test)\n",
    "    \n",
    "    success_stats = precision_recall_fscore_support(y_test, y_pred_rbf)\n",
    "    cat_0_precision = success_stats[0][0]\n",
    "    cat_0_error_rate = 1 - cat_0_precision\n",
    "    cat_1_precision = success_stats[0][1]\n",
    "    cat_1_error_rate = 1 - cat_1_precision\n",
    "    \n",
    "    cat_0_num_cases = confusion_matrix(y_test, y_pred_rbf)[0,0] + confusion_matrix(y_test, y_pred_rbf)[1,0]\n",
    "    cat_1_num_cases = confusion_matrix(y_test, y_pred_rbf)[1,1] + confusion_matrix(y_test, y_pred_rbf)[0,1]\n",
    "   \n",
    "    error_vec_cat_0[k_ind] = cat_0_error_rate\n",
    "    \n",
    "    # Avoid no predicted samples case in in the second loop:\n",
    "    if k_ind != 1:\n",
    "        error_vec_cat_1[k_ind] = cat_1_error_rate\n",
    "    \n",
    "    MSE_rbf = mean_squared_error(y_test, y_pred_rbf)\n",
    "    MSE_vec_kf_svc[k_ind] = MSE_rbf\n",
    "    \n",
    "    k_ind += 1\n",
    "    \n",
    "# Avoid no predicted samples case in in the second loop:\n",
    "error_vec_cat_1 = np.delete(error_vec_cat_1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the MSE of the model as the average MSE across the k = 4 test sets, and report the error rates for each category of mpg high as the average error rate for that category across the k = 4 test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE (SVC Model) =  0.507653061224\n",
      "Mean SE of MSE (SVC Model) = 0.0348846794163\n",
      "Mean Error Rate for Category 0 (SVC Model) = 0.381194110709\n",
      "Mean SE of Error for Category 0 (SVC Model) = 0.221165280503\n",
      "Mean Error Rate for Category 1 (SVC Model) = 0.184027777778\n",
      "Mean SE of Error for Category 1 (SVC Model) = 0.260254579187\n"
     ]
    }
   ],
   "source": [
    "mean_SVC_MSE = MSE_vec_kf_svc.mean()\n",
    "SVC_MSE_std = MSE_vec_kf_svc.std()\n",
    "error_cat_0 = error_vec_cat_0.mean()\n",
    "error_cat_0_std = error_vec_cat_0.std()\n",
    "error_cat_1 = error_vec_cat_1.mean()\n",
    "error_cat_1_std = error_vec_cat_1.std()\n",
    "print(\"Mean MSE (SVC Model) = \", mean_SVC_MSE)\n",
    "print('Mean SE of MSE (SVC Model) =', SVC_MSE_std)\n",
    "print('Mean Error Rate for Category 0 (SVC Model) =', error_cat_0)\n",
    "print('Mean SE of Error for Category 0 (SVC Model) =', error_cat_0_std)\n",
    "print('Mean Error Rate for Category 1 (SVC Model) =', error_cat_1)\n",
    "print('Mean SE of Error for Category 1 (SVC Model) =', error_cat_1_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d). Which of the above three models do you think is the best predictor of mpg high? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by the MSE values, Random Forest Classifier is the best, with the MSE value of 0.0663265306122. This indicates low average errors between what the model expects in a test set and what value test set actually has and is thus an indicator of predictive power. Suprisingly, the SVC model is the worst, with the MSE of 0.507653061224 (i.e. high predictive error rates). I would have thought that the SVC would be better at binary classification than the RFC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
