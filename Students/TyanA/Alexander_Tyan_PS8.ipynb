{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set #9\n",
    "### by Alexander Tyan, March 12, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "# Import dataset as pandas df:\n",
    "df = pd.read_csv(\"data/strongdrink.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a). Create a scatterplot of the data where the x-variable is alcohol (alco) and the y-variable is color intensity (color int). Make the dot of each of the three possible cultivar types a di\u000b",
    "erent color. Make sure your plot has a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAFuCAYAAABwaTS6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nopgsiIhI1P8H0rBETVyBj0cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1fea3668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.lmplot(x=\"alco\", y=\"color_int\", data=df, fit_reg=False, hue='cultivar', legend=True)\n",
    "plt.title('Alcohol vs Color Intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b). Use sklearn.linear model.LogisticRegression to fit a multinomial logistic model of cultivar on features alcohol (alco), malic acid (malic), total phenols (tot phen), and color intensity (color int) with the following linear predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependent/independent variables:\n",
    "X = df[[\"alco\", \"malic\", \"tot_phen\", \"color_int\"]]\n",
    "y = df[[\"cultivar\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75% sample training set:\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, test_size = 0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients:\n",
      "[-23.17959572  23.46463201  -0.28503629]\n",
      "[[ 1.56220083 -0.24119145  1.38349285  0.13566983]\n",
      " [-1.40220828 -0.35419071  0.37961117 -1.09904461]\n",
      " [-0.15999254  0.59538219 -1.763104    0.96337485]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.86      0.92      0.89        13\n",
      "          2       0.94      0.89      0.91        18\n",
      "          3       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       0.93      0.93      0.93        44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexander/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Train to fit a multinomial logistic regression:\n",
    "MultLogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                solver='newton-cg',\n",
    "                                penalty=\"l2\",\n",
    "                               C=1)\n",
    "MultLogReg.fit(X_train, y_train)\n",
    "# Predict response based on test indep. vars.:\n",
    "y_pred = MultLogReg.predict(X_test)\n",
    "print(\"Estimated Coefficients:\")\n",
    "print(MultLogReg.intercept_)\n",
    "print(MultLogReg.coef_)\n",
    "classification = classification_report(y_test, y_pred)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use k-fold cross-validation to estimate the MSE of the multinomial logit model. Play with the tuning parameter values penalty and C to get the lowest possible k-fold MSE. Report your minimized overall MSE along with the tuning parameter values you used for penalty and C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0909090909091\n",
      "test estimate MSE standard err= 0.0321412173267\n",
      "C = 0.1\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0795454545455\n",
      "test estimate MSE standard err= 0.0254098633807\n",
      "C = 0.2\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0681818181818\n",
      "test estimate MSE standard err= 0.0278351107134\n",
      "C = 0.3\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0681818181818\n",
      "test estimate MSE standard err= 0.0278351107134\n",
      "C = 0.4\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0738636363636\n",
      "test estimate MSE standard err= 0.0247664712701\n",
      "C = 0.5\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0852272727273\n",
      "test estimate MSE standard err= 0.0188444590361\n",
      "C = 0.7\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0852272727273\n",
      "test estimate MSE standard err= 0.0188444590361\n",
      "C = 0.8\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0852272727273\n",
      "test estimate MSE standard err= 0.0188444590361\n",
      "C = 1\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0909090909091\n",
      "test estimate MSE standard err= 0.0160706086633\n",
      "C = 5\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0852272727273\n",
      "test estimate MSE standard err= 0.0247664712701\n",
      "C = 10\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0852272727273\n",
      "test estimate MSE standard err= 0.0247664712701\n",
      "C = 100\n",
      "penalty = l2\n",
      "=================================================\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "=================================================\n",
      "test estimate MSE k-fold= 0.0909090909091\n",
      "test estimate MSE standard err= 0.0278351107134\n",
      "C = 1000\n",
      "penalty = l2\n",
      "=================================================\n",
      "=================================================\n",
      "MSE OPTIMIZATION RESULTS (NOTE: using newton-cg solver => only L2 penalty is availible):\n",
      "NOTE: using newton-cg solver => only L2 penalty is availible\n",
      "test estimate MSE k-fold= 0.0681818181818\n",
      "test estimate MSE standard err= 0.0278351107134\n",
      "C = 0.3\n",
      "penalty = l2\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "# Indicator function:\n",
    "def indicator_2(y_test_array, y_pred_array):\n",
    "    \"\"\"\n",
    "    Indicator function, returns 1 if the predicted\n",
    "    value is the same as the test value, 0 otherwise.\n",
    "    y_test_val: integer\n",
    "    y_pred_val: integer\n",
    "    return: integer, 0 or 1\n",
    "    \"\"\"\n",
    "    array = (y_test_array == y_pred_array)\n",
    "    return array.astype(int)\n",
    "\n",
    "# Separate into dependent/indepndent vars Numpy arrays:\n",
    "Xvars = df[[\"alco\", \"malic\", \"tot_phen\", \"color_int\"]].values\n",
    "yvals = df[[\"cultivar\"]].values.ravel()\n",
    "\n",
    "# Cross validation:\n",
    "def mult_logit(xvars, yvals, c, penalty, print_out=True):\n",
    "    k = 4\n",
    "    clf_mlog = KFold(n_splits=k, shuffle=True, random_state=22)\n",
    "    clf_mlog.get_n_splits(X)\n",
    "\n",
    "    results_df_clf_mlog = pd.DataFrame(columns=[\"y_test\", \"y_pred\", \"MSE\"])\n",
    "\n",
    "    MSE_vec_clf_mlog = np.zeros(k)\n",
    "\n",
    "    k_ind = int(0)\n",
    "    for train_index, test_index in clf_mlog.split(Xvars):\n",
    "        X_train, X_test = Xvars[train_index], Xvars[test_index]\n",
    "        y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "        MultLogReg = LogisticRegression(multi_class='multinomial', \n",
    "                                        solver='newton-cg',\n",
    "                                        penalty=penalty, C=c)\n",
    "        MultLogReg.fit(X_train, y_train)\n",
    "        y_pred = MultLogReg.predict(X_test)\n",
    "        df1 = pd.DataFrame(y_test, columns=[\"y_test\"])\n",
    "        df2 = pd.DataFrame(y_pred, columns=[\"y_pred\"])\n",
    "        df_concat = pd.concat([df1, df2], axis=1)\n",
    "        one_MSE = (1 - indicator_2(df_concat[\"y_test\"], df_concat[\"y_pred\"])) ** 2\n",
    "        df_concat[\"MSE\"] = one_MSE\n",
    "        results_df_clf_mlog = results_df_clf_mlog.append(df_concat)\n",
    "        MSE_vec_clf_mlog[k_ind] = one_MSE.mean()\n",
    "        k_ind += 1\n",
    "\n",
    "    # Enforce integer dtypes:\n",
    "    results_df_clf_mlog[[\"y_test\", \"y_pred\", \"MSE\"]] = results_df_clf_mlog[[\"y_test\", \"y_pred\", \"MSE\"]].apply(pd.to_numeric)\n",
    "    \n",
    "    # Report MSE Results:\n",
    "    MSE_clf_mlog = MSE_vec_clf_mlog.mean()\n",
    "    MSE_clf_mlog_std = MSE_vec_clf_mlog.std()\n",
    "    if print_out:\n",
    "        print(\"NOTE: using newton-cg solver => only L2 penalty is availible\")\n",
    "        print(\"=================================================\")\n",
    "        print('test estimate MSE k-fold=', MSE_clf_mlog)\n",
    "        print('test estimate MSE standard err=', MSE_clf_mlog_std)\n",
    "        print('C =', c)\n",
    "        print('penalty =', penalty)\n",
    "        print(\"=================================================\")\n",
    "    return c, penalty, MSE_clf_mlog, MSE_clf_mlog_std\n",
    "\n",
    "# Manual \"play around\":\n",
    "mult_logit(Xvars, yvals, .1, \"l2\")\n",
    "mult_logit(Xvars, yvals, .2, \"l2\")\n",
    "mult_logit(Xvars, yvals, .3, \"l2\")\n",
    "mult_logit(Xvars, yvals, .4, \"l2\")\n",
    "mult_logit(Xvars, yvals, .5, \"l2\")\n",
    "mult_logit(Xvars, yvals, .7, \"l2\")\n",
    "mult_logit(Xvars, yvals, .8, \"l2\")\n",
    "mult_logit(Xvars, yvals, 1, \"l2\")\n",
    "mult_logit(Xvars, yvals, 5, \"l2\")\n",
    "mult_logit(Xvars, yvals, 10, \"l2\")\n",
    "mult_logit(Xvars, yvals, 100, \"l2\")\n",
    "mult_logit(Xvars, yvals, 1000, \"l2\")\n",
    "\n",
    "\n",
    "# More automated \"play around\":\n",
    "def mult_logit_optimize(c_range_start, c_range_end):\n",
    "    best_MSE = 1000\n",
    "    bset_MSE_std = None\n",
    "    best_c = None\n",
    "    best_penalty = None\n",
    "    for c in np.arange(c_range_start, c_range_end, 0.1):\n",
    "        c, penalty, MSE_clf_mlog, MSE_clf_mlog_std = mult_logit(Xvars, yvals, c, \"l2\", False)\n",
    "        if MSE_clf_mlog < best_MSE:\n",
    "            best_MSE = MSE_clf_mlog\n",
    "            bset_MSE_std = MSE_clf_mlog_std\n",
    "            best_c = c\n",
    "            best_penalty = penalty\n",
    "    return best_MSE, bset_MSE_std, best_c, best_penalty\n",
    "        \n",
    "best_MSE, bset_MSE_std, best_c, best_penalty = mult_logit_optimize(0.1, 20)\n",
    "print(\"=================================================\")\n",
    "print(\"MSE OPTIMIZATION RESULTS (NOTE: using newton-cg solver => only L2 penalty is availible):\")\n",
    "print(\"NOTE: using newton-cg solver => only L2 penalty is availible\")\n",
    "print('test estimate MSE k-fold=', best_MSE)\n",
    "print('test estimate MSE standard err=', bset_MSE_std)\n",
    "print('C =', best_c)\n",
    "print('penalty =', best_penalty)\n",
    "print(\"=================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like with C=.3 (and C=.4) and penalty L2, we get the lowest MSE = 0.0681818181818."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c). Use sklearn.ensemble.RandomForestClassifier to \f",
    "t a random forest model of cultivar on the same four features used in part (b). set bootstrap=True, set oob score=True, and set random state=22. Use OOB cross-validation to generate the MSE of your random forest classi\f",
    "er. Play with the values of the tuning parameters n estimators, max depth, and min samples leaf to try and \f",
    "nd the lowest possible MSE from the OOB cross validation. Report your minimized overall MSE along with the tuning parameter values you used for n estimators, max depth, and min samples leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of estimators = 53\n",
      "Max depth = 5\n",
      "Min samples in a leaf = 5\n",
      "MSE = 0.126754651655\n",
      "=============================================\n",
      "Num of estimators = 100\n",
      "Max depth = 5\n",
      "Min samples in a leaf = 5\n",
      "MSE = 0.125630868435\n",
      "=============================================\n",
      "Num of estimators = 1000\n",
      "Max depth = 5\n",
      "Min samples in a leaf = 5\n",
      "MSE = 0.124780654982\n",
      "=============================================\n",
      "Num of estimators = 53\n",
      "Max depth = 5\n",
      "Min samples in a leaf = 5\n",
      "MSE = 0.126754651655\n",
      "=============================================\n",
      "Num of estimators = 53\n",
      "Max depth = 10\n",
      "Min samples in a leaf = 5\n",
      "MSE = 0.12683020618\n",
      "=============================================\n",
      "Num of estimators = 53\n",
      "Max depth = 50\n",
      "Min samples in a leaf = 5\n",
      "MSE = 0.12683020618\n",
      "=============================================\n",
      "Num of estimators = 53\n",
      "Max depth = 10\n",
      "Min samples in a leaf = 3\n",
      "MSE = 0.123611956237\n",
      "=============================================\n",
      "Num of estimators = 53\n",
      "Max depth = 10\n",
      "Min samples in a leaf = 10\n",
      "MSE = 0.132806645468\n",
      "=============================================\n",
      "Num of estimators = 53\n",
      "Max depth = 10\n",
      "Min samples in a leaf = 20\n",
      "MSE = 0.178892315018\n",
      "=============================================\n",
      "Num of estimators = 1000\n",
      "Max depth = 10\n",
      "Min samples in a leaf = 3\n",
      "MSE = 0.116086640331\n",
      "=============================================\n",
      "Num of estimators = 1000\n",
      "Max depth = 5\n",
      "Min samples in a leaf = 3\n",
      "MSE = 0.115935237276\n",
      "=============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 5, 3, 0.11593523727612604)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rand_forest(n_estimators, max_depth, min_samples_leaf, print_out=True):\n",
    "    rand_forest = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                        max_depth=max_depth,\n",
    "                                        min_samples_leaf=min_samples_leaf,\n",
    "                                        max_features='sqrt', bootstrap=True,\n",
    "                                        oob_score=True, random_state=22)\n",
    "    rand_forest.fit(Xvars, yvals)\n",
    "\n",
    "    rand_forest.score(Xvars, yvals)\n",
    "    rand_forest_pred = rand_forest.oob_prediction_\n",
    "    MSE_rand_forest = mean_squared_error(yvals, rand_forest_pred)\n",
    "    if print_out:\n",
    "        print(\"Num of estimators =\", n_estimators)\n",
    "        print(\"Max depth =\", max_depth)\n",
    "        print(\"Min samples in a leaf =\", min_samples_leaf)\n",
    "        print('MSE =', MSE_rand_forest)\n",
    "        print(\"=============================================\")\n",
    "    return n_estimators, max_depth, min_samples_leaf, MSE_rand_forest\n",
    "\n",
    "# Manual \"play around\":\n",
    "rand_forest(53, 5, 5)\n",
    "rand_forest(100, 5, 5)\n",
    "rand_forest(1000, 5, 5)\n",
    "rand_forest(53, 5, 5)\n",
    "rand_forest(53, 10, 5)\n",
    "rand_forest(53, 50, 5)\n",
    "rand_forest(53, 10, 3)\n",
    "rand_forest(53, 10, 10)\n",
    "rand_forest(53, 10, 20)\n",
    "rand_forest(1000, 10, 3)\n",
    "rand_forest(1000, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More automated \"play around\"(did not perform because of computational intensity):\n",
    "# def rand_foreset_mse_optimize(n_estimators_min, n_estimators_max, \n",
    "#                               max_depth_min, max_depth_max, \n",
    "#                               min_samples_leaf_min, min_samples_leaf_max):\n",
    "#     best_mse = 1000\n",
    "#     best_num_estimators = None\n",
    "#     best_max_depth = None\n",
    "#     best_min_samples = None\n",
    "#     for num_estimators in range(n_estimators_min, n_estimators_max + 1):\n",
    "#         for max_depth in range(max_depth_min, max_depth_max + 1):\n",
    "#             for min_sample_num in range(min_samples_leaf_min, min_samples_leaf_max + 1):\n",
    "#                 n_estimators, max_depth, min_samples_leaf, MSE_rand_forest = rand_forest(n_estimators=num_estimators, \n",
    "#                                                                                          max_depth=max_depth, \n",
    "#                                                                                          min_samples_leaf=min_sample_num, \n",
    "#                                                                                          print_out=False)\n",
    "#                 if MSE_rand_forest < best_mse:\n",
    "#                     best_mse = MSE_rand_forest\n",
    "#                     best_num_estimators = n_estimators\n",
    "#                     best_max_depth = max_depth\n",
    "#                     best_min_samples = min_samples_leaf\n",
    "#     return best_mse, best_num_estimators, best_max_depth, best_min_samples\n",
    "        \n",
    "# best_mse, best_num_estimators, best_max_depth, best_min_samples = rand_foreset_mse_optimize(n_estimators_min=10, n_estimators_max=1000, \n",
    "#                               max_depth_min=2, max_depth_max=10, \n",
    "#                               min_samples_leaf_min=2, min_samples_leaf_max=10)\n",
    "\n",
    "# print(\"=============================================\")\n",
    "# print(\"Automated Results:\")\n",
    "# print(\"Num of estimators =\", best_num_estimators)\n",
    "# print(\"Max depth =\", best_max_depth)\n",
    "# print(\"Min samples in a leaf =\", best_min_samples)\n",
    "# print('MSE =', best_mse)\n",
    "# print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest MSE of 0.115935237276 is achieved with 1000 estimators (trees), max depth of 5 and minimum samples in a leaf of 3 (This is without more systematic parameter testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d). Use sklearn.svm.SVC to fit a support vector machines model of cultivar with a Gaussian radial basis function kernel kernel='rbf' on the four features used in parts (b) and (c). Fit the model using k-fold cross validation with k = 4 folds exactly as in part (b). Play with the penalty parameter C and the coefficient on the radial basis function gamma to try and find the lowest possible MSE from the k-fold cross validation. Report your minimized overall MSE along with the tuning parameter values you used for C and gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.2\n",
      "C = 1\n",
      "Mean MSE (SVC Model) =  0.153409090909\n",
      "Mean SE of MSE (SVC Model) = 0.0969245574388\n",
      "================================================\n",
      "Gamma = 0.1\n",
      "C = 1\n",
      "Mean MSE (SVC Model) =  0.147727272727\n",
      "Mean SE of MSE (SVC Model) = 0.0997155044022\n",
      "================================================\n",
      "Gamma = 0.3\n",
      "C = 1\n",
      "Mean MSE (SVC Model) =  0.153409090909\n",
      "Mean SE of MSE (SVC Model) = 0.10084226903\n",
      "================================================\n",
      "Gamma = 0.01\n",
      "C = 1\n",
      "Mean MSE (SVC Model) =  0.170454545455\n",
      "Mean SE of MSE (SVC Model) = 0.0887528372262\n",
      "================================================\n",
      "Gamma = 0.15\n",
      "C = 1\n",
      "Mean MSE (SVC Model) =  0.153409090909\n",
      "Mean SE of MSE (SVC Model) = 0.0969245574388\n",
      "================================================\n",
      "Gamma = 0.1\n",
      "C = 10\n",
      "Mean MSE (SVC Model) =  0.107954545455\n",
      "Mean SE of MSE (SVC Model) = 0.0336140896767\n",
      "================================================\n",
      "Gamma = 0.1\n",
      "C = 100\n",
      "Mean MSE (SVC Model) =  0.147727272727\n",
      "Mean SE of MSE (SVC Model) = 0.0340909090909\n",
      "================================================\n",
      "Gamma = 0.1\n",
      "C = 50\n",
      "Mean MSE (SVC Model) =  0.136363636364\n",
      "Mean SE of MSE (SVC Model) = 0.0278351107134\n",
      "================================================\n",
      "Gamma = 0.1\n",
      "C = 20\n",
      "Mean MSE (SVC Model) =  0.125\n",
      "Mean SE of MSE (SVC Model) = 0.0254098633807\n",
      "================================================\n",
      "Gamma = 0.1\n",
      "C = 15\n",
      "Mean MSE (SVC Model) =  0.125\n",
      "Mean SE of MSE (SVC Model) = 0.0254098633807\n",
      "================================================\n",
      "Gamma = 0.1\n",
      "C = 12\n",
      "Mean MSE (SVC Model) =  0.125\n",
      "Mean SE of MSE (SVC Model) = 0.0254098633807\n",
      "================================================\n",
      "Gamma = 0.1\n",
      "C = 9\n",
      "Mean MSE (SVC Model) =  0.107954545455\n",
      "Mean SE of MSE (SVC Model) = 0.0336140896767\n",
      "================================================\n",
      "Gamma = 0.1\n",
      "C = 8\n",
      "Mean MSE (SVC Model) =  0.113636363636\n",
      "Mean SE of MSE (SVC Model) = 0.0278351107134\n",
      "================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.11363636363636365, 0.027835110713445205, 0.1, 8)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def svc(gamma, c, print_out=True):\n",
    "    k = 4\n",
    "    clf_svm = KFold(n_splits=k, shuffle=True, random_state=22)\n",
    "    clf_svm.get_n_splits(Xvars)\n",
    "\n",
    "    MSE_vec_clf_svm = np.zeros(k)\n",
    "    error_vec_cat_0 = np.zeros(k)\n",
    "    error_vec_cat_1 = np.zeros(k)\n",
    "\n",
    "    k_ind = int(0)\n",
    "\n",
    "    for train_index, test_index in clf_svm.split(Xvars):\n",
    "        X_train, X_test = Xvars[train_index], Xvars[test_index]\n",
    "        y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "\n",
    "        svc_rbf = svm.SVC(kernel='rbf', gamma=gamma, C=c)\n",
    "        svc_rbf.fit(X_train, y_train)\n",
    "        y_pred_rbf = svc_rbf.predict(X_test)\n",
    "        MSE_rbf = mean_squared_error(y_test, y_pred_rbf)\n",
    "        MSE_vec_clf_svm[k_ind] = MSE_rbf\n",
    "\n",
    "        k_ind += 1\n",
    "\n",
    "    mean_SVC_MSE = MSE_vec_clf_svm.mean()\n",
    "    SVC_MSE_std = MSE_vec_clf_svm.std()\n",
    "    if print_out:\n",
    "        print(\"Gamma =\", gamma)\n",
    "        print(\"C =\", c)\n",
    "        print(\"Mean MSE (SVC Model) = \", mean_SVC_MSE)\n",
    "        print('Mean SE of MSE (SVC Model) =', SVC_MSE_std)\n",
    "        print(\"================================================\")\n",
    "    return mean_SVC_MSE, SVC_MSE_std, gamma, c\n",
    "\n",
    "# Manual \"play around\":\n",
    "svc(.2, 1)\n",
    "svc(.1, 1)\n",
    "svc(.3, 1)\n",
    "svc(.01, 1)\n",
    "svc(.15, 1)\n",
    "svc(.1, 10)\n",
    "svc(.1, 100)\n",
    "svc(.1, 50)\n",
    "svc(.1, 20)\n",
    "svc(.1, 15)\n",
    "svc(.1, 12)\n",
    "svc(.1, 9)\n",
    "svc(.1, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest MSE of these trials is 0.107954545455 with Gamma = 0.1 and C = 10. Similar results can be achieve with Gamma = 0.1 and C = 9. (I did not do more automated MSE optimization due to computational intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e). Use sklearn.neural network.MLPClassifier to fit a single hidden layer neural network model of cultivar. Fit the model using k-fold cross validation with k = 4 folds exactly as in parts (b) and (d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation function: tanh\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 98\n",
      "Mean MSE (MLP Model) = 0.0738636363636\n",
      "Mean SE of MSE (MLP Model)= 0.0336140896767\n",
      "================================================\n",
      "Activation function: tanh\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 100\n",
      "Mean MSE (MLP Model) = 0.0568181818182\n",
      "Mean SE of MSE (MLP Model)= 0.0376889180722\n",
      "================================================\n",
      "Activation function: tanh\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 150\n",
      "Mean MSE (MLP Model) = 0.0454545454545\n",
      "Mean SE of MSE (MLP Model)= 0.0321412173267\n",
      "================================================\n",
      "Activation function: tanh\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 105\n",
      "Mean MSE (MLP Model) = 0.0625\n",
      "Mean SE of MSE (MLP Model)= 0.0188444590361\n",
      "================================================\n",
      "Activation function: tanh\n",
      "Regularization penalty alpha = 1\n",
      "Size of hidden layer: 100\n",
      "Mean MSE (MLP Model) = 0.0625\n",
      "Mean SE of MSE (MLP Model)= 0.0295235933108\n",
      "================================================\n",
      "Activation function: tanh\n",
      "Regularization penalty alpha = 0.2\n",
      "Size of hidden layer: 100\n",
      "Mean MSE (MLP Model) = 0.0568181818182\n",
      "Mean SE of MSE (MLP Model)= 0.0340909090909\n",
      "================================================\n",
      "Activation function: logistic\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 100\n",
      "Mean MSE (MLP Model) = 0.0568181818182\n",
      "Mean SE of MSE (MLP Model)= 0.0340909090909\n",
      "================================================\n",
      "Activation function: logistic\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 200\n",
      "Mean MSE (MLP Model) = 0.0625\n",
      "Mean SE of MSE (MLP Model)= 0.0336140896767\n",
      "================================================\n",
      "Activation function: logistic\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 300\n",
      "Mean MSE (MLP Model) = 0.0681818181818\n",
      "Mean SE of MSE (MLP Model)= 0.0278351107134\n",
      "================================================\n",
      "Activation function: logistic\n",
      "Regularization penalty alpha = 0.01\n",
      "Size of hidden layer: 100\n",
      "Mean MSE (MLP Model) = 0.0625\n",
      "Mean SE of MSE (MLP Model)= 0.0295235933108\n",
      "================================================\n",
      "Activation function: relu\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 100\n",
      "Mean MSE (MLP Model) = 0.0454545454545\n",
      "Mean SE of MSE (MLP Model)= 0.0278351107134\n",
      "================================================\n",
      "Activation function: relu\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 50\n",
      "Mean MSE (MLP Model) = 0.0681818181818\n",
      "Mean SE of MSE (MLP Model)= 0.0160706086633\n",
      "================================================\n",
      "Activation function: relu\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 100\n",
      "Mean MSE (MLP Model) = 0.0681818181818\n",
      "Mean SE of MSE (MLP Model)= 0.035934973411\n",
      "================================================\n",
      "Activation function: relu\n",
      "Regularization penalty alpha = 0.1\n",
      "Size of hidden layer: 150\n",
      "Mean MSE (MLP Model) = 0.0454545454545\n",
      "Mean SE of MSE (MLP Model)= 0.0278351107134\n",
      "================================================\n",
      "Activation function: relu\n",
      "Regularization penalty alpha = 0.2\n",
      "Size of hidden layer: 150\n",
      "Mean MSE (MLP Model) = 0.0625\n",
      "Mean SE of MSE (MLP Model)= 0.0436428735674\n",
      "================================================\n",
      "Activation function: relu\n",
      "Regularization penalty alpha = 0.09\n",
      "Size of hidden layer: 150\n",
      "Mean MSE (MLP Model) = 0.0511363636364\n",
      "Mean SE of MSE (MLP Model)= 0.0247664712701\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def mlp(activation, alpha, hidden_layer_sizes):\n",
    "    k = 4\n",
    "    clf_mlp = KFold(n_splits=k, shuffle=True, random_state=22)\n",
    "    clf_mlp.get_n_splits(Xvars)\n",
    "\n",
    "    MSE_vec_clf_mlp = np.zeros(k)\n",
    "    error_vec_cat_0 = np.zeros(k)\n",
    "    error_vec_cat_1 = np.zeros(k)\n",
    "\n",
    "    k_ind = int(0)\n",
    "\n",
    "    for train_index, test_index in clf_mlp.split(Xvars):\n",
    "        X_train, X_test = Xvars[train_index], Xvars[test_index]\n",
    "        y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "\n",
    "        mlp = MLPClassifier(activation=activation, solver='lbfgs', alpha=alpha, \n",
    "                            hidden_layer_sizes=hidden_layer_sizes)\n",
    "        mlp.fit(X_train, y_train)\n",
    "        y_pred_mlp = mlp.predict(X_test)\n",
    "        \n",
    "        MSE_mlp = 1 - mlp.score(X_test, y_test) #mean_squared_error(y_test, y_pred_mlp)\n",
    "        MSE_vec_clf_mlp[k_ind] = MSE_mlp\n",
    "\n",
    "        k_ind += 1\n",
    "    mean_mlp_MSE = MSE_vec_clf_mlp.mean()\n",
    "    mlp_MSE_std = MSE_vec_clf_mlp.std()\n",
    "    print(\"Activation function:\", activation)\n",
    "    print(\"Regularization penalty alpha =\", alpha)\n",
    "    print(\"Size of hidden layer:\", hidden_layer_sizes)\n",
    "    print(\"Mean MSE (MLP Model) =\", mean_mlp_MSE)\n",
    "    print('Mean SE of MSE (MLP Model)=', mlp_MSE_std)\n",
    "    print(\"================================================\")\n",
    "\n",
    "mlp(\"tanh\", 0.1, 98)\n",
    "mlp(\"tanh\", 0.1, 100)\n",
    "mlp(\"tanh\", 0.1, 150)\n",
    "mlp(\"tanh\", 0.1, 105)\n",
    "mlp(\"tanh\", 1, 100)\n",
    "mlp(\"tanh\", 0.2, 100)\n",
    "mlp(\"logistic\", 0.1, 100)\n",
    "mlp(\"logistic\", 0.1, 200)\n",
    "mlp(\"logistic\", 0.1, 300)\n",
    "mlp(\"logistic\", 0.01, 100)\n",
    "mlp(\"relu\", 0.1, 100)\n",
    "mlp(\"relu\", 0.1, 50)\n",
    "mlp(\"relu\", 0.1, 100)\n",
    "mlp(\"relu\", 0.1, 150)\n",
    "mlp(\"relu\", 0.2, 150)\n",
    "mlp(\"relu\", 0.09, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the tested parameters, the lowest MSE is 0.0454545454545, using the following characteristics:\n",
    "Activation function: relu\n",
    "Regularization penalty alpha = 0.1\n",
    "Size of hidden layer: 100\n",
    "Mean SE of MSE (MLP Model)= 0.0278351107134\n",
    "\n",
    "Similar results can be achieved with size of hidden layer = 150. Note that these results are not always replicable, because, despite the random_state setting, the MLP seems to have some in-built randomness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d). Which of the above three models do you think is the best predictor of cultivar? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the MLP classifier (a single hidden layer neural network) is the best predictor of cultivar, because of the lowest MSE score out of all models. However, note two things. One, as mentioned before, MLP classifier seems to have some in-built randomness, making it difficult to replicate results from trial to trial, despite the random_state setting in the KFold validator. Two, not all models were rigourously optmized using systematic testing of input characteristics, such as penalties because of computational intensity. So many models were tested using trial and error approach. A more systematic testing of model parameters would be needed for a more rigourous determination of the optimal predictor model for cultivar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
