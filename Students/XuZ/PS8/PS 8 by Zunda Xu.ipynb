{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set # 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACS 30100,  Dr. Evans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name : Zunda Xu ( zunda@uchicago.edu )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the packages and the dataset used in the problem set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cultivar</th>\n",
       "      <th>alco</th>\n",
       "      <th>malic</th>\n",
       "      <th>ash</th>\n",
       "      <th>alk</th>\n",
       "      <th>magn</th>\n",
       "      <th>tot_phen</th>\n",
       "      <th>flav</th>\n",
       "      <th>nonfl_phen</th>\n",
       "      <th>proanth</th>\n",
       "      <th>color_int</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280rat</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cultivar   alco  malic   ash   alk  magn  tot_phen  flav  nonfl_phen  \\\n",
       "0         1  14.23   1.71  2.43  15.6   127      2.80  3.06        0.28   \n",
       "1         1  13.20   1.78  2.14  11.2   100      2.65  2.76        0.26   \n",
       "2         1  13.16   2.36  2.67  18.6   101      2.80  3.24        0.30   \n",
       "3         1  14.37   1.95  2.50  16.8   113      3.85  3.49        0.24   \n",
       "4         1  13.24   2.59  2.87  21.0   118      2.80  2.69        0.39   \n",
       "\n",
       "   proanth  color_int   hue  OD280rat  proline  \n",
       "0     2.29       5.64  1.04      3.92     1065  \n",
       "1     1.28       4.38  1.05      3.40     1050  \n",
       "2     2.81       5.68  1.03      3.17     1185  \n",
       "3     2.18       7.80  0.86      3.45     1480  \n",
       "4     1.82       4.32  1.04      2.93      735  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset \n",
    "strongdrink = pd.read_csv('strongdrink.txt')\n",
    "strongdrink.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part ( a ) : Plot a scatterplot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAH1CAYAAACtPWl3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cultivars = strongdrink.groupby('cultivar')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for kind, cultivar in cultivars:\n",
    "    ax.plot(cultivar['alco'], cultivar['color_int'], marker = 'o',linestyle='', ms=4, label=kind)\n",
    "ax.legend()\n",
    "plt.xlabel('Alcohol', fontsize = 12)\n",
    "plt.ylabel('Color Intensity', fontsize = 12)\n",
    "plt.title('Scatterplot of Alcohol and Color Intensity',fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part ( b ) : Fit a multinomial logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = strongdrink[['alco', 'malic', 'tot_phen', 'color_int']].values\n",
    "y = strongdrink['cultivar'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "clf_mlog = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_mlog.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "C = np.zeros(200)\n",
    "MSE_C = np.zeros(200)\n",
    "\n",
    "for c in range(200):\n",
    "    k_ind = int(0)\n",
    "    for train_index, test_index in clf_mlog.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        LogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                    solver='newton-cg', C = (c/100 + 0.01))\n",
    "        LogReg.fit(X_train, y_train)\n",
    "        y_pred = LogReg.predict(X_test)\n",
    "        error = y_test != y_pred\n",
    "        MSE[k_ind] = error.mean()\n",
    "        k_ind += 1\n",
    "    C[c] = c/100 + 0.01\n",
    "    MSE_C[c] =  MSE.mean()\n",
    "MSE_multilog = pd.DataFrame({'C':C, 'MSE':MSE_C})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C       MSE\n",
       "43  0.44  0.068182\n",
       "44  0.45  0.068182\n",
       "42  0.43  0.068182\n",
       "41  0.42  0.068182\n",
       "40  0.41  0.068182\n",
       "39  0.40  0.068182\n",
       "27  0.28  0.068182\n",
       "28  0.29  0.068182\n",
       "29  0.30  0.068182\n",
       "30  0.31  0.068182\n",
       "31  0.32  0.068182\n",
       "32  0.33  0.068182\n",
       "33  0.34  0.068182\n",
       "34  0.35  0.068182\n",
       "35  0.36  0.068182\n",
       "36  0.37  0.068182\n",
       "37  0.38  0.068182\n",
       "46  0.47  0.068182\n",
       "45  0.46  0.068182\n",
       "38  0.39  0.068182\n",
       "47  0.48  0.068182\n",
       "56  0.57  0.073864\n",
       "54  0.55  0.073864\n",
       "26  0.27  0.073864\n",
       "25  0.26  0.073864"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_multilog.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From table shown above, we note that when **C takes value between 0,28 and 0.44 (penalty is 'l2')**, the multinomial logistic model get the **lowest MSE**, which is **0.068182**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part ( c ) : Fit a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE_rf = pd.DataFrame({\"n_estimator\" : np.zeros(),\n",
    "                         \"min_sample\" : np.zeros(5),\n",
    "                         \"max_depth\" : np.zeros(5),\n",
    "                         \"MSE\" : np.zeros(5)})\n",
    "MSE_df = MSE_rf[:0]\n",
    "\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        for tree in range(5):\n",
    "            rf = RandomForestClassifier(n_estimators = (tree * 50 + 50),\n",
    "                                        min_samples_leaf = (i * 5 + 5),\n",
    "                                        max_depth = (j + 1), bootstrap=True, \n",
    "                                        oob_score=True, random_state=22)\n",
    "            rf.fit(X, y)\n",
    "            MSE_rf[\"n_estimator\"][tree] = tree * 50 + 50\n",
    "            MSE_rf[\"max_depth\"][tree] = j + 1\n",
    "            MSE_rf[\"min_sample\"][tree] = i * 5 + 5\n",
    "            MSE_rf[\"MSE\"][tree] = 1 - rf.oob_score_\n",
    "        MSE_df= pd.concat([MSE_df, MSE_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_sample</th>\n",
       "      <th>n_estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.073864</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE  max_depth  min_sample  n_estimator\n",
       "96   0.068182        5.0        10.0        100.0\n",
       "126  0.068182       11.0        10.0        100.0\n",
       "116  0.068182        9.0        10.0        100.0\n",
       "131  0.068182       12.0        10.0        100.0\n",
       "111  0.068182        8.0        10.0        100.0\n",
       "106  0.068182        7.0        10.0        100.0\n",
       "121  0.068182       10.0        10.0        100.0\n",
       "101  0.068182        6.0        10.0        100.0\n",
       "136  0.068182       13.0        10.0        100.0\n",
       "86   0.068182        3.0        10.0        100.0\n",
       "88   0.068182        3.0        10.0        200.0\n",
       "91   0.068182        4.0        10.0        100.0\n",
       "146  0.068182       15.0        10.0        100.0\n",
       "141  0.068182       14.0        10.0        100.0\n",
       "138  0.073864       13.0        10.0        200.0\n",
       "142  0.073864       14.0        10.0        150.0\n",
       "133  0.073864       12.0        10.0        200.0\n",
       "132  0.073864       12.0        10.0        150.0\n",
       "143  0.073864       14.0        10.0        200.0\n",
       "128  0.073864       11.0        10.0        200.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_df.index = range(len(MSE_df))\n",
    "MSE_df.sort_values(['MSE']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the table shown above, we note that when **max_depth = 5, min_sample_leaf = 10 and tree = 100**, the random forest model get the **lowest MSE, which is also 0.068182**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part ( d ) : Fit a support vector machines model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "clf_svm = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_svm.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "MSE_C = pd.DataFrame({\"C\" : np.zeros(80),\n",
    "                      \"G\" : np.zeros(80),\n",
    "                      \"MSE\" : np.zeros(80)})\n",
    "MSE_SVM = MSE_C[:0]\n",
    "\n",
    "for g in range(80):\n",
    "    for c in range(80):\n",
    "        k_ind = int(0)\n",
    "        for train_index, test_index in clf_svm.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            svc = svm.SVC(kernel='rbf', gamma = (g/20 + 0.05),\n",
    "                          C=c/20 + 0.05)\n",
    "            svc.fit(X_train, y_train)\n",
    "            y_pred = svc.predict(X_test)\n",
    "            error = y_test != y_pred\n",
    "            MSE[k_ind] = error.mean()\n",
    "            k_ind += 1\n",
    "        MSE_C['C'][c] = c/20 + 0.05\n",
    "        MSE_C['G'][c] = g/20 + 0.05\n",
    "        MSE_C['MSE'][c] =  MSE.mean()\n",
    "    MSE_SVM = pd.concat([MSE_SVM, MSE_C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>G</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>3.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>3.30</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2.95</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>3.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>3.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2.90</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         C     G       MSE\n",
       "2659  1.00  1.70  0.045455\n",
       "2578  0.95  1.65  0.045455\n",
       "2658  0.95  1.70  0.045455\n",
       "224   3.25  0.15  0.051136\n",
       "225   3.30  0.15  0.051136\n",
       "226   3.35  0.15  0.051136\n",
       "227   3.40  0.15  0.051136\n",
       "228   3.45  0.15  0.051136\n",
       "229   3.50  0.15  0.051136\n",
       "218   2.95  0.15  0.051136\n",
       "230   3.55  0.15  0.051136\n",
       "231   3.60  0.15  0.051136\n",
       "232   3.65  0.15  0.051136\n",
       "233   3.70  0.15  0.051136\n",
       "234   3.75  0.15  0.051136\n",
       "235   3.80  0.15  0.051136\n",
       "223   3.20  0.15  0.051136\n",
       "222   3.15  0.15  0.051136\n",
       "221   3.10  0.15  0.051136\n",
       "220   3.05  0.15  0.051136\n",
       "219   3.00  0.15  0.051136\n",
       "217   2.90  0.15  0.051136\n",
       "216   2.85  0.15  0.051136\n",
       "215   2.80  0.15  0.051136\n",
       "214   2.75  0.15  0.051136"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_SVM.index = range(len(MSE_SVM))\n",
    "MSE_SVM.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table shown above, we note that when **cost = 1, gamma = 1.7**, the SVM model has the lowest MSE, which is  **0.045455**. Meanwhile when the cost = 0.95, gamma = 1.65 or 1.70 respectivlty, the SVM model can generate the similar lowest MSE, which is also 0.045455."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part ( e ) : Fit a single hidden layer neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "clf_mlp = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_mlp.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "activ = np.array(['identity', 'logistic', 'tanh', 'relu'])\n",
    "MSE_al = pd.DataFrame({'activation' : np.zeros(20),\n",
    "                       'hidden layer' : np.zeros(20),\n",
    "                       'alpha' : np.zeros(20),\n",
    "                       'MSE':np.zeros(20)})\n",
    "MSE_mlp = MSE_al[:0]\n",
    "\n",
    "for ac in range(4):\n",
    "    for h in range(8):\n",
    "        for al in range(20):\n",
    "            k_ind = int(0)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                mlp = MLPClassifier(activation=activ[ac], solver='lbfgs',\n",
    "                                    alpha=(al/20 + 0.05), \n",
    "                                    hidden_layer_sizes = ((50 * (h + 1)),))\n",
    "                mlp.fit(X_train, y_train)\n",
    "                y_pred = mlp.predict(X_test)\n",
    "                error = y_test != y_pred\n",
    "                MSE[k_ind] = error.mean()\n",
    "                k_ind += 1\n",
    "            MSE_al['activation'][al] = activ[ac]\n",
    "            MSE_al['hidden layer'][al] = 50 * (h + 1)\n",
    "            MSE_al['alpha'][al] = al/20 + 0.05\n",
    "            MSE_al['MSE'][al] =  MSE.mean()\n",
    "        MSE_mlp = pd.concat([MSE_mlp, MSE_al])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>hidden layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.20</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.75</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.85</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.95</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.50</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.85</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.95</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.55</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.40</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.30</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.55</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.55</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.40</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.30</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE activation  alpha  hidden layer\n",
       "483  0.039773       relu   0.20          50.0\n",
       "553  0.039773       relu   0.70         200.0\n",
       "554  0.039773       relu   0.75         200.0\n",
       "566  0.039773       relu   0.35         250.0\n",
       "528  0.039773       relu   0.45         150.0\n",
       "596  0.045455       relu   0.85         300.0\n",
       "578  0.045455       relu   0.95         250.0\n",
       "529  0.045455       relu   0.50         150.0\n",
       "531  0.045455       relu   0.60         150.0\n",
       "536  0.045455       relu   0.85         150.0\n",
       "606  0.045455       relu   0.35         350.0\n",
       "618  0.045455       relu   0.95         350.0\n",
       "570  0.045455       relu   0.55         250.0\n",
       "567  0.045455       relu   0.40         250.0\n",
       "621  0.045455       relu   0.10         400.0\n",
       "625  0.045455       relu   0.30         400.0\n",
       "611  0.045455       relu   0.60         350.0\n",
       "590  0.045455       relu   0.55         300.0\n",
       "550  0.045455       relu   0.55         200.0\n",
       "551  0.045455       relu   0.60         200.0\n",
       "547  0.045455       relu   0.40         200.0\n",
       "505  0.045455       relu   0.30         100.0\n",
       "628  0.045455       relu   0.45         400.0\n",
       "631  0.045455       relu   0.60         400.0\n",
       "604  0.045455       relu   0.25         350.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_mlp.index = range(len(MSE_mlp))\n",
    "MSE_mlp.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table shown above, we note that **when activation = relu, alpha = 0.2 and hidden layer = 250**, the MLP model can get the lowest MSE, which is **0.039773**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part ( f ) : Which model fits best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logit</th>\n",
       "      <th>MLP</th>\n",
       "      <th>RF</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Logit       MLP        RF       SVM\n",
       "0  0.068182  0.039773  0.068182  0.045455\n",
       "1  0.068182  0.039773  0.068182  0.045455\n",
       "2  0.068182  0.039773  0.068182  0.045455\n",
       "3  0.068182  0.039773  0.068182  0.051136\n",
       "4  0.068182  0.039773  0.068182  0.051136"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = np.array(MSE_multilog.sort_values(['MSE']).head(5)['MSE'])\n",
    "RF = np.array(MSE_df.sort_values(['MSE']).head(5)['MSE'])\n",
    "SVM = np.array(MSE_SVM.sort_values(['MSE']).head(5)['MSE'])\n",
    "mlp = np.array(MSE_mlp.sort_values(['MSE']).head(5)['MSE'])\n",
    "\n",
    "\n",
    "Com = pd.DataFrame({'Logit':logit, \n",
    "                    'RF':RF,\n",
    "                    'SVM':SVM,\n",
    "                    'MLP':mlp})\n",
    "Com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above lists the top five lowest MSE values in these four different models, we note that the MLP neural network model has the lowest MSE values compared with other three models, which means **the MLP neural network model** is the best predictor of cultivar among all of these four models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
