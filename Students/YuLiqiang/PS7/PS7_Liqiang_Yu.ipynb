{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set #7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MACS 30100, Dr. Evans\n",
    "<br>\n",
    "Due Monday, Mar. 5 at 11:30am\n",
    "<br>\n",
    "Liqiang Yu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "auto = pd.read_csv('Auto.csv',na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0    70   \n",
       "1  15.0          8         350.0       165.0    3693          11.5    70   \n",
       "2  18.0          8         318.0       150.0    3436          11.0    70   \n",
       "3  16.0          8         304.0       150.0    3433          12.0    70   \n",
       "4  17.0          8         302.0       140.0    3449          10.5    70   \n",
       "\n",
       "   origin                       name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "Classiﬁer “horse” race (10 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Use sklearn.linear model.LogisticRegression to ﬁt a logistic model of mpg high on features number of cylinders (cyl), engine displacement (dspl), horsepower (hpwr), vehicle weight (wgt), acceleration (accl), vehicle year (yr), vehicle origin (orgn). Make sure to include a constant term. Fit the model using k-fold cross validation with k = 4 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg             float64\n",
       "cylinders         int64\n",
       "displacement    float64\n",
       "horsepower      float64\n",
       "weight            int64\n",
       "acceleration    float64\n",
       "year              int64\n",
       "origin            int64\n",
       "name             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = auto.dropna()\n",
    "auto['mpg_high'] = auto['mpg'].apply(lambda x: 1 if x >= auto['mpg'].median() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K index:  1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.89      0.92        55\n",
      "          1       0.87      0.93      0.90        43\n",
      "\n",
      "avg / total       0.91      0.91      0.91        98\n",
      "\n",
      "The error rate(category 0) is 0.057692307692307696\n",
      "The error rate(category 1) is 0.13043478260869565\n",
      "The MSE is 0.09183673469387756\n",
      "K index:  2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.91      0.90        47\n",
      "          1       0.92      0.88      0.90        51\n",
      "\n",
      "avg / total       0.90      0.90      0.90        98\n",
      "\n",
      "The error rate(category 0) is 0.12244897959183673\n",
      "The error rate(category 1) is 0.08163265306122448\n",
      "The MSE is 0.10204081632653061\n",
      "K index:  3\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.87      0.86        45\n",
      "          1       0.88      0.87      0.88        53\n",
      "\n",
      "avg / total       0.87      0.87      0.87        98\n",
      "\n",
      "The error rate(category 0) is 0.15217391304347827\n",
      "The error rate(category 1) is 0.11538461538461539\n",
      "The MSE is 0.1326530612244898\n",
      "K index:  4\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.84      0.89        49\n",
      "          1       0.85      0.96      0.90        49\n",
      "\n",
      "avg / total       0.90      0.90      0.90        98\n",
      "\n",
      "The error rate(category 0) is 0.046511627906976744\n",
      "The error rate(category 1) is 0.14545454545454545\n",
      "The MSE is 0.10204081632653061\n",
      "\n",
      " k-Fold Reults:\n",
      "The average error rate (mpg_high = 0) is 0.09470670705864986\n",
      "The average error rate (mpg_high = 1) is 0.11822664912727025\n",
      "The average MSE is:  0.10714285714285715\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Xvars = auto[['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "              'acceleration', 'year', 'origin']].values\n",
    "yvals = auto['mpg_high'].values\n",
    "\n",
    "k = 4\n",
    "kf = KFold(n_splits=k, random_state=15, shuffle=True)\n",
    "kf.get_n_splits(Xvars)\n",
    "Logit_err_0 = np.zeros(k)\n",
    "Logit_err_1 = np.zeros(k)\n",
    "Logit_MSE_VEC = np.zeros(k)\n",
    "\n",
    "k_ind = int(0)\n",
    "for train_index, test_index in kf.split(Xvars):\n",
    "    print('K index: ', k_ind + 1)\n",
    "    X_train, X_test = Xvars[train_index], Xvars[test_index]\n",
    "    y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "    LogReg = LogisticRegression(fit_intercept=True)\n",
    "    LogReg.fit(X_train, y_train)\n",
    "    y_pred = LogReg.predict(X_test)\n",
    "    err = y_test != y_pred\n",
    "    Logit_MSE_VEC[k_ind] = err.mean()\n",
    "    Logit_err_0[k_ind] = ((y_pred == 0) * err).sum() / (y_pred == 0).sum() \n",
    "    Logit_err_1[k_ind] = ((y_pred == 1) * err).sum() / (y_pred == 1).sum() \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('The error rate(category 0) is', Logit_err_0[k_ind])\n",
    "    print('The error rate(category 1) is', Logit_err_1[k_ind])\n",
    "    print('The MSE is', Logit_MSE_VEC[k_ind])\n",
    "    k_ind += 1\n",
    "print('\\n k-Fold Reults:')\n",
    "print('The average error rate (mpg_high = 0) is', Logit_err_0.mean())\n",
    "print('The average error rate (mpg_high = 1) is', Logit_err_1.mean())\n",
    "print('The average MSE is: ', Logit_MSE_VEC.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Use sklearn.ensemble.RandomForestClassifier to ﬁt a random forest model of mpg high on max features=2 out of the seven possible features used in part (a). Set n estimators=20, set bootstrap=True, set oob score=True, and set random state=25. Report the MSE of the random forest model as the MSE from the .oob prediction object, and report the error rates for each category of mpg high from the .oob prediction object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.91      0.93       196\n",
      "          1       0.92      0.94      0.93       196\n",
      "\n",
      "avg / total       0.93      0.93      0.93       392\n",
      "\n",
      "The error rate (mpg_high = 0) is 0.05789473684210526\n",
      "The error rate (mpg_high = 1) is 0.08415841584158416\n",
      "The MSE of the model is 0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=20, max_features=2, bootstrap=True,oob_score=True, random_state=25)\n",
    "RF.fit(Xvars, yvals)\n",
    "oob_prediction = RF.oob_decision_function_.T[1]\n",
    "MSE_RF = pd.DataFrame({'pred' : oob_prediction, 'yvals': yvals})\n",
    "MSE_RF['pred'] = MSE_RF['pred'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "\n",
    "error = (MSE_RF['pred'] != MSE_RF['yvals']).apply(int)\n",
    "RF_err_0 = ((MSE_RF['pred'] == 0) * error).sum() / (MSE_RF['pred'] == 0).sum() \n",
    "RF_err_1 = ((MSE_RF['pred'] == 1) * error).sum() / (MSE_RF['pred'] == 1).sum() \n",
    "\n",
    "print(classification_report(MSE_RF['yvals'], MSE_RF['pred']))\n",
    "print('The error rate (mpg_high = 0) is', RF_err_0.mean())\n",
    "print('The error rate (mpg_high = 1) is', RF_err_1.mean())\n",
    "print('The MSE of the model is', error.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Use sklearn.svm.SVC to ﬁt a support vector machines model of mpg high with a Gaussian radial basis function kernel kernel=’rbf’ on the seven features used in part (a). Set the penalty parameter to C=1 and set gamma=0.2. Fit the model using k-fold cross validation with k = 4 folds exactly as in part (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K index:  1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      1.000     0.036     0.070        55\n",
      "          1      0.448     1.000     0.619        43\n",
      "\n",
      "avg / total      0.758     0.459     0.311        98\n",
      "\n",
      "The error rate(mpg_high = 0) is 0.0\n",
      "The error rate(mpg_high = 1) is 0.5520833333333334\n",
      "The MSE is 0.5408163265306123\n",
      "K index:  2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.480     1.000     0.648        47\n",
      "          1      0.000     0.000     0.000        51\n",
      "\n",
      "avg / total      0.230     0.480     0.311        98\n",
      "\n",
      "The error rate(mpg_high = 0) is 0.5204081632653061\n",
      "The error rate(mpg_high = 1) is nan\n",
      "The MSE is 0.5204081632653061\n",
      "K index:  3\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.469     1.000     0.638        45\n",
      "          1      1.000     0.038     0.073        53\n",
      "\n",
      "avg / total      0.756     0.480     0.332        98\n",
      "\n",
      "The error rate(mpg_high = 0) is 0.53125\n",
      "The error rate(mpg_high = 1) is 0.0\n",
      "The MSE is 0.5204081632653061\n",
      "K index:  4\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.527     1.000     0.690        49\n",
      "          1      1.000     0.102     0.185        49\n",
      "\n",
      "avg / total      0.763     0.551     0.438        98\n",
      "\n",
      "The error rate(mpg_high = 0) is 0.4731182795698925\n",
      "The error rate(mpg_high = 1) is 0.0\n",
      "The MSE is 0.4489795918367347\n",
      "\n",
      "k-Fold Reults:\n",
      "The average error rate(category 0) is 0.38119411070879966\n",
      "The average error rate(category 1) is 0.1840277777777778\n",
      "The average MSE of the model is 0.5076530612244898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rex/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/Rex/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVC_err_0 = np.zeros(k)\n",
    "SVC_err_1 = np.zeros(k)\n",
    "SVC_MSE_VEC = np.zeros(k)\n",
    "k_ind = int(0)\n",
    "for train_index, test_index in kf.split(Xvars):\n",
    "    print('K index: ', k_ind + 1)\n",
    "    X_train, X_test = Xvars[train_index], Xvars[test_index]\n",
    "    y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "    svc = svm.SVC(kernel='rbf', gamma=0.2, C=1)\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    err = y_test != y_pred\n",
    "    SVC_MSE_VEC[k_ind] = err.mean()\n",
    "    SVC_err_0[k_ind] = ((y_pred == 0) * err).sum() / (y_pred == 0).sum() \n",
    "    SVC_err_1[k_ind] = ((y_pred == 1) * err).sum() / (y_pred == 1).sum() \n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    print('The error rate(mpg_high = 0) is', SVC_err_0[k_ind])\n",
    "    print('The error rate(mpg_high = 1) is', SVC_err_1[k_ind])\n",
    "    print('The MSE is', SVC_MSE_VEC[k_ind])\n",
    "    k_ind += 1\n",
    "print('\\nk-Fold Reults:')\n",
    "SVC_err_1 = SVC_err_1[~np.isnan(SVC_err_1)]\n",
    "print('The average error rate(category 0) is', SVC_err_0.mean())\n",
    "print('The average error rate(category 1) is', SVC_err_1.mean())\n",
    "print('The average MSE of the model is', SVC_MSE_VEC.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Which of the above three models do you think is the best predictor of mpg high? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average MSE for Logistic Regression is:  0.10714285714285715\n",
    "<br>\n",
    "The average MSE for SVM is:  0.07142857142857142\n",
    "<br>\n",
    "The average MSE for Random Forest is:  0.5076530612244898"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the average MSE across Logistic Regression, SVM and Random Forest, we conclude that Random Forest gives the best result in terms of average MSE.\n",
    "<br>\n",
    "However, we notice that in SVM and Logistic Regression, they used the same k-fold data set, but we used Bootstrap and OOB to calculate the MSE for Random Forest. \n",
    "\n",
    "According to https://stats.stackexchange.com/questions/206807/are-k-fold-cross-validation-bootstrap-out-of-bag-fundamentally-same :\n",
    "\n",
    "K-Fold Cross Validation (CV)\n",
    "\n",
    "It consists in dividing the original set of observations into k subset of more or less same size. Then, you will use one of the subset as test set and the remaining subsets will be used to form your training set. You will repeat this kth times, where each time the subset used as test set will change. As an example, if you use 3-fold CV, your original set will be divided into k1, k2, k3. First, k1 will form the test set, k2 and k3 will form the training set. Then, k2 will form the test set, k1 and k3 will form the training set. Finally, k3 will form the test set, k1 and k3 will form the training set. For each fold, you output the results and you aggregate these to obtain the final result.\n",
    "\n",
    "Bootstrap\n",
    "\n",
    "A bootstrap is a random subset of your original data, sometimes drawn with replacement (check http://www.stat.washington.edu/courses/stat527/s13/readings/EfronTibshirani_JASA_1997.pdf on the .632 rule), sometimes not. But the idea is that a bootstrap contains only a part of your whole set of observations. It is different from CV as it does not contain a testing set. Bootstrap is used to train a different classifier each time on a different set of observations. To output your results, a combination method is used, like averaging for example.\n",
    "\n",
    "Out-of-bag\n",
    "\n",
    "As said above, not all observations are used to form bootstrap. The part not used forms the out-of-bag classifier, and can be used to assess the error rate of your classifier. Out-of-bag are typically used to compute the error-rate, and not to train your classifier.\n",
    "\n",
    "And https://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio : \n",
    "\n",
    "\"It comes down to variance and bias (as usual). CV tends to be less biased but K-fold CV has fairly large variance. On the other hand, bootstrapping tends to drastically reduce the variance but gives more biased results (they tend to be pessimistic). Other bootstrapping methods have been adapted to deal with the bootstrap bias (such as the 632 and 632+ rules).\n",
    "\n",
    "Two other approaches would be \"Monte Carlo CV\" aka \"leave-group-out CV\" which does many random splits of the data (sort of like mini-training and test splits). Variance is very low for this method and the bias isn't too bad if the percentage of data in the hold-out is low. Also, repeated CV does K-fold several times and averages the results similar to regular K-fold. I'm most partial to this since it keeps the low bias and reduces the variance.\"\n",
    "\n",
    "So bootstrapping tends not to overfit the data and thus tends to give a higher MSE. So we believe that Random Forest is the best model among all three models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
