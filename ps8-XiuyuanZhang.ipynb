{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set #8\n",
    "MACS30100 Dr. Evans  \n",
    "Xiuyuan Zhang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "### (a) Create a scatterplot of the data where the x-variable is alcohol (alco) and the y-variable is color intensity (color int). Make the dot of each of the three possible cultivar types a different color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cultivar</th>\n",
       "      <th>alco</th>\n",
       "      <th>malic</th>\n",
       "      <th>ash</th>\n",
       "      <th>alk</th>\n",
       "      <th>magn</th>\n",
       "      <th>tot_phen</th>\n",
       "      <th>flav</th>\n",
       "      <th>nonfl_phen</th>\n",
       "      <th>proanth</th>\n",
       "      <th>color_int</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280rat</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>14.10</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>14.12</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.32</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.39</td>\n",
       "      <td>11.4</td>\n",
       "      <td>91</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>14.38</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>102</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.96</td>\n",
       "      <td>7.50</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>13.63</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.70</td>\n",
       "      <td>17.2</td>\n",
       "      <td>112</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.46</td>\n",
       "      <td>7.30</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>14.30</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.72</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>13.83</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.62</td>\n",
       "      <td>20.0</td>\n",
       "      <td>115</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.72</td>\n",
       "      <td>6.60</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>14.19</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.48</td>\n",
       "      <td>16.5</td>\n",
       "      <td>108</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.86</td>\n",
       "      <td>8.70</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>13.64</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.56</td>\n",
       "      <td>15.2</td>\n",
       "      <td>116</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.66</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.36</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cultivar   alco  malic   ash   alk  magn  tot_phen  flav  nonfl_phen  \\\n",
       "0          1  14.23   1.71  2.43  15.6   127      2.80  3.06        0.28   \n",
       "1          1  13.20   1.78  2.14  11.2   100      2.65  2.76        0.26   \n",
       "2          1  13.16   2.36  2.67  18.6   101      2.80  3.24        0.30   \n",
       "3          1  14.37   1.95  2.50  16.8   113      3.85  3.49        0.24   \n",
       "4          1  13.24   2.59  2.87  21.0   118      2.80  2.69        0.39   \n",
       "5          1  14.20   1.76  2.45  15.2   112      3.27  3.39        0.34   \n",
       "6          1  14.39   1.87  2.45  14.6    96      2.50  2.52        0.30   \n",
       "7          1  14.06   2.15  2.61  17.6   121      2.60  2.51        0.31   \n",
       "8          1  14.83   1.64  2.17  14.0    97      2.80  2.98        0.29   \n",
       "9          1  13.86   1.35  2.27  16.0    98      2.98  3.15        0.22   \n",
       "10         1  14.10   2.16  2.30  18.0   105      2.95  3.32        0.22   \n",
       "11         1  14.12   1.48  2.32  16.8    95      2.20  2.43        0.26   \n",
       "12         1  13.75   1.73  2.41  16.0    89      2.60  2.76        0.29   \n",
       "13         1  14.75   1.73  2.39  11.4    91      3.10  3.69        0.43   \n",
       "14         1  14.38   1.87  2.38  12.0   102      3.30  3.64        0.29   \n",
       "15         1  13.63   1.81  2.70  17.2   112      2.85  2.91        0.30   \n",
       "16         1  14.30   1.92  2.72  20.0   120      2.80  3.14        0.33   \n",
       "17         1  13.83   1.57  2.62  20.0   115      2.95  3.40        0.40   \n",
       "18         1  14.19   1.59  2.48  16.5   108      3.30  3.93        0.32   \n",
       "19         1  13.64   3.10  2.56  15.2   116      2.70  3.03        0.17   \n",
       "\n",
       "    proanth  color_int   hue  OD280rat  proline  \n",
       "0      2.29       5.64  1.04      3.92     1065  \n",
       "1      1.28       4.38  1.05      3.40     1050  \n",
       "2      2.81       5.68  1.03      3.17     1185  \n",
       "3      2.18       7.80  0.86      3.45     1480  \n",
       "4      1.82       4.32  1.04      2.93      735  \n",
       "5      1.97       6.75  1.05      2.85     1450  \n",
       "6      1.98       5.25  1.02      3.58     1290  \n",
       "7      1.25       5.05  1.06      3.58     1295  \n",
       "8      1.98       5.20  1.08      2.85     1045  \n",
       "9      1.85       7.22  1.01      3.55     1045  \n",
       "10     2.38       5.75  1.25      3.17     1510  \n",
       "11     1.57       5.00  1.17      2.82     1280  \n",
       "12     1.81       5.60  1.15      2.90     1320  \n",
       "13     2.81       5.40  1.25      2.73     1150  \n",
       "14     2.96       7.50  1.20      3.00     1547  \n",
       "15     1.46       7.30  1.28      2.88     1310  \n",
       "16     1.97       6.20  1.07      2.65     1280  \n",
       "17     1.72       6.60  1.13      2.57     1130  \n",
       "18     1.86       8.70  1.23      2.82     1680  \n",
       "19     1.66       5.10  0.96      3.36      845  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "strongdrink = pd.read_csv('data/strongdrink.txt')\n",
    "strongdrink.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAH1CAYAAACtPWl3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a5dd320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cultivars = strongdrink.groupby('cultivar')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for kind, cultivar in cultivars:\n",
    "    ax.plot(cultivar['alco'], cultivar['color_int'], marker = 'o',linestyle='', ms=4, label=kind)\n",
    "ax.legend()\n",
    "plt.xlabel('Alcohol', fontsize = 12)\n",
    "plt.ylabel('Color Intensity', fontsize = 12)\n",
    "plt.title('Alcohol and Color Intensity',fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Use sklearn.linear model.LogisticRegression to fit a multinomial lo- gistic model of cultivar on features alcohol (alco), malic acid (malic), total phenols (tot phen), and color intensity (color int) with the following linear predictor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "X = strongdrink[['alco', 'malic', 'tot_phen', 'color_int']].values\n",
    "y = strongdrink['cultivar'].values\n",
    "\n",
    "# set k-fold value\n",
    "k = 4\n",
    "clf_mlog = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_mlog.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "C = np.zeros(200)\n",
    "MSE_C = np.zeros(200)\n",
    "\n",
    "for c in range(200):\n",
    "    k_ind = int(0)\n",
    "    for train_index, test_index in clf_mlog.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        LogReg = LogisticRegression(multi_class='multinomial',\n",
    "                                    solver='newton-cg', C = (c/100 + 0.01))\n",
    "        LogReg.fit(X_train, y_train)\n",
    "        y_pred = LogReg.predict(X_test)\n",
    "        error = y_test != y_pred\n",
    "        MSE[k_ind] = error.mean()\n",
    "        k_ind += 1\n",
    "    C[c] = c/100 + 0.01\n",
    "    MSE_C[c] =  MSE.mean()\n",
    "MSE_multilog = pd.DataFrame({'C':C, 'MSE':MSE_C})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C       MSE\n",
       "43  0.44  0.068182\n",
       "44  0.45  0.068182\n",
       "42  0.43  0.068182\n",
       "41  0.42  0.068182\n",
       "40  0.41  0.068182\n",
       "39  0.40  0.068182\n",
       "27  0.28  0.068182\n",
       "28  0.29  0.068182\n",
       "29  0.30  0.068182\n",
       "30  0.31  0.068182"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_multilog.sort_values(['MSE']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see from the table above that when C takes value between 0.28-0.44 (with the penalty being 'l2'), the multinomial logistic model get the lowest MSE, which is 0.068182."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Use sklearn.ensemble.RandomForestClassifier to fit a random for- est model of cultivar on the same four features used in part (b). set bootstrap=True, set oob score=True, and set random state=22. Use OOB cross-validation to generate the MSE of your random forest classifier. Play with the values of the tuning parameters n estimators, max depth, and min samples leaf to try and find the lowest possible MSE from the OOB cross validation. Report your minimized overall MSE along with the tuning parameter values you used for n estimators, max depth, and min samples leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_rf = pd.DataFrame({\"n_estimator\" : np.zeros(5),\n",
    "                         \"min_sample\" : np.zeros(5),\n",
    "                         \"max_depth\" : np.zeros(5),\n",
    "                         \"MSE\" : np.zeros(5)})\n",
    "MSE_df = MSE_rf[:0]\n",
    "\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        for tree in range(5):\n",
    "            rf = RandomForestClassifier(n_estimators = (tree * 50 + 50),\n",
    "                                        min_samples_leaf = (i * 5 + 5),\n",
    "                                        max_depth = (j + 1), bootstrap=True, \n",
    "                                        oob_score=True, random_state=22)\n",
    "            rf.fit(X, y)\n",
    "            MSE_rf[\"n_estimator\"][tree] = tree * 50 + 50\n",
    "            MSE_rf[\"max_depth\"][tree] = j + 1\n",
    "            MSE_rf[\"min_sample\"][tree] = i * 5 + 5\n",
    "            MSE_rf[\"MSE\"][tree] = 1 - rf.oob_score_\n",
    "        MSE_df= pd.concat([MSE_df, MSE_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_sample</th>\n",
       "      <th>n_estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE  max_depth  min_sample  n_estimator\n",
       "96   0.068182        5.0        10.0        100.0\n",
       "126  0.068182       11.0        10.0        100.0\n",
       "116  0.068182        9.0        10.0        100.0\n",
       "131  0.068182       12.0        10.0        100.0\n",
       "111  0.068182        8.0        10.0        100.0\n",
       "106  0.068182        7.0        10.0        100.0\n",
       "121  0.068182       10.0        10.0        100.0\n",
       "101  0.068182        6.0        10.0        100.0\n",
       "136  0.068182       13.0        10.0        100.0\n",
       "86   0.068182        3.0        10.0        100.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_df.index = range(len(MSE_df))\n",
    "MSE_df.sort_values(['MSE']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that with the max_depth = 5, min_sample_leaf = 10 and the n_estimator = 100, the random forest model get its lowest MSE of 0.068182."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Use sklearn.svm.SVC to fit a support vector machines model of cultivar with a Gaussian radial basis function kernel kernel=rbf on the four fea- tures used in parts (b) and (c). Fit the model using k-fold cross validation with k = 4 folds exactly as in part (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "clf_svm = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "clf_svm.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "MSE_C = pd.DataFrame({\"C\" : np.zeros(80),\n",
    "                      \"G\" : np.zeros(80),\n",
    "                      \"MSE\" : np.zeros(80)})\n",
    "MSE_SVM = MSE_C[:0]\n",
    "\n",
    "for g in range(80):\n",
    "    for c in range(80):\n",
    "        k_ind = int(0)\n",
    "        for train_index, test_index in clf_svm.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            svc = svm.SVC(kernel='rbf', gamma = (g/20 + 0.05),\n",
    "                          C=c/20 + 0.05)\n",
    "            svc.fit(X_train, y_train)\n",
    "            y_pred = svc.predict(X_test)\n",
    "            error = y_test != y_pred\n",
    "            MSE[k_ind] = error.mean()\n",
    "            k_ind += 1\n",
    "        MSE_C['C'][c] = c/20 + 0.05\n",
    "        MSE_C['G'][c] = g/20 + 0.05\n",
    "        MSE_C['MSE'][c] =  MSE.mean()\n",
    "    MSE_SVM = pd.concat([MSE_SVM, MSE_C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>G</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>3.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>3.30</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2.95</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>3.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>3.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2.90</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         C     G       MSE\n",
       "2659  1.00  1.70  0.045455\n",
       "2578  0.95  1.65  0.045455\n",
       "2658  0.95  1.70  0.045455\n",
       "224   3.25  0.15  0.051136\n",
       "225   3.30  0.15  0.051136\n",
       "226   3.35  0.15  0.051136\n",
       "227   3.40  0.15  0.051136\n",
       "228   3.45  0.15  0.051136\n",
       "229   3.50  0.15  0.051136\n",
       "218   2.95  0.15  0.051136\n",
       "230   3.55  0.15  0.051136\n",
       "231   3.60  0.15  0.051136\n",
       "232   3.65  0.15  0.051136\n",
       "233   3.70  0.15  0.051136\n",
       "234   3.75  0.15  0.051136\n",
       "235   3.80  0.15  0.051136\n",
       "223   3.20  0.15  0.051136\n",
       "222   3.15  0.15  0.051136\n",
       "221   3.10  0.15  0.051136\n",
       "220   3.05  0.15  0.051136\n",
       "219   3.00  0.15  0.051136\n",
       "217   2.90  0.15  0.051136\n",
       "216   2.85  0.15  0.051136\n",
       "215   2.80  0.15  0.051136\n",
       "214   2.75  0.15  0.051136"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_SVM.index = range(len(MSE_SVM))\n",
    "MSE_SVM.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see from the above table, when cost = 1, gamma = 1.7, the SVM model has the lowest MSE = 0.045455. With cost = 0.95, gamma = 1.65 or = 1.70, the SVM model can generate similar lowest MSE = 0.045455."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Use sklearn.neural network.MLPClassifier to fit a single hidden layer neural network model of cultivar. Fit the model using k-fold cross vali- dation with k = 4 folds exactly as in parts (b) and (d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "kf = KFold(n_splits=k, random_state=22, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "MSE = np.zeros(k)\n",
    "activ = np.array(['identity', 'logistic', 'tanh', 'relu'])\n",
    "MSE_al = pd.DataFrame({'activation' : np.zeros(20),\n",
    "                       'hidden layer' : np.zeros(20),\n",
    "                       'alpha' : np.zeros(20),\n",
    "                       'MSE':np.zeros(20)})\n",
    "MSE_mlp = MSE_al[:0]\n",
    "\n",
    "for ac in range(4):\n",
    "    for h in range(8):\n",
    "        for al in range(20):\n",
    "            k_ind = int(0)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                mlp = MLPClassifier(activation=activ[ac], solver='lbfgs',\n",
    "                                    alpha=(al/20 + 0.05), \n",
    "                                    hidden_layer_sizes = ((50 * (h + 1)),))\n",
    "                mlp.fit(X_train, y_train)\n",
    "                y_pred = mlp.predict(X_test)\n",
    "                error = y_test != y_pred\n",
    "                MSE[k_ind] = error.mean()\n",
    "                k_ind += 1\n",
    "            MSE_al['activation'][al] = activ[ac]\n",
    "            MSE_al['hidden layer'][al] = 50 * (h + 1)\n",
    "            MSE_al['alpha'][al] = al/20 + 0.05\n",
    "            MSE_al['MSE'][al] =  MSE.mean()\n",
    "        MSE_mlp = pd.concat([MSE_mlp, MSE_al])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>hidden layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.034091</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.30</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.75</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.20</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>0.039773</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.55</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.30</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.55</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>1.00</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.20</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.30</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.20</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.35</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.60</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.90</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.70</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.75</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.65</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>1.00</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSE activation  alpha  hidden layer\n",
       "505  0.034091       relu   0.30         100.0\n",
       "554  0.039773       relu   0.75         200.0\n",
       "608  0.039773       relu   0.45         350.0\n",
       "543  0.039773       relu   0.20         200.0\n",
       "571  0.039773       relu   0.60         250.0\n",
       "586  0.039773       relu   0.35         300.0\n",
       "610  0.045455       relu   0.55         350.0\n",
       "542  0.045455       relu   0.15         200.0\n",
       "545  0.045455       relu   0.30         200.0\n",
       "550  0.045455       relu   0.55         200.0\n",
       "599  0.045455       relu   1.00         300.0\n",
       "548  0.045455       relu   0.45         200.0\n",
       "591  0.045455       relu   0.60         300.0\n",
       "563  0.045455       relu   0.20         250.0\n",
       "585  0.045455       relu   0.30         300.0\n",
       "486  0.045455       relu   0.35          50.0\n",
       "583  0.045455       relu   0.20         300.0\n",
       "582  0.045455       relu   0.15         300.0\n",
       "566  0.045455       relu   0.35         250.0\n",
       "491  0.045455       relu   0.60          50.0\n",
       "577  0.045455       relu   0.90         250.0\n",
       "613  0.045455       relu   0.70         350.0\n",
       "614  0.045455       relu   0.75         350.0\n",
       "572  0.045455       relu   0.65         250.0\n",
       "499  0.045455       relu   1.00          50.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_mlp.index = range(len(MSE_mlp))\n",
    "MSE_mlp.sort_values(['MSE']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see from the table above that when activation = relu, alpha = 0.3 and hidden layer = 100, the MLP model gets the lowest MSE, which is 0.034091."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Which of the above three models do you think is the best predictor of cultivar? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logit</th>\n",
       "      <th>MLP</th>\n",
       "      <th>RF</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Logit       MLP        RF       SVM\n",
       "0  0.068182  0.034091  0.068182  0.045455\n",
       "1  0.068182  0.039773  0.068182  0.045455\n",
       "2  0.068182  0.039773  0.068182  0.045455\n",
       "3  0.068182  0.039773  0.068182  0.051136\n",
       "4  0.068182  0.039773  0.068182  0.051136"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = np.array(MSE_multilog.sort_values(['MSE']).head(5)['MSE'])\n",
    "RF = np.array(MSE_df.sort_values(['MSE']).head(5)['MSE'])\n",
    "SVM = np.array(MSE_SVM.sort_values(['MSE']).head(5)['MSE'])\n",
    "mlp = np.array(MSE_mlp.sort_values(['MSE']).head(5)['MSE'])\n",
    "\n",
    "\n",
    "Com = pd.DataFrame({'Logit':logit, \n",
    "                    'RF':RF,\n",
    "                    'SVM':SVM,\n",
    "                    'MLP':mlp})\n",
    "Com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, comparing different MSE values, I think that the best predictor of cultivar is MLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
